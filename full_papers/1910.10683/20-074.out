\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Setup}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Model}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{The Colossal Clean Crawled Corpus}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Downstream Tasks}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.4}{Input and Output Format}{section.2}% 6
\BOOKMARK [1][-]{section.3}{Experiments}{}% 7
\BOOKMARK [2][-]{subsection.3.1}{Baseline}{section.3}% 8
\BOOKMARK [3][-]{subsubsection.3.1.1}{Model}{subsection.3.1}% 9
\BOOKMARK [3][-]{subsubsection.3.1.2}{Training}{subsection.3.1}% 10
\BOOKMARK [3][-]{subsubsection.3.1.3}{Vocabulary}{subsection.3.1}% 11
\BOOKMARK [3][-]{subsubsection.3.1.4}{Unsupervised Objective}{subsection.3.1}% 12
\BOOKMARK [3][-]{subsubsection.3.1.5}{Baseline Performance}{subsection.3.1}% 13
\BOOKMARK [2][-]{subsection.3.2}{Architectures}{section.3}% 14
\BOOKMARK [3][-]{subsubsection.3.2.1}{Model Structures}{subsection.3.2}% 15
\BOOKMARK [3][-]{subsubsection.3.2.2}{Comparing Different Model Structures}{subsection.3.2}% 16
\BOOKMARK [3][-]{subsubsection.3.2.3}{Objectives}{subsection.3.2}% 17
\BOOKMARK [3][-]{subsubsection.3.2.4}{Results}{subsection.3.2}% 18
\BOOKMARK [2][-]{subsection.3.3}{Unsupervised Objectives}{section.3}% 19
\BOOKMARK [3][-]{subsubsection.3.3.1}{Disparate High-Level Approaches}{subsection.3.3}% 20
\BOOKMARK [3][-]{subsubsection.3.3.2}{Simplifying the BERT Objective}{subsection.3.3}% 21
\BOOKMARK [3][-]{subsubsection.3.3.3}{Varying the Corruption Rate}{subsection.3.3}% 22
\BOOKMARK [3][-]{subsubsection.3.3.4}{Corrupting Spans}{subsection.3.3}% 23
\BOOKMARK [3][-]{subsubsection.3.3.5}{Discussion}{subsection.3.3}% 24
\BOOKMARK [2][-]{subsection.3.4}{Pre-training Data set}{section.3}% 25
\BOOKMARK [3][-]{subsubsection.3.4.1}{Unlabeled Data Sets}{subsection.3.4}% 26
\BOOKMARK [3][-]{subsubsection.3.4.2}{Pre-training Data set Size}{subsection.3.4}% 27
\BOOKMARK [2][-]{subsection.3.5}{Training Strategy}{section.3}% 28
\BOOKMARK [3][-]{subsubsection.3.5.1}{Fine-tuning Methods}{subsection.3.5}% 29
\BOOKMARK [3][-]{subsubsection.3.5.2}{Multi-task Learning}{subsection.3.5}% 30
\BOOKMARK [3][-]{subsubsection.3.5.3}{Combining Multi-Task Learning with Fine-Tuning}{subsection.3.5}% 31
\BOOKMARK [2][-]{subsection.3.6}{Scaling}{section.3}% 32
\BOOKMARK [2][-]{subsection.3.7}{Putting It All Together}{section.3}% 33
\BOOKMARK [1][-]{section.4}{Reflection}{}% 34
\BOOKMARK [2][-]{subsection.4.1}{Takeaways}{section.4}% 35
\BOOKMARK [2][-]{subsection.4.2}{Outlook}{section.4}% 36
\BOOKMARK [1][-]{section.1}{Contributions}{}% 37
\BOOKMARK [1][-]{section.2}{Converting WNLI to Our Text-to-Text Format}{}% 38
\BOOKMARK [1][-]{section.3}{Example Predictions on CNN/Daily Mail}{}% 39
\BOOKMARK [1][-]{section.4}{Preprocessed Examples}{}% 40
\BOOKMARK [2][-]{subsection.4.1}{CoLA}{section.4}% 41
\BOOKMARK [2][-]{subsection.4.2}{RTE}{section.4}% 42
\BOOKMARK [2][-]{subsection.4.3}{MNLI}{section.4}% 43
\BOOKMARK [2][-]{subsection.4.4}{MRPC}{section.4}% 44
\BOOKMARK [2][-]{subsection.4.5}{QNLI}{section.4}% 45
\BOOKMARK [2][-]{subsection.4.6}{QQP}{section.4}% 46
\BOOKMARK [2][-]{subsection.4.7}{SST2}{section.4}% 47
\BOOKMARK [2][-]{subsection.4.8}{STSB}{section.4}% 48
\BOOKMARK [2][-]{subsection.4.9}{CB}{section.4}% 49
\BOOKMARK [2][-]{subsection.4.10}{COPA}{section.4}% 50
\BOOKMARK [2][-]{subsection.4.11}{MultiRC}{section.4}% 51
\BOOKMARK [2][-]{subsection.4.12}{WiC}{section.4}% 52
\BOOKMARK [2][-]{subsection.4.13}{WSC and DPR}{section.4}% 53
\BOOKMARK [2][-]{subsection.4.14}{CNN/Daily Mail}{section.4}% 54
\BOOKMARK [2][-]{subsection.4.15}{SQuAD}{section.4}% 55
\BOOKMARK [2][-]{subsection.4.16}{WMT English to German}{section.4}% 56
\BOOKMARK [2][-]{subsection.4.17}{WMT English to French}{section.4}% 57
\BOOKMARK [2][-]{subsection.4.18}{WMT English to Romanian}{section.4}% 58
\BOOKMARK [1][-]{section.5}{Scores on Every Task for All Experiments}{}% 59
