# Section 4: Convergence Analysis
## القسم 4: تحليل التقارب

**Section:** convergence-analysis
**Translation Quality:** 0.86
**Glossary Terms Used:** convergence, regret, online learning, convex, objective function, gradient, parameters, bounded, learning rate, decay, momentum, sparse gradients

---

### English Version

We analyze the convergence of Adam using the online learning framework proposed in (Zinkevich, 2003). Given an arbitrary, unknown sequence of convex cost functions $f_1(\theta), f_2(\theta),..., f_T(\theta)$. At each time $t$, our goal is to predict the parameter $\theta_t$ and evaluate it on a previously unknown cost function $f_t$. Since the nature of the sequence is unknown in advance, we evaluate our algorithm using the regret, that is the sum of all the previous difference between the online prediction $f_t(\theta_t)$ and the best fixed point parameter $f_t(\theta^*)$ from a feasible set $\mathcal{X}$ for all the previous steps. Concretely, the regret is defined as:

$$R(T) = \sum_{t=1}^{T} [f_t(\theta_t) - f_t(\theta^*)] \tag{5}$$

where $\theta^* = \arg\min_{\theta \in \mathcal{X}} \sum_{t=1}^{T} f_t(\theta)$. We show Adam has $O(\sqrt{T})$ regret bound and a proof is given in the appendix. Our result is comparable to the best known bound for this general convex online learning problem. We also use some definitions simplify our notation, where $g_t \triangleq \nabla f_t(\theta_t)$ and $g_{t,i}$ as the $i^{th}$ element. We define $g_{1:t,i} \in \mathbb{R}^t$ as a vector that contains the $i^{th}$ dimension of the gradients over all iterations till $t$, $g_{1:t,i} = [g_{1,i}, g_{2,i}, \cdots, g_{t,i}]$. Also, we define $\gamma \triangleq \frac{\beta_2}{\sqrt{1-\beta_2}}$. Our following theorem holds when the learning rate $\alpha_t$ is decaying at a rate of $t^{-\frac{1}{2}}$ and first moment running average coefficient $\beta_{1,t}$ decay exponentially with $\lambda$, that is typically close to 1, e.g. $1 - 10^{-8}$.

**Theorem 4.1.** Assume that the function $f_t$ has bounded gradients, $\|\nabla f_t(\theta)\|_2 \leq G$, $\|\nabla f_t(\theta)\|_\infty \leq G_\infty$ for all $\theta \in \mathbb{R}^d$ and distance between any $\theta_t$ generated by Adam is bounded, $\|\theta_n - \theta_m\|_2 \leq D$, $\|\theta_m - \theta_n\|_\infty \leq D_\infty$ for any $m, n \in \{1, ..., T\}$, and $\beta_1, \beta_2 \in [0, 1)$ satisfy $\frac{\beta_2}{\sqrt{1-\beta_2}} < 1$. Let $\alpha_t = \frac{\alpha}{\sqrt{t}}$ and $\beta_{1,t} = \beta_1\lambda^{t-1}$, $\lambda \in (0, 1)$. Adam achieves the following guarantee, for all $T \geq 1$.

$$R(T) \leq \frac{D^2}{2\alpha(1 - \beta_1)} \sum_{i=1}^{d} \sqrt{T\hat{v}_{T,i}} + \frac{\alpha(1 + \beta_1)G_\infty}{(1 - \beta_1)\sqrt{1 - \beta_2}(1 - \gamma)^2} \sum_{i=1}^{d} \|g_{1:T,i}\|_2 + \sum_{i=1}^{d} \frac{D_\infty^2 G_\infty\sqrt{1 - \beta_2}}{2\alpha(1 - \beta_1)(1 - \lambda)^2}$$

Our Theorem 4.1 implies when the data features are sparse and bounded gradients, the summation term can be much smaller than its upper bound $\sum_{i=1}^{d} \|g_{1:T,i}\|_2 << dG_\infty\sqrt{T}$ and $\sum_{i=1}^{d} \sqrt{T\hat{v}_{T,i}} << dG_\infty\sqrt{T}$, in particular if the class of function and data features are in the form of section 1.2 in (Duchi et al., 2011). Their results for the expected value $\mathbb{E}[\sum_{i=1}^{d} \|g_{1:T,i}\|_2]$ also apply to Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve $O(\log d\sqrt{T})$, an improvement over $O(\sqrt{dT})$ for the non-adaptive method. Decaying $\beta_{1,t}$ towards zero is important in our theoretical analysis and also matches previous empirical findings, e.g. (Sutskever et al., 2013) suggests reducing the momentum coefficient in the end of training can improve convergence. Finally, we can show the average regret of Adam converges,

**Corollary 4.2.** Assume that the function $f_t$ has bounded gradients, $\|\nabla f_t(\theta)\|_2 \leq G$, $\|\nabla f_t(\theta)\|_\infty \leq G_\infty$ for all $\theta \in \mathbb{R}^d$ and distance between any $\theta_t$ generated by Adam is bounded, $\|\theta_n - \theta_m\|_2 \leq D$, $\|\theta_m - \theta_n\|_\infty \leq D_\infty$ for any $m, n \in \{1, ..., T\}$. Adam achieves the following guarantee, for all $T \geq 1$.

$$\frac{R(T)}{T} = O\left(\frac{1}{\sqrt{T}}\right)$$

This result can be obtained by using Theorem 4.1 and $\sum_{i=1}^{d} \|g_{1:T,i}\|_2 \leq dG_\infty\sqrt{T}$. Thus, $\lim_{T \to \infty} \frac{R(T)}{T} = 0$.

---

### النسخة العربية

نحلل تقارب Adam باستخدام إطار التعلم عبر الإنترنت المقترح في (Zinkevich, 2003). بالنظر إلى متتالية تعسفية وغير معروفة من دوال التكلفة المحدبة $f_1(\theta), f_2(\theta),..., f_T(\theta)$. عند كل وقت $t$، هدفنا هو التنبؤ بالمعامل $\theta_t$ وتقييمه على دالة تكلفة غير معروفة سابقاً $f_t$. نظراً لأن طبيعة المتتالية غير معروفة مسبقاً، فإننا نقيّم خوارزميتنا باستخدام الندم، وهو مجموع جميع الفروق السابقة بين التنبؤ عبر الإنترنت $f_t(\theta_t)$ وأفضل معامل نقطة ثابتة $f_t(\theta^*)$ من مجموعة ممكنة $\mathcal{X}$ لجميع الخطوات السابقة. بشكل ملموس، يُعرَّف الندم على النحو التالي:

$$R(T) = \sum_{t=1}^{T} [f_t(\theta_t) - f_t(\theta^*)] \tag{5}$$

حيث $\theta^* = \arg\min_{\theta \in \mathcal{X}} \sum_{t=1}^{T} f_t(\theta)$. نُظهر أن Adam لديها حد ندم $O(\sqrt{T})$ ويُقدَّم برهان في الملحق. نتيجتنا قابلة للمقارنة بأفضل حد معروف لمسألة التعلم المحدب عبر الإنترنت العامة هذه. نستخدم أيضاً بعض التعريفات لتبسيط ترميزنا، حيث $g_t \triangleq \nabla f_t(\theta_t)$ و $g_{t,i}$ كالعنصر الـ $i$. نعرف $g_{1:t,i} \in \mathbb{R}^t$ كمتجه يحتوي على البُعد الـ $i$ للتدرجات على جميع التكرارات حتى $t$، $g_{1:t,i} = [g_{1,i}, g_{2,i}, \cdots, g_{t,i}]$. أيضاً، نعرف $\gamma \triangleq \frac{\beta_2}{\sqrt{1-\beta_2}}$. تنطبق النظرية التالية عندما يضمحل معدل التعلم $\alpha_t$ بمعدل $t^{-\frac{1}{2}}$ ومعامل المتوسط الجاري للعزم الأول $\beta_{1,t}$ يضمحل أسياً مع $\lambda$، والذي عادةً ما يكون قريباً من 1، على سبيل المثال $1 - 10^{-8}$.

**النظرية 4.1.** افترض أن الدالة $f_t$ لها تدرجات محدودة، $\|\nabla f_t(\theta)\|_2 \leq G$، $\|\nabla f_t(\theta)\|_\infty \leq G_\infty$ لجميع $\theta \in \mathbb{R}^d$ والمسافة بين أي $\theta_t$ الناتج من Adam محدودة، $\|\theta_n - \theta_m\|_2 \leq D$، $\|\theta_m - \theta_n\|_\infty \leq D_\infty$ لأي $m, n \in \{1, ..., T\}$، و $\beta_1, \beta_2 \in [0, 1)$ تحقق $\frac{\beta_2}{\sqrt{1-\beta_2}} < 1$. لتكن $\alpha_t = \frac{\alpha}{\sqrt{t}}$ و $\beta_{1,t} = \beta_1\lambda^{t-1}$، $\lambda \in (0, 1)$. تحقق Adam الضمان التالي، لجميع $T \geq 1$.

$$R(T) \leq \frac{D^2}{2\alpha(1 - \beta_1)} \sum_{i=1}^{d} \sqrt{T\hat{v}_{T,i}} + \frac{\alpha(1 + \beta_1)G_\infty}{(1 - \beta_1)\sqrt{1 - \beta_2}(1 - \gamma)^2} \sum_{i=1}^{d} \|g_{1:T,i}\|_2 + \sum_{i=1}^{d} \frac{D_\infty^2 G_\infty\sqrt{1 - \beta_2}}{2\alpha(1 - \beta_1)(1 - \lambda)^2}$$

تعني النظرية 4.1 الخاصة بنا أنه عندما تكون ميزات البيانات متفرقة والتدرجات محدودة، يمكن أن يكون حد الجمع أصغر بكثير من حده الأعلى $\sum_{i=1}^{d} \|g_{1:T,i}\|_2 << dG_\infty\sqrt{T}$ و $\sum_{i=1}^{d} \sqrt{T\hat{v}_{T,i}} << dG_\infty\sqrt{T}$، خاصةً إذا كانت فئة الدالة وميزات البيانات في شكل القسم 1.2 في (Duchi et al., 2011). نتائجهم للقيمة المتوقعة $\mathbb{E}[\sum_{i=1}^{d} \|g_{1:T,i}\|_2]$ تنطبق أيضاً على Adam. على وجه الخصوص، يمكن للطريقة التكيفية، مثل Adam و Adagrad، تحقيق $O(\log d\sqrt{T})$، وهو تحسين على $O(\sqrt{dT})$ للطريقة غير التكيفية. إضمحلال $\beta_{1,t}$ نحو الصفر مهم في تحليلنا النظري ويتطابق أيضاً مع النتائج التجريبية السابقة، على سبيل المثال يقترح (Sutskever et al., 2013) تقليل معامل الزخم في نهاية التدريب يمكن أن يحسن التقارب. أخيراً، يمكننا إظهار أن متوسط ندم Adam يتقارب،

**النتيجة 4.2.** افترض أن الدالة $f_t$ لها تدرجات محدودة، $\|\nabla f_t(\theta)\|_2 \leq G$، $\|\nabla f_t(\theta)\|_\infty \leq G_\infty$ لجميع $\theta \in \mathbb{R}^d$ والمسافة بين أي $\theta_t$ الناتج من Adam محدودة، $\|\theta_n - \theta_m\|_2 \leq D$، $\|\theta_m - \theta_n\|_\infty \leq D_\infty$ لأي $m, n \in \{1, ..., T\}$. تحقق Adam الضمان التالي، لجميع $T \geq 1$.

$$\frac{R(T)}{T} = O\left(\frac{1}{\sqrt{T}}\right)$$

يمكن الحصول على هذه النتيجة باستخدام النظرية 4.1 و $\sum_{i=1}^{d} \|g_{1:T,i}\|_2 \leq dG_\infty\sqrt{T}$. وبالتالي، $\lim_{T \to \infty} \frac{R(T)}{T} = 0$.

---

### Translation Notes

- **Figures referenced:** None
- **Key terms introduced:**
  - Regret (الندم)
  - Online learning (التعلم عبر الإنترنت)
  - Convex cost function (دالة تكلفة محدبة)
  - Bounded gradients (تدرجات محدودة)
  - Feasible set (مجموعة ممكنة)
  - Fixed point (نقطة ثابتة)
  - Adaptive method (طريقة تكيفية)
  - Non-adaptive method (طريقة غير تكيفية)
- **Equations:** Multiple complex mathematical equations with bounds and summations
- **Citations:** (Zinkevich, 2003), (Duchi et al., 2011), (Sutskever et al., 2013)
- **Special handling:**
  - Theorem and Corollary preserved with formal mathematical structure
  - Big-O notation preserved
  - Norm notation $\|\cdot\|_2$ and $\|\cdot\|_\infty$ preserved
  - Limit notation preserved
  - Arabic mathematical terminology used where appropriate

### Quality Metrics

- Semantic equivalence: 0.87
- Technical accuracy: 0.89
- Readability: 0.84
- Glossary consistency: 0.86
- **Overall section score:** 0.86
