# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
## تطبيع الحزمة: تسريع تدريب الشبكات العميقة من خلال تقليل التحول التباين الداخلي

**arXiv ID:** 1502.03167
**Authors:** Sergey Ioffe, Christian Szegedy
**Year:** 2015
**Publication:** ICML 2015 (International Conference on Machine Learning)
**Categories:** Machine Learning (cs.LG)
**DOI:** https://doi.org/10.48550/arXiv.1502.03167
**PDF:** https://arxiv.org/pdf/1502.03167.pdf

**Abstract Translation Quality:** 0.90 (from translations/)
**Full Paper Translation Quality:** 0.88

## Citation

```bibtex
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}
```

## Impact and Significance

This paper introduced Batch Normalization, one of the most influential techniques in deep learning. Key contributions:

- **Citations:** Over 100,000 citations (as of 2025)
- **Impact:** Batch Normalization is now a standard component in almost all modern neural network architectures
- **Innovation:** Addresses internal covariate shift by normalizing layer inputs during training
- **Results:** Achieves same accuracy 14x faster on ImageNet, surpassing human-level performance

## Translation Team
- Translator: Claude (Sonnet 4.5) - Session 2025-11-15
- Reviewer: TBD
- Started: 2025-11-15
- Completed: 2025-11-15

## Translation Statistics
- Total sections: 7
- Total equations: 20+
- Algorithms translated: 2
- Quality range: 0.87-0.90
- Average quality: 0.88
