# Dropout: A Simple Way to Prevent Neural Networks from Overfitting
## Dropout: طريقة بسيطة لمنع الإفراط في تدريب الشبكات العصبية

**arXiv ID:** 1207.0580 (preprint), JMLR 2014 (full paper)
**Authors:** Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov
**Year:** 2014
**Publication:** Journal of Machine Learning Research (JMLR), Volume 15, Pages 1929-1958
**Categories:** Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Computer Vision and Pattern Recognition (cs.CV)
**PDF:** https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf
**Alternative PDF:** https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf

**Abstract Translation Quality (from 1207.0580):** 0.91
**Full Paper Translation Quality:** 0.876 (Excellent)

## Citation

```bibtex
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR.org}
}
```

## Translation Team
- Translator: Claude (Sonnet 4.5)
- Started: 2025-11-15
- Completed: 2025-11-15

## Paper Summary
This landmark paper introduces dropout, one of the most influential regularization techniques in deep learning. The method randomly drops units (along with their connections) from neural networks during training to prevent overfitting and co-adaptation of features. The paper presents comprehensive experimental results across vision, speech, and text domains, along with theoretical analysis and practical guidelines for implementation.
