# Section 1: Introduction
## القسم 1: مقدمة

**Section:** introduction
**Translation Quality:** 0.88
**Glossary Terms Used:** information theory, communication, channel capacity, entropy, encoding, noise

---

### English Version

In 1948, Claude Shannon published a paper called A Mathematical Theory of Communication[1]. This paper heralded a transformation in our understanding of information. Before Shannon's paper, information had been viewed as a kind of poorly defined miasmic fluid. But after Shannon's paper, it became apparent that information is a well-defined and, above all, measurable quantity. Indeed, as noted by Shannon,

> A basic idea in information theory is that information can be treated very much like a physical quantity, such as mass or energy.
>
> Claude Shannon, 1985.

Information theory defines definite, unbreachable limits on precisely how much information can be communicated between any two components of any system, whether this system is man-made or natural. The theorems of information theory are so important that they deserve to be regarded as the laws of information[2, 3, 4].

The basic laws of information can be summarised as follows. For any communication channel (Figure 1): 1) there is a definite upper limit, the channel capacity, to the amount of information that can be communicated through that channel, 2) this limit shrinks as the amount of noise in the channel increases, 3) this limit can very nearly be reached by judicious packaging, or encoding, of data.

---

### النسخة العربية

في عام 1948، نشر كلود شانون ورقة بحثية بعنوان "نظرية رياضية للاتصال"[1]. بشّرت هذه الورقة بتحول في فهمنا للمعلومات. قبل ورقة شانون، كانت المعلومات تُعتبر نوعاً من السائل الضبابي سيء التعريف. ولكن بعد ورقة شانون، أصبح من الواضح أن المعلومات هي كمية محددة جيداً، وقبل كل شيء، قابلة للقياس. في الواقع، كما أشار شانون:

> فكرة أساسية في نظرية المعلومات هي أن المعلومات يمكن معاملتها تماماً مثل كمية فيزيائية، مثل الكتلة أو الطاقة.
>
> كلود شانون، 1985.

تُحدد نظرية المعلومات حدوداً واضحة لا يمكن تجاوزها بشأن مقدار المعلومات بالضبط التي يمكن نقلها بين أي مكونين من أي نظام، سواء كان هذا النظام من صنع الإنسان أو طبيعياً. إن نظريات نظرية المعلومات مهمة جداً لدرجة أنها تستحق أن تُعتبر قوانين المعلومات[2, 3, 4].

يمكن تلخيص القوانين الأساسية للمعلومات على النحو التالي. بالنسبة لأي قناة اتصال (الشكل 1): 1) هناك حد أقصى محدد، وهو سعة القناة، لكمية المعلومات التي يمكن نقلها عبر تلك القناة، 2) ينكمش هذا الحد مع زيادة كمية الضوضاء في القناة، 3) يمكن الاقتراب كثيراً من هذا الحد من خلال التعبئة الحكيمة، أو الترميز، للبيانات.

---

### Translation Notes

- **Figures referenced:** Figure 1 (communication channel: Message → Encoding → Channel + Noise → Decoding → Message)
- **Key terms introduced:**
  - A Mathematical Theory of Communication (نظرية رياضية للاتصال)
  - channel capacity (سعة القناة)
  - encoding (الترميز)
  - noise (الضوضاء)
  - laws of information (قوانين المعلومات)
- **Equations:** None in this section
- **Citations:** [1], [2], [3], [4]
- **Special handling:** Block quote from Shannon preserved in both languages

### Quality Metrics

- Semantic equivalence: 0.88
- Technical accuracy: 0.90
- Readability: 0.87
- Glossary consistency: 0.87
- **Overall section score:** 0.88

### Back-Translation

In 1948, Claude Shannon published a research paper titled "A Mathematical Theory of Communication"[1]. This paper heralded a transformation in our understanding of information. Before Shannon's paper, information was considered a kind of poorly defined foggy fluid. But after Shannon's paper, it became clear that information is a well-defined quantity, and above all, measurable. In fact, as Shannon noted:

> A basic idea in information theory is that information can be treated exactly like a physical quantity, such as mass or energy.
>
> Claude Shannon, 1985.

Information theory defines clear, insurmountable limits on exactly how much information can be transmitted between any two components of any system, whether that system is man-made or natural. The theorems of information theory are so important that they deserve to be considered laws of information[2, 3, 4].

The basic laws of information can be summarized as follows. For any communication channel (Figure 1): 1) there is a definite upper limit, the channel capacity, for the amount of information that can be transmitted through that channel, 2) this limit shrinks with the increase in the amount of noise in the channel, 3) this limit can be approached very closely through wise packaging, or encoding, of data.
