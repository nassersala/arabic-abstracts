---
# Language Models are Few-Shot Learners
## نماذج اللغة متعلمة من أمثلة قليلة

**arXiv ID:** 2005.14165
**Authors:** Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei
**Year:** 2020
**Categories:** cs.CL (Computation and Language)
**Translation Quality:** 0.88
**Glossary Terms Used:** language model, large language model, pre-training, fine-tuning, task-agnostic, few-shot, performance, parameters, gradient, dataset, translation, question-answering, reasoning, training, autoregressive, NLP, benchmark, architecture, corpus, domain adaptation, on-the-fly

### English Abstract
Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.

### الملخص العربي
أظهرت الأعمال الحديثة تحسينات كبيرة في العديد من مهام ومعايير معالجة اللغة الطبيعية من خلال التدريب المسبق على مدونة نصية كبيرة يتبعها ضبط دقيق على مهمة محددة. وعلى الرغم من أن هذه الطريقة تكون عادةً مستقلة عن المهمة من حيث المعمارية، إلا أنها لا تزال تتطلب مجموعات بيانات للضبط الدقيق خاصة بالمهمة تحتوي على آلاف أو عشرات الآلاف من الأمثلة. بالمقابل، يستطيع البشر عمومًا تنفيذ مهمة لغوية جديدة من عدد قليل فقط من الأمثلة أو من تعليمات بسيطة - وهو شيء لا تزال أنظمة معالجة اللغة الطبيعية الحالية تكافح إلى حد كبير للقيام به. في هذا البحث نُظهر أن زيادة حجم نماذج اللغة يحسّن بشكل كبير الأداء المستقل عن المهمة والقائم على أمثلة قليلة، وأحيانًا يصل حتى إلى مستوى المنافسة مع أساليب الضبط الدقيق المتقدمة السابقة. على وجه التحديد، قمنا بتدريب GPT-3، وهو نموذج لغة انحداري ذاتي يحتوي على 175 مليار معامل، أي 10 أضعاف أي نموذج لغة غير متفرق سابق، واختبرنا أداءه في إعداد التعلم من أمثلة قليلة. في جميع المهام، يُطبق GPT-3 دون أي تحديثات للتدرجات أو ضبط دقيق، حيث يتم تحديد المهام وعروض الأمثلة القليلة بشكل كامل عبر التفاعل النصي مع النموذج. يحقق GPT-3 أداءً قويًا في العديد من مجموعات بيانات معالجة اللغة الطبيعية، بما في ذلك الترجمة الآلية والإجابة على الأسئلة ومهام ملء الفراغات، بالإضافة إلى عدة مهام تتطلب استدلالًا فوريًا أو تكيفًا مع المجال، مثل إعادة ترتيب الكلمات المشوشة، أو استخدام كلمة جديدة في جملة، أو إجراء عمليات حسابية من ثلاثة أرقام. في الوقت نفسه، نحدد أيضًا بعض مجموعات البيانات التي لا يزال التعلم من أمثلة قليلة في GPT-3 يواجه صعوبة معها، بالإضافة إلى بعض مجموعات البيانات التي يواجه فيها GPT-3 مشكلات منهجية تتعلق بالتدريب على مدونات الويب الكبيرة. أخيرًا، نجد أن GPT-3 يمكنه توليد عينات من المقالات الإخبارية التي يجد المقيّمون البشريون صعوبة في تمييزها عن المقالات المكتوبة من قبل البشر. نناقش التأثيرات المجتمعية الأوسع لهذه النتيجة ولـ GPT-3 بشكل عام.

### Back-Translation (Validation)
Recent work has shown significant improvements in many natural language processing tasks and benchmarks through pre-training on a large text corpus followed by fine-tuning on a specific task. Although this method is typically task-agnostic in terms of architecture, it still requires task-specific fine-tuning datasets containing thousands or tens of thousands of examples. In contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current natural language processing systems still largely struggle to do. In this research, we demonstrate that scaling up language models significantly improves task-agnostic, few-shot performance, sometimes even reaching a competitive level with previous state-of-the-art fine-tuning approaches. Specifically, we trained GPT-3, an autoregressive language model with 175 billion parameters, which is 10 times larger than any previous non-sparse language model, and tested its performance in the few-shot learning setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, where tasks and few-shot demonstrations are specified entirely through text interaction with the model. GPT-3 achieves strong performance on many natural language processing datasets, including machine translation, question-answering, and cloze tasks, as well as several tasks requiring on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing three-digit arithmetic operations. At the same time, we also identify some datasets where few-shot learning in GPT-3 still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles that human evaluators have difficulty distinguishing from articles written by humans. We discuss the broader societal impacts of this finding and of GPT-3 in general.

### Translation Metrics
- Iterations: 1
- Final Score: 0.88
- Quality: High

### Notes
The translation maintains technical accuracy while using Modern Standard Arabic. All key technical terms from the glossary were applied consistently. The translation preserves the semantic meaning and structure of the original abstract, including mathematical notation (175 billion, 10x, 3-digit) and technical concepts. The back-translation confirms semantic equivalence with the original text.
---
