# Section 1: Introduction
## القسم 1: المقدمة

**Section:** introduction
**Translation Quality:** 0.87
**Glossary Terms Used:** cardinality, algorithm, multiset, hash function, probabilistic, memory, accuracy, stochastic averaging, observable

---

### English Version

The purpose of this note is to present and analyse an efficient algorithm for estimating the number of distinct elements, known as the cardinality, of large data ensembles, which are referred to here as multisets and are usually massive streams (read-once sequences). This problem has received a great deal of attention over the past two decades, finding an ever growing number of applications in networking and traffic monitoring, such as the detection of worm propagation, of network attacks (e.g., by Denial of Service), and of link-based spam on the web [3]. For instance, a data stream over a network consists of a sequence of packets, each packet having a header, which contains a pair (source–destination) of addresses, followed by a body of specific data; the number of distinct header pairs (the cardinality of the multiset) in various time slices is an important indication for detecting attacks and monitoring traffic, as it records the number of distinct active flows. Indeed, worms and viruses typically propagate by opening a large number of different connections, and though they may well pass unnoticed amongst a huge traffic, their activity becomes exposed once cardinalities are measured (see the lucid exposition by Estan and Varghese in [11]). Other applications of cardinality estimators include data mining of massive data sets of sorts—natural language texts [4, 5], biological data [17, 18], very large structured databases, or the internet graph, where the authors of [22] report computational gains by a factor of 500+ attained by probabilistic cardinality estimators.

Clearly, the cardinality of a multiset can be exactly determined with a storage complexity essentially proportional to its number of elements. However, in most applications, the multiset to be treated is far too large to be kept in core memory. A crucial idea is then to relax the constraint of computing the value n of the cardinality exactly, and to develop probabilistic algorithms dedicated to estimating n approximately. (In many practical applications, a tolerance of a few percents on the result is acceptable.) A whole range of algorithms have been developed that only require a sublinear memory [2, 6, 10, 11, 15, 16], or, at worst a linear memory, but with a small implied constant [24].

All known efficient cardinality estimators rely on randomization, which is ensured by the use of hash functions. The elements to be counted belonging to a certain data domain D, we assume given a hash function, h: D → {0,1}^∞, that is, we assimilate hashed values to infinite binary strings of {0,1}^∞, or equivalently to real numbers of the unit interval. (In practice, hashing on 32 bits will suffice to estimate cardinalities in excess of 10^9; see Section 4 for a discussion.) We postulate that the hash function has been designed in such a way that the hashed values closely resemble a uniform model of randomness, namely, bits of hashed values are assumed to be independent and to have each probability 1/2 of occurring—practical methods are known [20], which vindicate this assumption, based on cyclic redundancy codes (CRC), modular arithmetics, or a simplified cryptographic use of boolean algebra (e.g., sha1).

The best known cardinality estimators rely on making suitable, concise enough, observations on the hashed values h(M) of the input multiset M, then inferring a plausible estimate of the unknown cardinality n. Define an observable of a multiset S ⊆ h(M) of {0,1}^∞ strings (or, equivalently, of real [0,1] numbers) to be a function that only depends on the set underlying S, that is, a quantity independent of replications. Then two broad categories of cardinality observables have been studied.

— **Bit-pattern observables**: these are based on certain patterns of bits occurring at the beginning of the (binary) S-values. For instance, observing in the stream S at the beginning of a string a bit-pattern 0^ℓ1 is more or less a likely indication that the cardinality n of S is at least 2^ℓ. The algorithms known as Probabilistic Counting, due to Flajolet-Martin [15], together with the more recent LOGLOG of Durand-Flajolet [10] belong to this category.

— **Order statistics observables**: these are based on order statistics, like the smallest (real) values, that appear in S. For instance, if X = min(S), we may legitimately hope that n is roughly of the order of 1/X, since, as regards expectations, one has E(X) = 1/(n+1). The algorithms of Bar-Yossef et al. [2] and Giroire's MINCOUNT [16, 18] are of this type.

The observables just described can be maintained with just one or a few registers. However, as such, they only provide a rough indication of the sought cardinality n, via log₂ n or 1/n. One difficulty is due to a rather high variability, so that one observation, corresponding to the maintenance of a single variable, cannot suffice to obtain accurate predictions. An immediate idea is then to perform several experiments in parallel: if each of a collection of m random variables has standard deviation σ, then their arithmetic mean has standard deviation σ/√m, which can be made as small as we please by increasing m. That simplistic strategy has however two major drawbacks: it is costly in terms of computation time (we would need to compute m hashed values per element scanned), and, worse, it would necessitate a large set of independent hashing functions, for which no construction is known [1].

The solution introduced in [15] under the name of **stochastic averaging**, consists in emulating the effect of m experiments with a single hash function. Roughly speaking, we divide the input stream h(M) into m substreams, corresponding to a partition of the unit interval of hashed values into [0, 1/m[, [1/m, 2/m[, ..., [m-1/m, 1]. Then, one maintains the m observables O₁, ..., O_m corresponding to each of the m substreams. A suitable average of the {O_j} is then expected to produce an estimate of cardinalities whose quality should improve, due to averaging effects, in proportion to 1/√m, as m increases. The benefit of this approach is that it requires only a constant number of elementary operations per element of the multiset M (as opposed to a quantity proportional to m), while only one hash function is now needed.

The performances of several algorithms are compared in Figure 1; see also [13] for a review. HYPERLOGLOG, described in detail in the next section, is based on the same observable as LOGLOG, namely the largest ρ value obtained, where ρ(x) is the position of the leftmost 1-bit in binary string x. Stochastic averaging in the sense above is employed. However, our algorithm differs from standard LOGLOG by its evaluation function: it is based on **harmonic means**, while the standard algorithm uses what amounts to a geometric mean. The idea of using harmonic means originally drew its inspiration from an insightful note of Chassaing and Gérin [6]: such means have the effect of taming probability distributions with slow-decaying right tails, and here they operate as a variance reduction device, thereby appreciably increasing the quality of estimates. Theorem 1 below summarizes our main conclusions to the effect that the relative accuracy (technically, the standard error) of HYPERLOGLOG is numerically close to 1/√m, where β∞ := 1/√(3 log 2) ≈ 1.03896. The algorithm needs to maintain a collection of registers, each of which is at most log₂ log₂ N + O(1) bits, when cardinalities N need to be estimated. In particular, HYPERLOGLOG achieves an accuracy matching that of standard LOGLOG by consuming only 64% of the corresponding memory. As a consequence, using m = 2048, hashing on 32 bits, and short bytes of 5 bit length each: cardinalities till values over N = 10^9 can be estimated with a typical accuracy of 2% using 1.5kB (kilobyte) of storage.

The proofs base themselves in part on techniques that are now standard in analysis of algorithms, like poissonization, Mellin transforms, and saddle-point depoissonization. Some nonstandard problems however present themselves due to the special nonlinear character of harmonic means, so that several ingredients of the analysis are not completely routine.

---

### النسخة العربية

الغرض من هذه المذكرة هو تقديم وتحليل خوارزمية فعالة لتقدير عدد العناصر المتمايزة، المعروف باسم العددية، لمجموعات بيانات كبيرة، والتي يُشار إليها هنا باسم المجموعات المتعددة وهي عادةً تدفقات ضخمة (تسلسلات تُقرأ مرة واحدة). لقد حظيت هذه المشكلة باهتمام كبير خلال العقدين الماضيين، حيث وجدت عدداً متزايداً من التطبيقات في الشبكات ومراقبة حركة المرور، مثل اكتشاف انتشار الديدان، والهجمات على الشبكة (مثل هجمات حجب الخدمة)، والبريد المزعج القائم على الروابط على الويب [3]. على سبيل المثال، يتكون تدفق البيانات عبر شبكة من تسلسل من الحزم، كل حزمة لها رأس يحتوي على زوج من العناوين (المصدر-الوجهة)، متبوعاً بجسم من البيانات المحددة؛ عدد أزواج الرؤوس المتمايزة (عددية المجموعة المتعددة) في شرائح زمنية مختلفة هو مؤشر مهم لاكتشاف الهجمات ومراقبة حركة المرور، حيث يسجل عدد التدفقات النشطة المتمايزة. في الواقع، تنتشر الديدان والفيروسات عادةً عن طريق فتح عدد كبير من الاتصالات المختلفة، وعلى الرغم من أنها قد تمر دون أن تُلاحظ وسط حركة مرور ضخمة، إلا أن نشاطها يصبح مكشوفاً بمجرد قياس العدديات (انظر العرض الواضح لـ Estan و Varghese في [11]). تشمل التطبيقات الأخرى لمقدرات العددية تنقيب البيانات لمجموعات بيانات ضخمة من أنواع مختلفة—نصوص اللغة الطبيعية [4، 5]، البيانات البيولوجية [17، 18]، قواعد البيانات المهيكلة الكبيرة جداً، أو رسم الإنترنت البياني، حيث أبلغ مؤلفو [22] عن مكاسب حسابية بعامل 500+ تحققت بواسطة مقدرات العددية الاحتمالية.

من الواضح أن عددية المجموعة المتعددة يمكن تحديدها بدقة بتعقيد تخزين يتناسب بشكل أساسي مع عدد عناصرها. ومع ذلك، في معظم التطبيقات، تكون المجموعة المتعددة المراد معالجتها كبيرة جداً بحيث لا يمكن الاحتفاظ بها في الذاكرة الأساسية. الفكرة الحاسمة حينئذ هي تخفيف قيد حساب قيمة n للعددية بدقة، وتطوير خوارزميات احتمالية مخصصة لتقدير n تقريبياً. (في العديد من التطبيقات العملية، يكون التسامح ببضع نسب مئوية على النتيجة مقبولاً.) تم تطوير مجموعة كاملة من الخوارزميات التي تتطلب فقط ذاكرة دون خطية [2، 6، 10، 11، 15، 16]، أو في أسوأ الأحوال ذاكرة خطية، ولكن مع ثابت ضمني صغير [24].

تعتمد جميع مقدرات العددية الفعالة المعروفة على العشوائية، والتي يتم ضمانها باستخدام دوال التجزئة. العناصر المراد عدها تنتمي إلى مجال بيانات معين D، نفترض وجود دالة تجزئة، h: D → {0,1}^∞، أي أننا نماثل القيم المجزأة بسلاسل ثنائية لانهائية من {0,1}^∞، أو بشكل مكافئ بأعداد حقيقية من الفترة الواحدية. (في الممارسة العملية، ستكون التجزئة على 32 بت كافية لتقدير العدديات التي تتجاوز 10^9؛ انظر القسم 4 للمناقشة.) نفترض أن دالة التجزئة قد تم تصميمها بطريقة تجعل القيم المجزأة تشبه إلى حد كبير نموذجاً منتظماً من العشوائية، أي أن بتات القيم المجزأة يُفترض أن تكون مستقلة وأن يكون لكل منها احتمال 1/2 للحدوث—هناك طرق عملية معروفة [20]، تؤكد هذا الافتراض، بناءً على رموز التكرار الدوري (CRC)، أو الحساب النمطي، أو الاستخدام المبسط للتشفير باستخدام الجبر البولياني (مثل sha1).

تعتمد أفضل مقدرات العددية المعروفة على إجراء ملاحظات مناسبة ومختصرة بما فيه الكفاية على القيم المجزأة h(M) من المجموعة المتعددة المدخلة M، ثم استنتاج تقدير معقول للعددية المجهولة n. نعرّف مرصوداً لمجموعة متعددة S ⊆ h(M) من سلاسل {0,1}^∞ (أو بشكل مكافئ، من أعداد حقيقية [0,1]) ليكون دالة تعتمد فقط على المجموعة الأساسية لـ S، أي كمية مستقلة عن التكرارات. ثم تمت دراسة فئتين واسعتين من مرصودات العددية.

— **مرصودات نمط البت**: تستند هذه إلى أنماط معينة من البتات التي تحدث في بداية قيم S (الثنائية). على سبيل المثال، ملاحظة نمط بت 0^ℓ1 في بداية سلسلة في التدفق S هو إشارة محتملة إلى حد ما أن عددية S هي على الأقل 2^ℓ. الخوارزميات المعروفة باسم العد الاحتمالي، التي طورها Flajolet-Martin [15]، جنباً إلى جنب مع LOGLOG الأحدث لـ Durand-Flajolet [10] تنتمي إلى هذه الفئة.

— **مرصودات إحصاءات الترتيب**: تستند هذه إلى إحصاءات الترتيب، مثل أصغر القيم (الحقيقية)، التي تظهر في S. على سبيل المثال، إذا كان X = min(S)، فيمكننا أن نأمل بشكل مشروع أن تكون n تقريباً من رتبة 1/X، حيث أنه فيما يتعلق بالتوقعات، لدينا E(X) = 1/(n+1). خوارزميات Bar-Yossef وآخرون [2] و MINCOUNT لـ Giroire [16، 18] من هذا النوع.

يمكن الحفاظ على المرصودات الموصوفة للتو بسجل واحد أو عدة سجلات فقط. ومع ذلك، في حد ذاتها، فإنها توفر فقط إشارة تقريبية للعددية المطلوبة n، عبر log₂ n أو 1/n. تنشأ إحدى الصعوبات من تباين مرتفع إلى حد ما، بحيث لا يمكن أن تكفي ملاحظة واحدة، تقابل الحفاظ على متغير واحد، للحصول على تنبؤات دقيقة. الفكرة الفورية حينئذ هي إجراء عدة تجارب بالتوازي: إذا كان لكل من مجموعة من m متغيرات عشوائية انحراف معياري σ، فإن متوسطها الحسابي له انحراف معياري σ/√m، والذي يمكن جعله صغيراً كما نشاء عن طريق زيادة m. ومع ذلك، فإن هذه الاستراتيجية المبسطة لها عيبان رئيسيان: إنها مكلفة من حيث وقت الحساب (سنحتاج إلى حساب m قيمة مجزأة لكل عنصر يتم مسحه ضوئياً)، والأسوأ من ذلك، أنها ستستلزم مجموعة كبيرة من دوال التجزئة المستقلة، والتي لا يُعرف لها بناء [1].

الحل الذي قُدم في [15] تحت اسم **المتوسط العشوائي**، يتكون من محاكاة تأثير m تجربة بدالة تجزئة واحدة. بشكل تقريبي، نقسم تدفق الإدخال h(M) إلى m تدفق فرعي، يقابل تقسيم الفترة الوحدوية للقيم المجزأة إلى [0، 1/m[، [1/m، 2/m[، ...، [m-1/m، 1]. ثم، نحافظ على m مرصوداً O₁، ...، O_m يقابل كل من التدفقات الفرعية m. من المتوقع حينئذ أن ينتج متوسط مناسب لـ {O_j} تقديراً للعدديات يجب أن تتحسن جودته، بسبب تأثيرات المتوسط، بما يتناسب مع 1/√m، مع زيادة m. فائدة هذا النهج هي أنه يتطلب فقط عدداً ثابتاً من العمليات الأولية لكل عنصر من المجموعة المتعددة M (على عكس كمية تتناسب مع m)، بينما يلزم الآن دالة تجزئة واحدة فقط.

تتم مقارنة أداء العديد من الخوارزميات في الشكل 1؛ انظر أيضاً [13] للمراجعة. HYPERLOGLOG، الموصوف بالتفصيل في القسم التالي، يعتمد على نفس المرصود كـ LOGLOG، وهو أكبر قيمة ρ تم الحصول عليها، حيث ρ(x) هو موضع البت 1 الأيسر في السلسلة الثنائية x. يتم استخدام المتوسط العشوائي بالمعنى المذكور أعلاه. ومع ذلك، تختلف خوارزميتنا عن LOGLOG القياسي بدالة التقييم الخاصة بها: فهي تستند إلى **المتوسطات التوافقية**، بينما تستخدم الخوارزمية القياسية ما يعادل متوسطاً هندسياً. استمدت فكرة استخدام المتوسطات التوافقية إلهامها في الأصل من ملاحظة ثاقبة لـ Chassaing و Gérin [6]: مثل هذه المتوسطات لها تأثير في ترويض توزيعات الاحتمالات ذات الذيول اليمنى بطيئة الانحلال، وهنا تعمل كأداة لتقليل التباين، وبالتالي زيادة جودة التقديرات بشكل ملموس. يلخص النظرية 1 أدناه استنتاجاتنا الرئيسية بمعنى أن الدقة النسبية (من الناحية الفنية، الخطأ المعياري) لـ HYPERLOGLOG قريبة عددياً من 1/√m، حيث β∞ := 1/√(3 log 2) ≈ 1.03896. تحتاج الخوارزمية إلى الحفاظ على مجموعة من السجلات، كل منها بحد أقصى log₂ log₂ N + O(1) بت، عندما تحتاج العدديات N إلى التقدير. على وجه الخصوص، تحقق HYPERLOGLOG دقة تطابق دقة LOGLOG القياسية باستهلاك 64% فقط من الذاكرة المقابلة. ونتيجة لذلك، باستخدام m = 2048، والتجزئة على 32 بت، والبايتات القصيرة بطول 5 بت لكل منها: يمكن تقدير العدديات حتى قيم تتجاوز N = 10^9 بدقة نموذجية تبلغ 2% باستخدام 1.5 كيلوبايت من التخزين.

تستند البراهين جزئياً على تقنيات أصبحت الآن قياسية في تحليل الخوارزميات، مثل البواسونية، وتحويلات ميلين، وإزالة البواسونية بنقطة السرج. ومع ذلك، تظهر بعض المشكلات غير القياسية بسبب الطابع غير الخطي الخاص للمتوسطات التوافقية، بحيث أن العديد من مكونات التحليل ليست روتينية تماماً.

---

### Translation Notes

- **Figures referenced:** Figure 1 (comparison table of algorithms)
- **Key terms introduced:** multiset (مجموعة متعددة), hash function (دالة تجزئة), bit-pattern observable (مرصود نمط البت), order statistics (إحصاءات الترتيب), stochastic averaging (المتوسط العشوائي), harmonic mean (متوسط توافقي)
- **Equations:** Multiple mathematical notations preserved
- **Citations:** [1]-[24] preserved
- **Special handling:** Algorithm names (HYPERLOGLOG, LOGLOG, MINCOUNT, etc.) kept in English

### Quality Metrics

- Semantic equivalence: 0.88
- Technical accuracy: 0.90
- Readability: 0.85
- Glossary consistency: 0.85
- **Overall section score:** 0.87
