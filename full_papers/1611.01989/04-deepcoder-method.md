# Section 4: DeepCoder (Method)
## القسم 4: DeepCoder (المنهجية)

**Section:** DeepCoder Method
**Translation Quality:** 0.87
**Glossary Terms Used:** DSL, encoder, decoder, embedding, neural network, feed-forward, hidden layer, sigmoid, one-hot encoding, depth-first search, enumeration, SMT solver, cross entropy, marginal probability

---

### English Version

Here we describe DeepCoder, our instantiation of LIPS including a choice of DSL, a data generation strategy, models for encoding input-output sets, and algorithms for searching over program space.

#### 4.1 Domain Specific Language and Attributes

We consider binary attributes indicating the presence or absence of high-level functions in the target program. To make this effective, the chosen DSL needs to contain constructs that are not so low-level that they all appear in the vast majority of programs, but at the same time should be common enough so that predicting their occurrence from input-output examples can be learned successfully.

Following this observation, our DSL is loosely inspired by query languages such as SQL or LINQ, where high-level functions are used in sequence to manipulate data. A program in our DSL is a sequence of function calls, where the result of each call initializes a fresh variable that is either a singleton integer or an integer array. Functions can be applied to any of the inputs or previously computed (intermediate) variables. The output of the program is the return value of the last function call, i.e., the last variable. See Fig. 1 for an example program of length T = 4 in our DSL.

```
a ← [int]                      An input-output example:
b ← FILTER (<0) a              Input:
c ← MAP (*4) b                 [-17, -3, 4, 11, 0, -5, -9, 13, 6, 6, -8, 11]
d ← SORT c                     Output:
e ← REVERSE d                  [-12, -20, -32, -36, -68]
```

**Figure 1:** An example program in our DSL that takes a single integer array as its input.

Overall, our DSL contains the first-order functions HEAD, LAST, TAKE, DROP, ACCESS, MINIMUM, MAXIMUM, REVERSE, SORT, SUM, and the higher-order functions MAP, FILTER, COUNT, ZIPWITH, SCANL1. Higher-order functions require suitable lambda functions for their behavior to be fully specified: for MAP our DSL provides lambdas (+1), (-1), (*2), (/2), (*(-1)), (**2), (*3), (/3), (*4), (/4); for FILTER and COUNT there are predicates (>0), (<0), (%2==0), (%2==1) and for ZIPWITH and SCANL1 the DSL provides lambdas (+), (-), (*), MIN, MAX. A description of the semantics of all functions is provided in Appendix F.

Note that while the language only allows linear control flow, many of its functions do perform branching and looping internally (e.g., SORT, COUNT, ...). Examples of more sophisticated programs expressible in our DSL, which were inspired by the simplest problems appearing on programming competition websites, are shown in Appendix A.

#### 4.2 Data Generation

To generate a dataset, we enumerate programs in the DSL, heuristically pruning away those with easily detectable issues such as a redundant variable whose value does not affect the program output, or, more generally, existence of a shorter equivalent program (equivalence can be overapproximated by identical behavior on randomly or carefully chosen inputs). To generate valid inputs for a program, we enforce a constraint on the output value bounding integers to some predetermined range, and then propagate these constraints backward through the program to obtain a range of valid values for each input. If one of these ranges is empty, we discard the program. Otherwise, input-output pairs can be generated by picking inputs from the pre-computed valid ranges and executing the program to obtain the output values. The binary attribute vectors are easily computed from the program source codes.

#### 4.3 Machine Learning Model

Observe how the input-output data in Fig. 1 is informative of the functions appearing in the program: the values in the output are all negative, divisible by 4, they are sorted in decreasing order, and they happen to be multiples of numbers appearing in the input. Our aim is to learn to recognize such patterns in the input-output examples, and to leverage them to predict the presence or absence of individual functions. We employ neural networks to model and learn the mapping from input-output examples to attributes. We can think of these networks as consisting of two parts:

1. an **encoder**: a differentiable mapping from a set of M input-output examples generated by a single program to a latent real-valued vector, and

2. a **decoder**: a differentiable mapping from the latent vector representing a set of M input-output examples to predictions of the ground truth program's attributes.

For the encoder we use a simple feed-forward architecture. First, we represent the input and output types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum length L with a special NULL value. Second, each integer in the inputs and in the output is mapped to a learned embedding vector of size E = 20. (The range of integers is restricted to a finite range and each embedding is parametrized individually.) Third, for each input-output example separately, we concatenate the embeddings of the input types, the inputs, the output type, and the output into a single (fixed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each. The third hidden layer thus provides an encoding of each individual input-output example. Finally, for input-output examples in a set generated from the same program, we pool these representations together by simple arithmetic averaging. See Appendix C for more details.

The advantage of this encoder lies in its simplicity, and we found it reasonably easy to train. A disadvantage is that it requires an upper bound L on the length of arrays appearing in the input and output. We confirmed that the chosen encoder architecture is sensible in that it performs empirically at least as well as an RNN encoder, a natural baseline, which may however be more difficult to train.

DeepCoder learns to predict presence or absence of individual functions of the DSL. We shall see this can already be exploited by various search techniques to large computational gains. We use a decoder that pre-multiplies the encoding of input-output examples by a learned C ×K matrix, where C = 34 is the number of functions in our DSL (higher-order functions and lambdas are predicted independently), and treats the resulting C numbers as log-unnormalized probabilities (logits) of each function appearing in the source code. Fig. 2 shows the predictions a trained neural network made from 5 input-output examples for the program shown in Fig. 1.

**Figure 2:** Neural network predicts the probability of each function appearing in the source code.

#### 4.4 Search

One of the central ideas of this work is to use a neural network to guide the search for a program consistent with a set of input-output examples instead of directly predicting the entire source code. This section briefly describes the search techniques and how they integrate the predicted attributes.

**Depth-first search (DFS).** We use an optimized version of DFS to search over programs with a given maximum length T (see Appendix D for details). When the search procedure extends a partial program by a new function, it has to try the functions in the DSL in some order. At this point DFS can opt to consider the functions as ordered by their predicted probabilities from the neural network.

**"Sort and add" enumeration.** A stronger way of utilizing the predicted probabilities of functions in an enumerative search procedure is to use a Sort and add scheme, which maintains a set of active functions and performs DFS with the active function set only. Whenever the search fails, the next most probable function (or several) are added to the active set and the search restarts with this larger active set. Note that this scheme has the deficiency of potentially re-exploring some parts of the search space several times, which could be avoided by a more sophisticated search procedure.

**Sketch.** Sketch (Solar-Lezama, 2008) is a successful SMT-based program synthesis tool from the programming languages research community. While its main use case is to synthesize programs by filling in "holes" in incomplete source code so as to match specified requirements, it is flexible enough for our use case as well. The function in each step and its arguments can be treated as the "holes", and the requirement to be satisfied is consistency with the provided set of input-output examples. Sketch can utilize the neural network predictions in a Sort and add scheme as described above, as the possibilities for each function hole can be restricted to the current active set.

**λ2.** λ2 (Feser et al., 2015) is a program synthesis tool from the programming languages community that combines enumerative search with deduction to prune the search space. It is designed to infer small functional programs for data structure manipulation from input-output examples, by combining functions from a provided library. λ2 can be used in our framework using a Sort and add scheme as described above by choosing the library of functions according to the neural network predictions.

#### 4.5 Training Loss Function

We use the negative cross entropy loss to train the neural network described in Sect. 4.3, so that its predictions about each function can be interpreted as marginal probabilities. The LIPS framework dictates learning q(a | E), the joint distribution of all attributes a given the input-output examples, and it is not clear a priori how much DeepCoder loses by ignoring correlations between functions. However, under the simplifying assumption that the runtime of searching for a program of length T with C functions made available to a search routine is proportional to C^T, the following result for Sort and add procedures shows that their runtime can be optimized using marginal probabilities.

**Lemma 1.** For any fixed program length T, the expected total runtime of a Sort and add search scheme can be upper bounded by a quantity that is minimized by adding the functions in the order of decreasing true marginal probabilities.

**Proof.** Predicting source code functions from input-output examples can be seen as a multi-label classification problem, where each set of input-output examples is associated with a set of relevant labels (functions appearing in the ground truth source code). Dembczynski et al. (2010) showed that in multi-label classification under a so-called Rank loss, it is Bayes optimal to rank the labels according to their marginal probabilities. If the runtime of search with C functions is proportional to C^T, the total runtime of a Sort and add procedure can be monotonically transformed so that it is upper bounded by this Rank loss. See Appendix E for more details.

---

### النسخة العربية

هنا نصف DeepCoder، تجسيدنا لـ LIPS بما في ذلك اختيار اللغة الخاصة بالمجال، واستراتيجية توليد البيانات، ونماذج لترميز مجموعات الإدخال والإخراج، وخوارزميات للبحث عبر فضاء البرامج.

#### 4.1 اللغة الخاصة بالمجال والخصائص

نعتبر خصائص ثنائية تشير إلى وجود أو غياب الدوال عالية المستوى في البرنامج المستهدف. لجعل هذا فعالاً، تحتاج اللغة الخاصة بالمجال المختارة إلى احتواء تراكيب ليست منخفضة المستوى بحيث تظهر جميعها في الغالبية العظمى من البرامج، ولكن في نفس الوقت يجب أن تكون شائعة بما يكفي بحيث يمكن تعلم التنبؤ بحدوثها من أمثلة الإدخال والإخراج بنجاح.

بناءً على هذه الملاحظة، فإن لغتنا الخاصة بالمجال مستوحاة بشكل فضفاض من لغات الاستعلام مثل SQL أو LINQ، حيث تُستخدم الدوال عالية المستوى بشكل متسلسل لمعالجة البيانات. البرنامج في لغتنا الخاصة بالمجال هو تسلسل من استدعاءات الدوال، حيث تهيئ نتيجة كل استدعاء متغيراً جديداً يكون إما عدداً صحيحاً مفرداً أو مصفوفة من الأعداد الصحيحة. يمكن تطبيق الدوال على أي من المدخلات أو المتغيرات المحسوبة سابقاً (الوسيطة). مخرج البرنامج هو القيمة المرجعة من آخر استدعاء دالة، أي المتغير الأخير. انظر الشكل 1 لمثال برنامج بطول T = 4 في لغتنا الخاصة بالمجال.

```
a ← [int]                      مثال إدخال-إخراج:
b ← FILTER (<0) a              المدخل:
c ← MAP (*4) b                 [-17, -3, 4, 11, 0, -5, -9, 13, 6, 6, -8, 11]
d ← SORT c                     المخرج:
e ← REVERSE d                  [-12, -20, -32, -36, -68]
```

**الشكل 1:** مثال برنامج في لغتنا الخاصة بالمجال يأخذ مصفوفة من الأعداد الصحيحة كمدخل.

بشكل عام، تحتوي لغتنا الخاصة بالمجال على الدوال من الدرجة الأولى HEAD، LAST، TAKE، DROP، ACCESS، MINIMUM، MAXIMUM، REVERSE، SORT، SUM، والدوال من الرتبة الأعلى MAP، FILTER، COUNT، ZIPWITH، SCANL1. تتطلب الدوال من الرتبة الأعلى دوال لامبدا مناسبة لتحديد سلوكها بالكامل: لـ MAP توفر لغتنا الخاصة بالمجال دوال لامبدا (+1)، (-1)، (*2)، (/2)، (*(-1))، (**2)، (*3)، (/3)، (*4)، (/4)؛ لـ FILTER و COUNT هناك محمولات (>0)، (<0)، (%2==0)، (%2==1) ولـ ZIPWITH و SCANL1 توفر اللغة الخاصة بالمجال دوال لامبدا (+)، (-)، (*)، MIN، MAX. يتم توفير وصف لدلالات جميع الدوال في الملحق F.

لاحظ أنه بينما تسمح اللغة فقط بتدفق تحكم خطي، فإن العديد من دوالها تؤدي التفرع والحلقات داخلياً (على سبيل المثال، SORT، COUNT، ...). أمثلة على البرامج الأكثر تطوراً القابلة للتعبير في لغتنا الخاصة بالمجال، والتي استُلهمت من أبسط المشاكل الظاهرة على مواقع مسابقات البرمجة، موضحة في الملحق A.

#### 4.2 توليد البيانات

لتوليد مجموعة بيانات، نعدد البرامج في اللغة الخاصة بالمجال، مع التقليم الاستدلالي لتلك التي بها مشاكل يمكن اكتشافها بسهولة مثل متغير زائد عن الحاجة لا تؤثر قيمته على مخرج البرنامج، أو بشكل أعم، وجود برنامج مكافئ أقصر (يمكن التقريب الزائد للتكافؤ بسلوك متطابق على مدخلات مختارة عشوائياً أو بعناية). لتوليد مدخلات صالحة لبرنامج، نفرض قيداً على قيمة المخرج لحصر الأعداد الصحيحة في نطاق محدد مسبقاً، ثم ننشر هذه القيود للخلف عبر البرنامج للحصول على نطاق من القيم الصالحة لكل مدخل. إذا كان أحد هذه النطاقات فارغاً، نتجاهل البرنامج. وإلا، يمكن توليد أزواج الإدخال والإخراج عن طريق اختيار المدخلات من النطاقات الصالحة المحسوبة مسبقاً وتنفيذ البرنامج للحصول على قيم المخرجات. يتم حساب متجهات الخصائص الثنائية بسهولة من أكواد البرامج المصدرية.

#### 4.3 نموذج تعلم الآلة

لاحظ كيف أن بيانات الإدخال والإخراج في الشكل 1 تحتوي على معلومات عن الدوال الظاهرة في البرنامج: القيم في المخرج كلها سالبة، قابلة للقسمة على 4، مرتبة بترتيب تنازلي، وتصادف أنها مضاعفات لأرقام تظهر في المدخل. هدفنا هو تعلم التعرف على مثل هذه الأنماط في أمثلة الإدخال والإخراج، والاستفادة منها للتنبؤ بوجود أو غياب الدوال الفردية. نستخدم الشبكات العصبية لنمذجة وتعلم التعيين من أمثلة الإدخال والإخراج إلى الخصائص. يمكننا التفكير في هذه الشبكات على أنها تتكون من جزأين:

1. **مشفر**: تعيين قابل للاشتقاق من مجموعة من M أمثلة الإدخال والإخراج المولدة بواسطة برنامج واحد إلى متجه كامن ذو قيم حقيقية، و

2. **فك تشفير**: تعيين قابل للاشتقاق من المتجه الكامن الذي يمثل مجموعة من M أمثلة الإدخال والإخراج إلى تنبؤات لخصائص برنامج الحقيقة الأرضية.

للمشفر نستخدم معمارية تغذية أمامية بسيطة. أولاً، نمثل أنواع المدخلات والمخرجات (مفرد أو مصفوفة) بترميز أحادي الفعالية، ونملأ المدخلات والمخرجات إلى طول أقصى L بقيمة NULL خاصة. ثانياً، يتم تعيين كل عدد صحيح في المدخلات وفي المخرج إلى متجه تضمين متعلم بحجم E = 20. (نطاق الأعداد الصحيحة مقيد بنطاق محدود وكل تضمين معامَل بشكل فردي.) ثالثاً، لكل مثال إدخال-إخراج على حدة، نربط تضمينات أنواع المدخلات والمدخلات ونوع المخرج والمخرج في متجه واحد (بطول ثابت)، ونمرر هذا المتجه عبر H = 3 طبقات مخفية تحتوي على K = 256 وحدة سيجمويد لكل منها. توفر الطبقة المخفية الثالثة بالتالي ترميزاً لكل مثال إدخال-إخراج فردي. أخيراً، لأمثلة الإدخال والإخراج في مجموعة مولدة من نفس البرنامج، نجمع هذه التمثيلات معاً عن طريق المتوسط الحسابي البسيط. انظر الملحق C لمزيد من التفاصيل.

تكمن ميزة هذا المشفر في بساطته، ووجدنا أنه سهل التدريب بشكل معقول. العيب هو أنه يتطلب حداً أعلى L على طول المصفوفات الظاهرة في المدخل والمخرج. أكدنا أن معمارية المشفر المختارة منطقية من حيث أنها تؤدي تجريبياً على الأقل بنفس جودة مشفر RNN، وهو خط أساس طبيعي، والذي قد يكون مع ذلك أكثر صعوبة في التدريب.

يتعلم DeepCoder التنبؤ بوجود أو غياب الدوال الفردية للغة الخاصة بالمجال. سنرى أن هذا يمكن استغلاله بالفعل من قبل تقنيات البحث المختلفة لتحقيق مكاسب حسابية كبيرة. نستخدم فك تشفير يضرب مسبقاً ترميز أمثلة الإدخال والإخراج بمصفوفة C ×K متعلمة، حيث C = 34 هو عدد الدوال في لغتنا الخاصة بالمجال (يتم التنبؤ بالدوال من الرتبة الأعلى ودوال لامبدا بشكل مستقل)، ويعامل الأرقام C الناتجة كاحتماليات لوغاريتمية غير معيارية (logits) لظهور كل دالة في الكود المصدري. يُظهر الشكل 2 التنبؤات التي قامت بها شبكة عصبية مدربة من 5 أمثلة إدخال-إخراج للبرنامج الموضح في الشكل 1.

**الشكل 2:** تتنبأ الشبكة العصبية باحتمالية ظهور كل دالة في الكود المصدري.

#### 4.4 البحث

إحدى الأفكار المحورية لهذا العمل هي استخدام شبكة عصبية لتوجيه البحث عن برنامج متسق مع مجموعة من أمثلة الإدخال والإخراج بدلاً من التنبؤ المباشر بالكود المصدري بالكامل. يصف هذا القسم بإيجاز تقنيات البحث وكيفية دمج الخصائص المتنبأ بها.

**البحث بالعمق أولاً (DFS).** نستخدم نسخة محسنة من DFS للبحث عبر البرامج بطول أقصى معطى T (انظر الملحق D للتفاصيل). عندما يمد إجراء البحث برنامجاً جزئياً بدالة جديدة، يجب أن يجرب الدوال في اللغة الخاصة بالمجال بترتيب ما. في هذه المرحلة يمكن لـ DFS اختيار النظر في الدوال مرتبة حسب احتمالاتها المتنبأ بها من الشبكة العصبية.

**تعداد "الفرز والإضافة".** طريقة أقوى للاستفادة من الاحتماليات المتنبأ بها للدوال في إجراء بحث تعدادي هي استخدام مخطط الفرز والإضافة، الذي يحافظ على مجموعة من الدوال النشطة ويؤدي DFS بمجموعة الدوال النشطة فقط. كلما فشل البحث، تُضاف الدالة الأكثر احتمالاً التالية (أو عدة) إلى المجموعة النشطة ويعيد البحث التشغيل مع هذه المجموعة النشطة الأكبر. لاحظ أن هذا المخطط له عيب إعادة استكشاف بعض أجزاء فضاء البحث عدة مرات محتملاً، والذي يمكن تجنبه بإجراء بحث أكثر تطوراً.

**Sketch.** Sketch (Solar-Lezama، 2008) هو أداة توليد برامج قائمة على SMT ناجحة من مجتمع أبحاث لغات البرمجة. بينما حالة استخدامها الرئيسية هي توليد البرامج من خلال ملء "الثقوب" في كود مصدري غير كامل بحيث يطابق المتطلبات المحددة، فهي مرنة بما يكفي لحالة استخدامنا أيضاً. يمكن معاملة الدالة في كل خطوة ومعاملاتها على أنها "الثقوب"، والمتطلب الذي يجب إرضاؤه هو التناسق مع مجموعة أمثلة الإدخال والإخراج المقدمة. يمكن لـ Sketch الاستفادة من تنبؤات الشبكة العصبية في مخطط الفرز والإضافة كما هو موضح أعلاه، حيث يمكن تقييد الاحتماليات لكل ثقب دالة إلى المجموعة النشطة الحالية.

**λ2.** λ2 (Feser وآخرون، 2015) هي أداة توليد برامج من مجتمع لغات البرمجة تجمع بين البحث التعدادي والاستنتاج لتقليم فضاء البحث. تم تصميمها لاستنتاج برامج وظيفية صغيرة لمعالجة بنية البيانات من أمثلة الإدخال والإخراج، من خلال دمج الدوال من مكتبة مقدمة. يمكن استخدام λ2 في إطار عملنا باستخدام مخطط الفرز والإضافة كما هو موضح أعلاه عن طريق اختيار مكتبة الدوال وفقاً لتنبؤات الشبكة العصبية.

#### 4.5 دالة خسارة التدريب

نستخدم خسارة الإنتروبيا المتقاطعة السالبة لتدريب الشبكة العصبية الموصوفة في القسم 4.3، بحيث يمكن تفسير تنبؤاتها حول كل دالة كاحتماليات هامشية. يملي إطار LIPS تعلم q(a | E)، التوزيع المشترك لجميع الخصائص a بالنظر إلى أمثلة الإدخال والإخراج، وليس من الواضح مسبقاً مقدار ما يخسره DeepCoder بتجاهل الارتباطات بين الدوال. ومع ذلك، تحت الافتراض المبسط أن وقت تشغيل البحث عن برنامج بطول T مع C دوال متاحة لروتين بحث يتناسب مع C^T، فإن النتيجة التالية لإجراءات الفرز والإضافة تُظهر أن وقت تشغيلها يمكن تحسينه باستخدام الاحتماليات الهامشية.

**المبرهنة 1.** لأي طول برنامج ثابت T، يمكن حصر وقت التشغيل الإجمالي المتوقع لمخطط بحث الفرز والإضافة بكمية يتم تصغيرها عن طريق إضافة الدوال بترتيب الاحتماليات الهامشية الحقيقية المتناقصة.

**البرهان.** يمكن رؤية التنبؤ بدوال الكود المصدري من أمثلة الإدخال والإخراج كمشكلة تصنيف متعدد التسميات، حيث ترتبط كل مجموعة من أمثلة الإدخال والإخراج بمجموعة من التسميات ذات الصلة (الدوال الظاهرة في كود المصدر للحقيقة الأرضية). أظهر Dembczynski وآخرون (2010) أنه في التصنيف متعدد التسميات تحت ما يسمى بخسارة الرتبة، فإنه من الأمثل بايزياً ترتيب التسميات وفقاً لاحتمالاتها الهامشية. إذا كان وقت تشغيل البحث مع C دوال يتناسب مع C^T، فيمكن تحويل وقت التشغيل الإجمالي لإجراء الفرز والإضافة بشكل رتيب بحيث يكون محصوراً بهذه خسارة الرتبة. انظر الملحق E لمزيد من التفاصيل.

---

### Translation Notes

- **Key terms introduced:**
  - binary attribute = خاصية ثنائية
  - first-order function = دالة من الدرجة الأولى
  - higher-order function = دالة من الرتبة الأعلى
  - lambda function = دالة لامبدا
  - predicate = محمول
  - feed-forward architecture = معمارية تغذية أمامية
  - one-hot encoding = ترميز أحادي الفعالية
  - embedding vector = متجه التضمين
  - hidden layer = طبقة مخفية
  - pooling = التجميع
  - logit = احتمالية لوغاريتمية غير معيارية
  - depth-first search (DFS) = البحث بالعمق أولاً
  - Sort and add = الفرز والإضافة
  - cross entropy = الإنتروبيا المتقاطعة
  - marginal probability = احتمالية هامشية
  - multi-label classification = تصنيف متعدد التسميات
  - Rank loss = خسارة الرتبة

- **Figures referenced:** Figure 1 (example program), Figure 2 (neural network predictions)
- **Mathematical notation:** Preserved (C^T, q(a | E), E = 20, H = 3, K = 256, etc.)
- **Equations:** Preserved mathematical formulas and symbols
- **Citations:** Solar-Lezama (2008), Feser et al. (2015), Dembczynski et al. (2010)
- **Special handling:**
  - Function names preserved in English (FILTER, MAP, SORT, etc.)
  - System names preserved (Sketch, λ2)
  - Code example preserved with Arabic caption

### Quality Metrics

- Semantic equivalence: 0.88
- Technical accuracy: 0.89
- Readability: 0.86
- Glossary consistency: 0.87
- **Overall section score:** 0.87
