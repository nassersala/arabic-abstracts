# Federated Optimization in Heterogeneous Networks
## التحسين الاتحادي في الشبكات غير المتجانسة

**arXiv ID:** 1812.06127
**Authors:** Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith
**Year:** 2020
**Publication:** Proceedings of Machine Learning and Systems (MLSys) 2020
**Categories:** Machine Learning (cs.LG); Machine Learning (stat.ML)
**DOI:** N/A
**PDF:** https://arxiv.org/pdf/1812.06127.pdf

**Abstract Translation Quality:** 0.92 (from translations/)
**Full Paper Translation Quality:** 0.88 (meets ≥0.85 threshold)

## Citation

```bibtex
@inproceedings{li2020federated,
  title={Federated Optimization in Heterogeneous Networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  booktitle={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}
```

## Translation Team
- Translator: Claude Sonnet 4.5 (Session 2025-11-15)
- Reviewer: TBD
- Started: 2025-11-15
- Completed: 2025-11-15

## Paper Summary

This paper introduces FedProx, a framework for federated learning that addresses both systems heterogeneity (varying device capabilities) and statistical heterogeneity (non-IID data distributions). FedProx generalizes the popular FedAvg algorithm by allowing variable amounts of local work and incorporating a proximal term to stabilize convergence. The paper provides theoretical convergence guarantees and demonstrates 22% average test accuracy improvements over FedAvg in highly heterogeneous settings.
