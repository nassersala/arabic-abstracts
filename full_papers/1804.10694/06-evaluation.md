# Section 6: Evaluation
## القسم 6: التقييم

**Section:** evaluation
**Translation Quality:** 0.87
**Glossary Terms Used:** benchmark, performance, optimization, vectorization, unrolling, Intel MKL, cuBLAS, multicore CPU, GPU, distributed system, image processing, deep learning, linear algebra, speedup, kernel, fusion, scaling, strong scaling

---

### English Version

We evaluate TIRAMISU on two sets of benchmarks. The first is a set of deep learning and linear algebra benchmarks. The second is a set of image processing benchmarks.

We performed the evaluation on a cluster of 16 nodes. Each node is a dual-socket machine with two 24-core Intel Xeon E5-2680v3 CPUs, 128 GB RAM, Ubuntu 14.04, and an Infiniband interconnect. We use the MVAPICH2 2.0 [25] implementation of MPI for the distributed tests. The multicore experiments (CPU) are performed on one of these nodes. GPU experiments are performed on an NVIDIA Tesla K40 with 12 GB of RAM. Each experiment is repeated 30× and the median time is reported.

**A. Deep Learning and Linear Algebra Benchmarks**

We evaluated TIRAMISU by implementing a set of deep learning and linear algebra benchmarks, including Conv (a direct implementation of a neural network convolution layer), VGG (a block of a VGG neural network), and sgemm (matrix multiplication used to implement convolutions), HPCG (a benchmark for multigrid preconditioned conjugate gradient, CG), and Baryon (a dense tensor contraction code for constructing Baryon Building Blocks [16]). For all of these benchmarks, we compare the TIRAMISU implementation with Intel MKL, except for HPCG and Baryon, where we compare TIRAMISU with reference implementations. Figure 5 shows a comparison between the performance of CPU code generated by Tiramisu and reference code. For sgemm and HPCG we use matrices of size 1060 × 1060 and vectors of size 1060 while for Conv and VGG we use 512 × 512 as the data input size, 16 as the number of input/output features and a batch size of 32. For Baryon, we use the same tensor sizes as in the reference code.

For sgemm, TIRAMISU matches the performance of Intel MKL. sgemm is interesting in particular because the Intel MKL implementation of this kernel is well-known for its hand-optimized performance. We used a large set of optimizations to match Intel MKL. These optimizations include two-level blocking of the three-dimensional sgemm loop, vectorization, unrolling, array packing, register blocking, and separation of full and partial tiles (which is crucial to enable vectorization, unrolling, and reducing control overhead). We also used auto-tuning to find the best tile size and unrolling factor for the machine on which we run our experiments.

For the Conv kernel, TIRAMISU outperforms the Intel MKL implementation because the TIRAMISU-generated code uses a fixed size for the convolution filter. We generate specialized versions for common convolution filter sizes (3 × 3, 5 × 5, 7×7, 9×9 and 11×11). This allows the TIRAMISU compiler to apply optimizations that Intel MKL does not perform; for example this allows TIRAMISU to unroll the innermost (convolution filter) loops since their size is known at compile time. In VGG, TIRAMISU fuses the two convolution loops of the VGG block, which improves data locality. In addition, we generate code with fixed sizes for convolution filters (as we did in Conv). This provides 2.3× speedup over Intel MKL.

The TIRAMISU speedup over the Baryon reference code is achieved through vectorization, but this vectorization is not trivial since it requires the application of array expansion and then the use of scatter/gather operations, which are both not implemented in the reference Baryon code.

**B. Image Processing Benchmarks**

We used the following image processing benchmarks in our evaluation: edgeDetector, a ring blur followed by Roberts edge detection [41]; cvtColor, which converts an RGB image to grayscale; conv2D, a simple 2D convolution; warpAffine, which does affine warping on an image; gaussian, which performs a gaussian blur; nb, a synthetic pipeline composed of 4 stages that computes a negative and a brightened image from the same input image; and ticket #2373, a code snippet from a bug filed against Halide.

We used a 2112 × 3520 RGB input image for the experiments. We compare TIRAMISU with two other compilers: Halide [39], an industrial-quality DSL for image processing that has a scheduling language, and PENCIL [3], a state-of-the-art fully automatic polyhedral compiler.

Figure 6 compares the normalized execution time of code generated by TIRAMISU to other state-of-the-art frameworks on three architectures: single-node multicore, GPU and distributed (16 nodes).

**a) Single-node multicore:** In four of the benchmarks, the performance of the code generated by TIRAMISU matches the performance of Halide. Two of the other benchmarks, edgeDetector and ticket #2373, cannot be implemented in Halide. edgeDetector creates a cyclic dependence graph, which Halide cannot express. In ticket #2373, which exhibits a triangular iteration domain, Halide's bounds inference over-approximates the computed bounds, which leads the generated code to fail.

For nb, the code generated from TIRAMISU achieves 3.77× speedup over the Halide-generated code. This is primarily due to loop fusion.

The slowdown of the PENCIL compiler in gaussian is due to a suboptimal decision made by PENCIL regarding loop interchange versus spatial locality trade-offs.

**b) GPU:** For the GPU backend, the reported times are the total execution times (data copy and kernel execution). Code generated by TIRAMISU for conv2D and gaussian is faster than that of Halide because code generated by TIRAMISU uses constant memory to store the weights array.

**c) Distributed:** We assume the data are already distributed across the nodes by rows. Figure 6 compares the execution time of distributed TIRAMISU and distributed Halide. TIRAMISU is faster than distributed Halide in each case. It achieves up to 3.25× speedup for conv2D.

For the kernels involving communication, code generated by distributed Halide has two problems compared to TIRAMISU: distributed Halide overestimates the amount of data it needs to send, and unnecessarily packs together contiguous data into a separate buffer before sending.

Figure 7 shows the speedup of the kernels with distributed TIRAMISU when running on 2, 4, 8, and 16 nodes. This graph shows that distributed code generated from TIRAMISU scales well as the number of nodes increases (strong scaling).

---

### النسخة العربية

نقيّم TIRAMISU على مجموعتين من المعايير. الأولى هي مجموعة من معايير التعلم العميق والجبر الخطي. والثانية هي مجموعة من معايير معالجة الصور.

أجرينا التقييم على مجموعة من 16 عقدة. كل عقدة هي آلة ثنائية المقبس مع اثنين من وحدات المعالجة المركزية Intel Xeon E5-2680v3 ذات 24 نواة، وذاكرة وصول عشوائي 128 جيجابايت، وUbuntu 14.04، وترابط Infiniband. نستخدم تطبيق MVAPICH2 2.0 [25] لـ MPI للاختبارات الموزعة. يتم إجراء تجارب متعددة الأنوية (CPU) على إحدى هذه العقد. يتم إجراء تجارب GPU على NVIDIA Tesla K40 مع 12 جيجابايت من ذاكرة الوصول العشوائي. يتم تكرار كل تجربة 30 مرة ويتم الإبلاغ عن الوقت المتوسط.

**أ. معايير التعلم العميق والجبر الخطي**

قيّمنا TIRAMISU من خلال تنفيذ مجموعة من معايير التعلم العميق والجبر الخطي، بما في ذلك Conv (تطبيق مباشر لطبقة التفاف الشبكة العصبية)، وVGG (كتلة من شبكة VGG العصبية)، وsgemm (ضرب المصفوفات المستخدم لتنفيذ الالتفافات)، وHPCG (معيار للتدرج المترافق المسبق الشرط متعدد الشبكات، CG)، وBaryon (شفرة تقلص موتر كثيف لبناء كتل بناء باريون [16]). لجميع هذه المعايير، نقارن تطبيق TIRAMISU مع Intel MKL، باستثناء HPCG وBaryon، حيث نقارن TIRAMISU مع التطبيقات المرجعية. يُظهر الشكل 5 مقارنة بين أداء شفرة CPU المولدة بواسطة Tiramisu والشفرة المرجعية. بالنسبة لـ sgemm وHPCG نستخدم مصفوفات بحجم 1060 × 1060 ومتجهات بحجم 1060 بينما لـ Conv وVGG نستخدم 512 × 512 كحجم إدخال البيانات، و16 كعدد الميزات المدخلة/المخرجة وحجم دفعة 32. بالنسبة لـ Baryon، نستخدم نفس أحجام الموتر كما في الشفرة المرجعية.

بالنسبة لـ sgemm، يطابق TIRAMISU أداء Intel MKL. إن sgemm مثير للاهتمام بشكل خاص لأن تطبيق Intel MKL لهذه النواة معروف بأدائه المحسن يدوياً. استخدمنا مجموعة كبيرة من التحسينات لمطابقة Intel MKL. تتضمن هذه التحسينات الحجب ثنائي المستوى لحلقة sgemm ثلاثية الأبعاد، والتوجيه المتجه، وفك الحلقات، وتعبئة المصفوفات، وحجب السجلات، وفصل البلاطات الكاملة والجزئية (وهو أمر بالغ الأهمية لتمكين التوجيه المتجه، وفك الحلقات، وتقليل عبء التحكم). استخدمنا أيضاً الضبط التلقائي لإيجاد أفضل حجم بلاطة وعامل فك للآلة التي نجري عليها تجاربنا.

بالنسبة لنواة Conv، يتفوق TIRAMISU على تطبيق Intel MKL لأن الشفرة المولدة بواسطة TIRAMISU تستخدم حجماً ثابتاً لمرشح الالتفاف. نولد إصدارات متخصصة لأحجام مرشحات الالتفاف الشائعة (3 × 3، 5 × 5، 7×7، 9×9 و11×11). يتيح هذا لمترجم TIRAMISU تطبيق تحسينات لا يقوم Intel MKL بتنفيذها؛ على سبيل المثال، يتيح ذلك لـ TIRAMISU فك حلقات (مرشح الالتفاف) الأعمق نظراً لأن حجمها معروف في وقت الترجمة. في VGG، يدمج TIRAMISU حلقتي الالتفاف لكتلة VGG، مما يحسن موضعية البيانات. بالإضافة إلى ذلك، نولد شفرة بأحجام ثابتة لمرشحات الالتفاف (كما فعلنا في Conv). هذا يوفر تسريعاً بمقدار 2.3× على Intel MKL.

يتم تحقيق تسريع TIRAMISU على شفرة Baryon المرجعية من خلال التوجيه المتجه، لكن التوجيه المتجه هذا ليس تافهاً لأنه يتطلب تطبيق توسيع المصفوفات ثم استخدام عمليات التشتيت/التجميع، وكلاهما غير منفذ في شفرة Baryon المرجعية.

**ب. معايير معالجة الصور**

استخدمنا معايير معالجة الصور التالية في تقييمنا: edgeDetector، وهو ضبابية حلقية متبوعة باكتشاف حافة Roberts [41]؛ cvtColor، الذي يحول صورة RGB إلى تدرج الرمادي؛ conv2D، التفاف ثنائي الأبعاد بسيط؛ warpAffine، الذي يقوم بالتواء أفيني على صورة؛ gaussian، الذي ينفذ ضبابية غاوسية؛ nb، خط أنابيب اصطناعي يتكون من 4 مراحل يحسب صورة سلبية وصورة أكثر سطوعاً من نفس صورة الإدخال؛ وticket #2373، مقتطف شفرة من خطأ مسجل ضد Halide.

استخدمنا صورة RGB بحجم إدخال 2112 × 3520 للتجارب. نقارن TIRAMISU مع مترجمين آخرين: Halide [39]، وهي لغة خاصة بالمجال بجودة صناعية لمعالجة الصور ولها لغة جدولة، وPENCIL [3]، وهو مترجم متعدد السطوح أوتوماتيكي بالكامل حديث.

يقارن الشكل 6 وقت التنفيذ الموحد للشفرة المولدة بواسطة TIRAMISU مع أطر العمل الحديثة الأخرى على ثلاث معماريات: عقدة واحدة متعددة الأنوية، وGPU وموزع (16 عقدة).

**أ) عقدة واحدة متعددة الأنوية:** في أربعة من المعايير، يطابق أداء الشفرة المولدة بواسطة TIRAMISU أداء Halide. اثنان من المعايير الأخرى، edgeDetector وticket #2373، لا يمكن تنفيذهما في Halide. ينشئ edgeDetector رسماً بيانياً للاعتماديات الدورية، والذي لا يمكن لـ Halide التعبير عنه. في ticket #2373، الذي يعرض مجال تكرار مثلثي، يبالغ استنتاج حدود Halide في تقدير الحدود المحسوبة، مما يؤدي إلى فشل الشفرة المولدة.

بالنسبة لـ nb، تحقق الشفرة المولدة من TIRAMISU تسريعاً بمقدار 3.77× على الشفرة المولدة بواسطة Halide. يرجع ذلك في المقام الأول إلى دمج الحلقات.

يرجع تباطؤ مترجم PENCIL في gaussian إلى قرار دون المستوى الأمثل اتخذه PENCIL فيما يتعلق بمقايضات تبادل الحلقات مقابل الموضعية المكانية.

**ب) GPU:** بالنسبة للواجهة الخلفية لـ GPU، فإن الأوقات المبلغ عنها هي أوقات التنفيذ الإجمالية (نسخ البيانات وتنفيذ النواة). الشفرة المولدة بواسطة TIRAMISU لـ conv2D وgaussian أسرع من تلك الخاصة بـ Halide لأن الشفرة المولدة بواسطة TIRAMISU تستخدم الذاكرة الثابتة لتخزين مصفوفة الأوزان.

**ج) موزع:** نفترض أن البيانات موزعة بالفعل عبر العقد بالصفوف. يقارن الشكل 6 وقت تنفيذ TIRAMISU الموزع وHalide الموزع. TIRAMISU أسرع من Halide الموزع في كل حالة. يحقق تسريعاً يصل إلى 3.25× لـ conv2D.

بالنسبة للنوى التي تتضمن الاتصال، فإن الشفرة المولدة بواسطة Halide الموزع لديها مشكلتان مقارنةً بـ TIRAMISU: يبالغ Halide الموزع في تقدير كمية البيانات التي يحتاج إلى إرسالها، ويحزم بشكل غير ضروري البيانات المتجاورة معاً في مخزن مؤقت منفصل قبل الإرسال.

يُظهر الشكل 7 تسريع النوى مع TIRAMISU الموزع عند التشغيل على 2 و4 و8 و16 عقدة. يُظهر هذا الرسم البياني أن الشفرة الموزعة المولدة من TIRAMISU تتوسع بشكل جيد مع زيادة عدد العقد (التوسع القوي).

---

### Translation Notes

- **Figures referenced:** Figure 5 (deep learning/linear algebra performance), Figure 6 (image processing heatmap), Figure 7 (scaling graph)
- **Key terms introduced:** auto-tuning, array expansion, scatter/gather operations, edge detection, grayscale conversion, affine warping, gaussian blur, bounds inference, strong scaling, median time, tile size, control overhead
- **Equations:** Matrix sizes, image dimensions
- **Citations:** [3], [16], [25], [39], [41]
- **Special handling:** Performance comparisons, benchmark descriptions, experimental setup

### Quality Metrics

- Semantic equivalence: 0.88
- Technical accuracy: 0.89
- Readability: 0.86
- Glossary consistency: 0.87
- **Overall section score:** 0.87
