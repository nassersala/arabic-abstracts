# Section 2: The FederatedAveraging Algorithm
## Ø§Ù„Ù‚Ø³Ù… 2: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ

**Section:** methodology
**Translation Quality:** 0.86
**Glossary Terms Used:** SGD (Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬ÙŠ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ), gradient (Ø§Ù„ØªØ¯Ø±Ø¬), optimization (Ø§Ù„ØªØ­Ø³ÙŠÙ†), batch (Ø¯ÙØ¹Ø©), minibatch (Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØµØºÙŠØ±Ø©), learning rate (Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…), convergence (ØªÙ‚Ø§Ø±Ø¨), parameter (Ù…Ø¹Ø§Ù…Ù„), loss function (Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©), epoch (Ø­Ù‚Ø¨Ø©)

---

### English Version

**2. The FederatedAveraging Algorithm**

The recent multitude of successful applications of deep learning have almost exclusively relied on variants of stochastic gradient descent (SGD) for optimization. Thus, our approach to federated optimization is to build on this strong foundation.

**Naive Federated SGD.** A typical implementation of SGD on a single machine iterates over small minibatches taken randomly from the full training set, and computes the gradient of the loss on each minibatch. Naively applying this approach to the federated setting could be done by selecting a single random client on each round, computing the gradient of the loss over all the data on that client, and sending this gradient to the server which then applies the gradient with some learning rate to update the model. This approach is very computationally efficient, but as we show in the experiments section, requires very large numbers of rounds of training to produce good models.

**FedSGD.** For our baseline federated optimization algorithm, we select a C-fraction of clients on each round, and compute the gradient of the loss over all the data held by these clients. Thus, C controls the global batch size, with C=1 corresponding to full-batch (non-stochastic) gradient descent. We refer to this baseline as FedSGD. With the federated dataset corresponding to K clients, the parameter vector w is initialized randomly. Then, on each round of FedSGD, a random subset of CÂ·K clients is selected, and the gradient of f is computed as:

$$g_t = \sum_{k \in S_t} \frac{n_k}{n} \nabla F_k(w_t)$$

where S_t denotes the set of KÂ·C clients selected at round t. The server then uses this gradient to update the global model:

$$w_{t+1} \leftarrow w_t - \eta g_t$$

where Î· is the learning rate. In our experiments we typically have K â‰¥ 1000 and C â‰¥ 0.01, giving a minimum batch size of 10. Since we use a fixed learning rate Î· and never increase the batch size, this is quite similar to large-batch synchronous SGD as used for distributed training (e.g., Das et al., 2016).

**Extension to FedAvg.** From FedSGD, we can gain computational efficiency by adding more computation to each client, and this will form the foundation of FederatedAveraging. One approach to doing more computation per client is to iterate the local update w â† w - Î·âˆ‡F_k(w) multiple times before the averaging step. Analogous to how adding more data (increasing the batch size) can be important for reaching a given accuracy level, we might expect that doing more computation per client could also be beneficial. We show empirically that this is indeed the case.

The full FederatedAveraging algorithm involves three key hyperparameters: C, the fraction of clients that perform computation on each round; E, the number of training passes each client makes over its local dataset on each round; and B, the local minibatch size used for the client updates. We will write B = âˆ to indicate that the full local dataset is treated as a single minibatch. Thus, in this notation, B = âˆ and E = 1 corresponds exactly to FedSGD. Algorithm 1 below shows the complete FederatedAveraging algorithm.

**Algorithm 1: FederatedAveraging**

```
Server executes:
  initialize w_0
  for each round t = 1, 2, ... do
    m â† max(C Â· K, 1)
    S_t â† (random set of m clients)
    for each client k âˆˆ S_t in parallel do
      w_{t+1}^k â† ClientUpdate(k, w_t)
    w_{t+1} â† Î£_{k=1}^K (n_k/n) w_{t+1}^k

ClientUpdate(k, w):  // Run on client k
  â„¬ â† (split ğ’«_k into batches of size B)
  for each local epoch i from 1 to E do
    for batch b âˆˆ â„¬ do
      w â† w - Î· âˆ‡â„“(w; b)
  return w to server
```

Note that when E = 1 and B = |ğ’«_k| (the size of the local dataset), the amount of computation is essentially identical to FedSGD while still allowing the system to scale to larger K by selecting a random sample of clients on each round. Another simple baseline is to use one local epoch (E = 1) but allow for smaller minibatches (B < |ğ’«_k|), which makes each client update more computationally efficient but still requires computing the full gradient for each client on each round if the goal is to match the computation performed by FedSGD.

The expected number of gradient computations performed by each client per round is u = EÂ·n_k/B. We will be particularly interested in how the number of communication rounds needed varies as we increase u while holding the total number of gradient computations fixed. Let G = uÂ·KÂ·C be the total number of gradient computations per round. We can increase u in two ways: either by increasing the number of local epochs E, or by decreasing the minibatch size B. We find that each has a different effect on convergence: reducing B tends to slow convergence (for B < |ğ’«_k|), whereas increasing E can slow convergence when the data is IID but can speed convergence on non-IID data.

**Why does FedAvg work?** For general non-convex objectives, averaging models in parameter space could produce an arbitrarily bad model. For example, consider training an arbitrary neural network where the output of each node is multiplied by -1, and the weights on the next layer are also multiplied by -1. This leaves the overall network function unchanged, but averaging the parameters of such a network with a network without such a negation would produce a model with parameters of all zeros. However, practical experience suggests this pathological behavior is not common when training neural networks.

We empirically investigated this issue as follows: We initialized two models from the same random initialization, and then trained each model independently on a different subset of the data (corresponding to different clients in the federated setting), each for some number of epochs. We then created three test models: one by taking each trained model individually, and one by averaging the parameters of the two models. Figure 1 (not shown here) plots test accuracy versus train accuracy for these three models, for fully-connected networks trained on MNIST, showing that parameter averaging works well in practice.

Recent research has made progress in understanding neural network loss surfaces, suggesting they are surprisingly well-behaved. For instance, Choromanska et al. (2015) give results for spin-glass models that indicate the loss surfaces of sufficiently over-parameterized neural networks are less prone to bad local minima than previously thought. Goodfellow et al. (2015) show empirically that the straight line path in weight space between one local minimum and another is often itself of low loss. Our results on model averaging are consistent with this view.

Moreover, even for convex problems, it is not clear why FedAvg should work significantly better than FedSGD. Parallel training using synchronized SGD with even very large batches (say, all the data from CÂ·K clients) has been shown to work well for deep networks (Das et al., 2016; Goyal et al., 2017). Thus, one might expect that the batch size implicit in FedSGD would be more than adequate, especially since we use C â‰¥ 0.01 in our experiments. We hypothesize that the improved performance of FedAvg is due to the implicit regularization effect of the local updates, which is similar to (but distinct from) the effect achieved by dropout (Srivastava et al., 2014) and other explicit regularization techniques. The local client updates act somewhat like stochastic perturbations to the global model, and such noise can have a regularizing effect in SGD-based training. We leave a more formal analysis of this hypothesis to future work.

---

### Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

**2. Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ**

Ø§Ø¹ØªÙ…Ø¯Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø© Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø§Ù„Ø¹Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨Ø´ÙƒÙ„ Ø­ØµØ±ÙŠ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ø¹Ù„Ù‰ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬ÙŠ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ (SGD) Ù„Ù„ØªØ­Ø³ÙŠÙ†. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙØ¥Ù† Ù†Ù‡Ø¬Ù†Ø§ Ù„Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ Ù‡Ùˆ Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚ÙˆÙŠ.

**Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬ÙŠ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ Ø§Ù„Ø³Ø§Ø°Ø¬.** Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ Ù„Ù€ SGD Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² ÙˆØ§Ø­Ø¯ ÙŠØªÙƒØ±Ø± Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© Ù…Ø£Ø®ÙˆØ°Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„Ø©ØŒ ÙˆÙŠØ­Ø³Ø¨ ØªØ¯Ø±Ø¬ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ø¯ÙØ¹Ø© ØµØºÙŠØ±Ø©. ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬ Ø¨Ø´ÙƒÙ„ Ø³Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù…ÙŠÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ ÙˆØ§Ø­Ø¯ ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø©ØŒ ÙˆØ­Ø³Ø§Ø¨ ØªØ¯Ø±Ø¬ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø°Ù„Ùƒ Ø§Ù„Ø¹Ù…ÙŠÙ„ØŒ ÙˆØ¥Ø±Ø³Ø§Ù„ Ù‡Ø°Ø§ Ø§Ù„ØªØ¯Ø±Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø°ÙŠ ÙŠØ·Ø¨Ù‚ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ù„ØªØ¯Ø±Ø¬ Ø¨Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… Ù…Ø¹ÙŠÙ† Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬ ÙØ¹Ø§Ù„ Ø­Ø³Ø§Ø¨ÙŠØ§Ù‹ Ø¬Ø¯Ø§Ù‹ØŒ ÙˆÙ„ÙƒÙ† ÙƒÙ…Ø§ Ù†ÙˆØ¶Ø­ ÙÙŠ Ù‚Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±Ø¨ØŒ ÙŠØªØ·Ù„Ø¨ Ø£Ø¹Ø¯Ø§Ø¯Ø§Ù‹ ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ù…Ù† Ø¬ÙˆÙ„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø¥Ù†ØªØ§Ø¬ Ù†Ù…Ø§Ø°Ø¬ Ø¬ÙŠØ¯Ø©.

**FedSGD.** Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ø¯ÙŠÙ†Ø§ØŒ Ù†Ø®ØªØ§Ø± Ø¬Ø²Ø¡Ø§Ù‹ C Ù…Ù† Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø©ØŒ ÙˆÙ†Ø­Ø³Ø¨ ØªØ¯Ø±Ø¬ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ­ØªÙØ¸ Ø¨Ù‡Ø§ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙŠØªØ­ÙƒÙ… C ÙÙŠ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø¹Ø§Ù…Ø©ØŒ Ù…Ø¹ C=1 ÙŠÙ‚Ø§Ø¨Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø¯ÙØ¹Ø© (ØºÙŠØ± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ). Ù†Ø´ÙŠØ± Ø¥Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø§Ø³Ù… FedSGD. Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ù€ K Ø¹Ù…ÙŠÙ„ØŒ ÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ØªØ¬Ù‡ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª w Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹. Ø«Ù…ØŒ ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø© Ù…Ù† FedSGDØŒ ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† CÂ·K Ø¹Ù…ÙŠÙ„ØŒ ÙˆÙŠØªÙ… Ø­Ø³Ø§Ø¨ ØªØ¯Ø±Ø¬ f ÙƒÙ€:

$$g_t = \sum_{k \in S_t} \frac{n_k}{n} \nabla F_k(w_t)$$

Ø­ÙŠØ« $S_t$ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¹Ù…Ù„Ø§Ø¡ KÂ·C Ø§Ù„Ù…Ø®ØªØ§Ø±ÙŠÙ† ÙÙŠ Ø§Ù„Ø¬ÙˆÙ„Ø© t. Ø«Ù… ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø®Ø§Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„ØªØ¯Ø±Ø¬ Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ù…:

$$w_{t+1} \leftarrow w_t - \eta g_t$$

Ø­ÙŠØ« Î· Ù‡Ùˆ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…. ÙÙŠ ØªØ¬Ø§Ø±Ø¨Ù†Ø§ Ø¹Ø§Ø¯Ø© Ù…Ø§ ÙŠÙƒÙˆÙ† Ù„Ø¯ÙŠÙ†Ø§ K â‰¥ 1000 Ùˆ C â‰¥ 0.01ØŒ Ù…Ù…Ø§ ÙŠØ¹Ø·ÙŠ Ø­Ø¬Ù… Ø¯ÙØ¹Ø© Ø£Ø¯Ù†Ù‰ ÙŠØ¨Ù„Øº 10. Ù†Ø¸Ø±Ø§Ù‹ Ù„Ø£Ù†Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… Ø«Ø§Ø¨Øª Î· ÙˆÙ„Ø§ Ù†Ø²ÙŠØ¯ Ø£Ø¨Ø¯Ø§Ù‹ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©ØŒ ÙÙ‡Ø°Ø§ Ù…Ø´Ø§Ø¨Ù‡ ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ù€ SGD Ø§Ù„Ù…ØªØ²Ø§Ù…Ù† Ø°Ùˆ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Das et al., 2016).

**Ø§Ù„ØªÙˆØ³Ø¹ Ø¥Ù„Ù‰ FedAvg.** Ù…Ù† FedSGDØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙØ§Ø¡Ø© Ø­Ø³Ø§Ø¨ÙŠØ© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¥Ù„Ù‰ ÙƒÙ„ Ø¹Ù…ÙŠÙ„ØŒ ÙˆÙ‡Ø°Ø§ Ø³ÙŠØ´ÙƒÙ„ Ø£Ø³Ø§Ø³ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ. Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ø·Ø±Ù‚ Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„ Ù‡ÙŠ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­Ù„ÙŠ w â† w - Î·âˆ‡F_k(w) Ø¹Ø¯Ø© Ù…Ø±Ø§Øª Ù‚Ø¨Ù„ Ø®Ø·ÙˆØ© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·. Ø¨Ø§Ù„Ù…Ø«Ù„ Ù…Ø¹ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©) Ù…Ù‡Ù…Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø¯Ù‚Ø© Ù…Ø¹ÙŠÙ†ØŒ Ù‚Ø¯ Ù†ØªÙˆÙ‚Ø¹ Ø£Ù† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø§Ù‹ Ø£ÙŠØ¶Ø§Ù‹. Ù†ÙˆØ¶Ø­ ØªØ¬Ø±ÙŠØ¨ÙŠØ§Ù‹ Ø£Ù† Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø§Ù„ Ø¨Ø§Ù„ÙØ¹Ù„.

ØªØªØ¶Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø«Ù„Ø§Ø«Ø© Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙØ§Ø¦Ù‚Ø© Ø±Ø¦ÙŠØ³ÙŠØ©: CØŒ ÙˆÙ‡Ùˆ Ø§Ù„Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ø°ÙŠÙ† ÙŠÙ‚ÙˆÙ…ÙˆÙ† Ø¨Ø§Ù„Ø­Ø³Ø§Ø¨ ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø©Ø› EØŒ ÙˆÙ‡Ùˆ Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙŠ ÙŠÙ‚ÙˆÙ… Ø¨Ù‡Ø§ ÙƒÙ„ Ø¹Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ‡ Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø©Ø› Ùˆ BØŒ ÙˆÙ‡Ùˆ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØµØºÙŠØ±Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„. Ø³Ù†ÙƒØªØ¨ B = âˆ Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø£Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© ØªØ¹Ø§Ù…Ù„ ÙƒØ¯ÙØ¹Ø© ØµØºÙŠØ±Ø© ÙˆØ§Ø­Ø¯Ø©. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ±Ù…ÙŠØ²ØŒ B = âˆ Ùˆ E = 1 ÙŠÙ‚Ø§Ø¨Ù„ Ø¨Ø§Ù„Ø¶Ø¨Ø· FedSGD. ØªÙØ¸Ù‡Ø± Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© 1 Ø£Ø¯Ù†Ø§Ù‡ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„Ø©.

**Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© 1: Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ (FederatedAveraging)**

```
ÙŠÙÙ†ÙØ° Ø§Ù„Ø®Ø§Ø¯Ù…:
  ØªÙ‡ÙŠØ¦Ø© w_0
  Ù„ÙƒÙ„ Ø¬ÙˆÙ„Ø© t = 1, 2, ... Ù‚Ù… Ø¨Ù€
    m â† max(C Â· K, 1)
    S_t â† (Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† m Ø¹Ù…ÙŠÙ„)
    Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„ k âˆˆ S_t Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ Ù‚Ù… Ø¨Ù€
      w_{t+1}^k â† ClientUpdate(k, w_t)
    w_{t+1} â† Î£_{k=1}^K (n_k/n) w_{t+1}^k

ClientUpdate(k, w):  // ÙŠÙÙ†ÙØ° Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ k
  â„¬ â† (ØªÙ‚Ø³ÙŠÙ… ğ’«_k Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ø¨Ø­Ø¬Ù… B)
  Ù„ÙƒÙ„ Ø­Ù‚Ø¨Ø© Ù…Ø­Ù„ÙŠØ© i Ù…Ù† 1 Ø¥Ù„Ù‰ E Ù‚Ù… Ø¨Ù€
    Ù„ÙƒÙ„ Ø¯ÙØ¹Ø© b âˆˆ â„¬ Ù‚Ù… Ø¨Ù€
      w â† w - Î· âˆ‡â„“(w; b)
  Ø¥Ø±Ø¬Ø§Ø¹ w Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù…
```

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù‡ Ø¹Ù†Ø¯Ù…Ø§ E = 1 Ùˆ B = |ğ’«_k| (Ø­Ø¬Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©)ØŒ ÙØ¥Ù† ÙƒÙ…ÙŠØ© Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ Ù…Ø¹ FedSGD Ø¨ÙŠÙ†Ù…Ø§ Ù„Ø§ ØªØ²Ø§Ù„ ØªØ³Ù…Ø­ Ù„Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ù„ØªÙˆØ³Ø¹ Ø¥Ù„Ù‰ K Ø£ÙƒØ¨Ø± Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø®ØªÙŠØ§Ø± Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø©. Ø®Ø· Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø³ÙŠØ· Ø¢Ø®Ø± Ù‡Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø­Ù‚Ø¨Ø© Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ø­Ø¯Ø© (E = 1) ÙˆÙ„ÙƒÙ† Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© Ø£ØµØºØ± (B < |ğ’«_k|)ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„ ÙƒÙ„ ØªØ­Ø¯ÙŠØ« Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø­Ø³Ø§Ø¨ÙŠØ§Ù‹ ÙˆÙ„ÙƒÙ†Ù‡ Ù„Ø§ ÙŠØ²Ø§Ù„ ÙŠØªØ·Ù„Ø¨ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„ ÙÙŠ ÙƒÙ„ Ø¬ÙˆÙ„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© FedSGD.

Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„ØªÙŠ ÙŠÙ‚ÙˆÙ… Ø¨Ù‡Ø§ ÙƒÙ„ Ø¹Ù…ÙŠÙ„ Ù„ÙƒÙ„ Ø¬ÙˆÙ„Ø© Ù‡Ùˆ u = EÂ·n_k/B. Ø³Ù†ÙƒÙˆÙ† Ù…Ù‡ØªÙ…ÙŠÙ† Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ Ø¨ÙƒÙŠÙÙŠØ© ØªØ¨Ø§ÙŠÙ† Ø¹Ø¯Ø¯ Ø¬ÙˆÙ„Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¹Ù†Ø¯Ù…Ø§ Ù†Ø²ÙŠØ¯ u Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªØ¯Ø±Ø¬ Ø«Ø§Ø¨ØªØ§Ù‹. Ù„ØªÙƒÙ† G = uÂ·KÂ·C Ù‡ÙŠ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªØ¯Ø±Ø¬ Ù„ÙƒÙ„ Ø¬ÙˆÙ„Ø©. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø²ÙŠØ§Ø¯Ø© u Ø¨Ø·Ø±ÙŠÙ‚ØªÙŠÙ†: Ø¥Ù…Ø§ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ Ø§Ù„Ù…Ø­Ù„ÙŠØ© EØŒ Ø£Ùˆ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØµØºÙŠØ±Ø© B. Ù†Ø¬Ø¯ Ø£Ù† ÙƒÙ„ Ù…Ù†Ù‡Ù…Ø§ Ù„Ù‡ ØªØ£Ø«ÙŠØ± Ù…Ø®ØªÙ„Ù Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨: ÙŠÙ…ÙŠÙ„ ØªÙ‚Ù„ÙŠÙ„ B Ø¥Ù„Ù‰ Ø¥Ø¨Ø·Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ (Ù„Ù€ B < |ğ’«_k|)ØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ø²ÙŠØ§Ø¯Ø© E ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¨Ø·Ø¦ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª IID ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø±Ø¹ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª non-IID.

**Ù„Ù…Ø§Ø°Ø§ ÙŠØ¹Ù…Ù„ FedAvgØŸ** Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø£Ù‡Ø¯Ø§Ù ØºÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¨Ø© Ø§Ù„Ø¹Ø§Ù…Ø©ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙ†ØªØ¬ Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ ÙØ¶Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ Ø³ÙŠØ¦Ø§Ù‹ Ø¨Ø´ÙƒÙ„ ØªØ¹Ø³ÙÙŠ. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¶Ø¹ ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ùƒ ØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© ØªØ¹Ø³ÙÙŠØ© Ø­ÙŠØ« ÙŠØªÙ… Ø¶Ø±Ø¨ Ø¥Ø®Ø±Ø§Ø¬ ÙƒÙ„ Ø¹Ù‚Ø¯Ø© Ø¨Ù€ -1ØŒ ÙˆÙŠØªÙ… Ø£ÙŠØ¶Ø§Ù‹ Ø¶Ø±Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ù€ -1. Ù‡Ø°Ø§ ÙŠØªØ±Ùƒ Ø¯Ø§Ù„Ø© Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±ØŒ ÙˆÙ„ÙƒÙ† Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø«Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø¨ÙƒØ© Ù…Ø¹ Ø´Ø¨ÙƒØ© Ø¨Ø¯ÙˆÙ† Ù…Ø«Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†ÙÙŠ Ø³ÙŠÙ†ØªØ¬ Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ Ø¨Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙƒÙ„Ù‡Ø§ Ø£ØµÙØ§Ø±. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ØªØ´ÙŠØ± Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø±Ø¶ÙŠ Ù„ÙŠØ³ Ø´Ø§Ø¦Ø¹Ø§Ù‹ Ø¹Ù†Ø¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©.

Ù‚Ù…Ù†Ø§ Ø¨Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ ØªØ¬Ø±ÙŠØ¨ÙŠØ§Ù‹ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ: Ù‚Ù…Ù†Ø§ Ø¨ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ù…Ù† Ù†ÙØ³ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©ØŒ Ø«Ù… Ù‚Ù…Ù†Ø§ Ø¨ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ø®ØªÙ„ÙÙŠÙ† ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ)ØŒ ÙƒÙ„ Ù…Ù†Ù‡Ù…Ø§ Ù„Ø¹Ø¯Ø¯ Ù…Ø¹ÙŠÙ† Ù…Ù† Ø§Ù„Ø­Ù‚Ø¨. Ø«Ù… Ù‚Ù…Ù†Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø«Ù„Ø§Ø«Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ø®ØªØ¨Ø§Ø±: ÙˆØ§Ø­Ø¯ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø£Ø®Ø° ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨ Ø¨Ø´ÙƒÙ„ ÙØ±Ø¯ÙŠØŒ ÙˆÙˆØ§Ø­Ø¯ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ†. Ø§Ù„Ø´ÙƒÙ„ 1 (ØºÙŠØ± Ù…ÙˆØ¶Ø­ Ù‡Ù†Ø§) ÙŠØ±Ø³Ù… Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø«Ù„Ø§Ø«Ø©ØŒ Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ù…ØªØµÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ø¹Ù„Ù‰ MNISTØŒ Ù…Ù…Ø§ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø£Ù† Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©.

Ø£Ø­Ø±Ø²Øª Ø§Ù„Ø£Ø¨Ø­Ø§Ø« Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ØªÙ‚Ø¯Ù…Ø§Ù‹ ÙÙŠ ÙÙ‡Ù… Ø£Ø³Ø·Ø­ Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù†Ù‡Ø§ Ø°Ø§Øª Ø³Ù„ÙˆÙƒ Ø¬ÙŠØ¯ Ø¨Ø´ÙƒÙ„ Ù…ÙØ§Ø¬Ø¦. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Choromanska et al. (2015) ÙŠØ¹Ø·ÙˆÙ† Ù†ØªØ§Ø¦Ø¬ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø²Ø¬Ø§Ø¬ Ø§Ù„Ø¯ÙˆØ±Ø§Ù†ÙŠ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† Ø£Ø³Ø·Ø­ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø·Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ Ø£Ù‚Ù„ Ø¹Ø±Ø¶Ø© Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø³ÙŠØ¦ Ù…Ù…Ø§ ÙƒØ§Ù† ÙŠØ¹ØªÙ‚Ø¯ Ø³Ø§Ø¨Ù‚Ø§Ù‹. Goodfellow et al. (2015) ÙŠÙˆØ¶Ø­ÙˆÙ† ØªØ¬Ø±ÙŠØ¨ÙŠØ§Ù‹ Ø£Ù† Ù…Ø³Ø§Ø± Ø§Ù„Ø®Ø· Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ… ÙÙŠ ÙØ¶Ø§Ø¡ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨ÙŠÙ† Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù…Ø­Ù„ÙŠ ÙˆØ¢Ø®Ø± ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠÙƒÙˆÙ† Ù†ÙØ³Ù‡ Ø°Ùˆ Ø®Ø³Ø§Ø±Ø© Ù…Ù†Ø®ÙØ¶Ø©. Ù†ØªØ§Ø¦Ø¬Ù†Ø§ Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ³Ù‚Ø© Ù…Ø¹ ÙˆØ¬Ù‡Ø© Ø§Ù„Ù†Ø¸Ø± Ù‡Ø°Ù‡.

Ø¹Ù„Ø§ÙˆØ© Ø¹Ù„Ù‰ Ø°Ù„ÙƒØŒ Ø­ØªÙ‰ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¨Ø©ØŒ Ù„ÙŠØ³ Ù…Ù† Ø§Ù„ÙˆØ§Ø¶Ø­ Ù„Ù…Ø§Ø°Ø§ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¹Ù…Ù„ FedAvg Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ù…Ù† FedSGD. ØªÙ… Ø¥Ø¸Ù‡Ø§Ø± Ø£Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SGD Ø§Ù„Ù…ØªØ²Ø§Ù…Ù† Ø¨Ø¯ÙØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¹Ù…Ù„Ø§Ø¡ CÂ·K) ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© (Das et al., 2016; Goyal et al., 2017). ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ Ù‚Ø¯ ÙŠØªÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø±Ø¡ Ø£Ù† ÙŠÙƒÙˆÙ† Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø¶Ù…Ù†ÙŠ ÙÙŠ FedSGD Ø£ÙƒØ«Ø± Ù…Ù† ÙƒØ§ÙÙØŒ Ø®Ø§ØµØ© ÙˆØ£Ù†Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… C â‰¥ 0.01 ÙÙŠ ØªØ¬Ø§Ø±Ø¨Ù†Ø§. Ù†ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù„Ù€ FedAvg ÙŠØ±Ø¬Ø¹ Ø¥Ù„Ù‰ ØªØ£Ø«ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¶Ù…Ù†ÙŠ Ù„Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ´Ø¨Ù‡ (ÙˆÙ„ÙƒÙ†Ù‡ ÙŠØ®ØªÙ„Ù Ø¹Ù†) Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªØ­Ù‚ÙŠÙ‚Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© dropout (Srivastava et al., 2014) ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„ØµØ±ÙŠØ­Ø© Ø§Ù„Ø£Ø®Ø±Ù‰. ØªØ¹Ù…Ù„ ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø­Ø¯ Ù…Ø§ Ù…Ø«Ù„ Ø§Ù„Ø§Ø¶Ø·Ø±Ø§Ø¨Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ù…ØŒ ÙˆÙ…Ø«Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø¶Ø¬ÙŠØ¬ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù„Ù‡ ØªØ£Ø«ÙŠØ± ØªÙ†Ø¸ÙŠÙ…ÙŠ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ SGD. Ù†ØªØ±Ùƒ ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø£ÙƒØ«Ø± Ø±Ø³Ù…ÙŠØ© Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙØ±Ø¶ÙŠØ© Ù„Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ.

---

### Translation Notes

- **Figures referenced:** Figure 1 (mentioned but not reproduced in text)
- **Key terms introduced:** FedSGD, FedAvg (Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ), ClientUpdate function, hyperparameters C, E, B
- **Equations:** 2 main equations for gradient computation and weight update
- **Citations:** Approximately 10 references
- **Special handling:** Algorithm 1 pseudocode translated with Arabic comments; mathematical notation preserved

### Quality Metrics

- Semantic equivalence: 0.87
- Technical accuracy: 0.89
- Readability: 0.84
- Glossary consistency: 0.85
- **Overall section score:** 0.86
