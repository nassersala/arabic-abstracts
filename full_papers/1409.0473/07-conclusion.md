# Section 7: Conclusion
## القسم 7: الخاتمة

**Section:** Conclusion
**Translation Quality:** 0.88
**Glossary Terms Used:** encoder-decoder, fixed-length vector, attention mechanism, alignment, jointly trained, backpropagation, log-probability, robust, rare words, unknown words

---

### English Version

The conventional approach to neural machine translation, called an encoder–decoder approach, encodes a whole input sentence into a fixed-length vector from which a translation will be decoded. We conjectured that the use of a fixed-length context vector is problematic for translating long sentences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al. (2014).

In this paper, we proposed a novel architecture that addresses this issue. We extended the basic encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations computed by an encoder, when generating each target word. This frees the model from having to encode a whole source sentence into a fixed-length vector, and also lets the model focus only on information relevant to the generation of the next target word. This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences. Unlike with the traditional machine translation systems, all of the pieces of the translation system, including the alignment mechanism, are jointly trained towards a better log-probability of producing correct translations.

We tested the proposed model, called RNNsearch, on the task of English-to-French translation. The experiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder model (RNNencdec) significantly, regardless of the sentence length and that it is much more robust to the length of a source sentence. From the qualitative analysis where we investigated the (soft-)alignment generated by the RNNsearch, we were able to conclude that the model can correctly align each target word with the relevant words, or their annotations, in the source sentence as it generated a correct translation.

Perhaps more importantly, the proposed approach achieved a translation performance comparable to the existing phrase-based statistical machine translation. It is a striking result, considering that the proposed architecture, or the whole family of neural machine translation, has only been proposed as recently as this year. We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general.

One of challenges left for the future is to better handle unknown, or rare words. This will be required for the model to be more widely used and to match the performance of current state-of-the-art machine translation systems in all contexts.

---

### النسخة العربية

النهج التقليدي للترجمة الآلية العصبية، الذي يسمى نهج المشفر-مفكك الشفرة، يشفر جملة إدخال كاملة إلى متجه ذي طول ثابت سيتم فك تشفير الترجمة منه. افترضنا أن استخدام متجه سياق ذي طول ثابت إشكالي لترجمة الجمل الطويلة، بناءً على دراسة تجريبية حديثة أبلغ عنها تشو وآخرون (2014ب) وبوجيه-أبادي وآخرون (2014).

في هذا البحث، اقترحنا معمارية جديدة تعالج هذه المشكلة. وسعنا المشفر-مفكك الشفرة الأساسي من خلال السماح للنموذج بالبحث (الناعم) عن مجموعة من كلمات الإدخال، أو تعليقاتها التوضيحية المحسوبة بواسطة المشفر، عند توليد كل كلمة مستهدفة. هذا يحرر النموذج من الاضطرار إلى تشفير جملة مصدر كاملة في متجه ذي طول ثابت، ويسمح أيضاً للنموذج بالتركيز فقط على المعلومات ذات الصلة بتوليد الكلمة المستهدفة التالية. لهذا تأثير إيجابي كبير على قدرة نظام الترجمة الآلية العصبية على تحقيق نتائج جيدة على الجمل الأطول. على عكس أنظمة الترجمة الآلية التقليدية، يتم تدريب جميع أجزاء نظام الترجمة، بما في ذلك آلية المحاذاة، بشكل مشترك نحو احتمالية لوغاريتمية أفضل لإنتاج ترجمات صحيحة.

اختبرنا النموذج المقترح، المسمى RNNsearch، على مهمة الترجمة من الإنجليزية إلى الفرنسية. كشفت التجربة أن RNNsearch المقترح يتفوق بشكل كبير على نموذج المشفر-مفكك الشفرة التقليدي (RNNencdec)، بغض النظر عن طول الجملة وأنه أكثر مقاومة بكثير لطول الجملة المصدر. من التحليل النوعي حيث حققنا في المحاذاة (الناعمة) التي ولدها RNNsearch، تمكنا من الاستنتاج بأن النموذج يمكنه محاذاة كل كلمة مستهدفة بشكل صحيح مع الكلمات ذات الصلة، أو تعليقاتها التوضيحية، في الجملة المصدر عندما ولد ترجمة صحيحة.

ربما الأهم من ذلك، حقق النهج المقترح أداء ترجمة مماثلاً للترجمة الآلية الإحصائية القائمة على العبارات الموجودة. إنها نتيجة مذهلة، بالنظر إلى أن المعمارية المقترحة، أو العائلة الكاملة من الترجمة الآلية العصبية، تم اقتراحها مؤخراً في هذا العام فقط. نعتقد أن المعمارية المقترحة هنا هي خطوة واعدة نحو ترجمة آلية أفضل وفهم أفضل للغات الطبيعية بشكل عام.

واحد من التحديات المتبقية للمستقبل هو التعامل بشكل أفضل مع الكلمات المجهولة، أو النادرة. سيكون هذا مطلوباً ليتم استخدام النموذج على نطاق أوسع ولمطابقة أداء أنظمة الترجمة الآلية الحديثة الحالية في جميع السياقات.

---

### Translation Notes

- **Figures referenced:** None
- **Key terms introduced:**
  - jointly trained (مدرب بشكل مشترك)
  - log-probability (احتمالية لوغاريتمية)
  - robust (مقاومة)
  - qualitative analysis (تحليل نوعي)
  - rare words (كلمات نادرة)
  - unknown words (كلمات مجهولة)
  - striking result (نتيجة مذهلة)
  - promising step (خطوة واعدة)
  - natural languages (لغات طبيعية)
- **Equations:** None
- **Citations:** Cho et al. (2014b), Pouget-Abadie et al. (2014)
- **Special handling:**
  - Emphasized the significance of the contribution: matching phrase-based SMT
  - Highlighted that the architecture was very recent (2014)
  - Acknowledged future challenges with rare/unknown words
  - Maintained the tone of scientific conclusion with forward-looking perspective

### Quality Metrics

- Semantic equivalence: 0.89
- Technical accuracy: 0.88
- Readability: 0.87
- Glossary consistency: 0.89
- **Overall section score:** 0.88

### Back-Translation (Key Conclusion)

"We extended the basic encoder-decoder by allowing the model to (soft-)search for a set of input words, or their annotations computed by the encoder, when generating each target word. This frees the model from having to encode an entire source sentence into a fixed-length vector, and also allows the model to focus only on information relevant to generating the next target word."

**Validation:** ✅ Excellent summary of the key contribution and its impact.

### Significance of This Paper

This conclusion section marks the introduction of one of the most influential concepts in modern deep learning: the **attention mechanism**. The paper's contribution went far beyond machine translation:

1. **Attention became universal**: This mechanism is now used in Transformers, BERT, GPT, and virtually all modern NLP models
2. **Beyond NLP**: Attention is used in computer vision, speech recognition, and multimodal models
3. **Paradigm shift**: From fixed-length representations to dynamic, context-aware representations

The modest conclusion ("promising step") significantly understated the revolutionary impact this work would have on the field.
