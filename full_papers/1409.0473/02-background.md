# Section 2: Background: Neural Machine Translation
## القسم 2: الخلفية: الترجمة الآلية العصبية

**Section:** Background
**Translation Quality:** 0.88
**Glossary Terms Used:** neural machine translation, probabilistic, conditional probability, parameterized model, parallel training corpus, encoder-decoder, RNN (recurrent neural network), LSTM (long short-term memory), hidden state, nonlinear function, state-of-the-art, phrase-based system

---

### English Version

From a probabilistic perspective, translation is equivalent to finding a target sentence **y** that maximizes the conditional probability of **y** given a source sentence **x**, i.e., arg max_y p(**y** | **x**). In neural machine translation, we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability.

Recently, a number of papers have proposed the use of neural networks to directly learn this conditional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and Ñeco, 1997). This neural machine translation approach typically consists of two components, the first of which encodes a source sentence **x** and the second decodes to a target sentence **y**. For instance, two recurrent neural networks (RNN) were used by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence.

Despite being a quite new approach, neural machine translation has already shown promising results. Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long short-term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task. Adding neural components to existing translation systems, for instance, to score the phrase pairs in the phrase table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to surpass the previous state-of-the-art performance level.

**2.1 RNN ENCODER–DECODER**

Here, we describe briefly the underlying framework, called RNN Encoder–Decoder, proposed by Cho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns to align and translate simultaneously.

In the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors **x** = (x₁, ⋯, xₜₓ), into a vector **c**. The most common approach is to use an RNN such that

h_t = f(x_t, h_{t-1})     (1)

and

c = q({h₁, ⋯, hₜₓ}),

where h_t ∈ ℝⁿ is a hidden state at time t, and **c** is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and q({h₁, ⋯, hₜ}) = hₜ, for instance.

The decoder is often trained to predict the next word y_t' given the context vector **c** and all the previously predicted words {y₁, ⋯, y_{t'-1}}. In other words, the decoder defines a probability over the translation **y** by decomposing the joint probability into the ordered conditionals:

p(**y**) = ∏ᵗ_{t=1} p(y_t | {y₁, ⋯, y_{t-1}}, **c**)     (2)

where **y** = (y₁, ⋯, y_Tᵧ). With an RNN, each conditional probability is modeled as

p(y_t | {y₁, ⋯, y_{t-1}}, **c**) = g(y_{t-1}, s_t, **c**)     (3)

where g is a nonlinear, potentially multi-layered, function that outputs the probability of y_t, and s_t is the hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).

---

### النسخة العربية

من منظور احتمالي، الترجمة تعادل إيجاد جملة مستهدفة **y** التي تعظم الاحتمال الشرطي لـ **y** بمعطى جملة مصدر **x**، أي arg max_y p(**y** | **x**). في الترجمة الآلية العصبية، نلائم نموذجاً معلمياً لتعظيم الاحتمال الشرطي لأزواج الجمل باستخدام مدونة تدريب متوازية. بمجرد أن يتعلم نموذج الترجمة التوزيع الشرطي، يمكن توليد ترجمة مقابلة لجملة مصدر معينة من خلال البحث عن الجملة التي تعظم الاحتمال الشرطي.

مؤخراً، اقترح عدد من الأوراق البحثية استخدام الشبكات العصبية لتعلم هذا التوزيع الشرطي بشكل مباشر (انظر، على سبيل المثال، كالشبرينر وبلانسوم، 2013؛ تشو وآخرون، 2014أ؛ سوتسكيفر وآخرون، 2014؛ تشو وآخرون، 2014ب؛ فوركادا ونيكو، 1997). عادة ما يتكون نهج الترجمة الآلية العصبية هذا من مكونين، الأول يشفر جملة مصدر **x** والثاني يفك التشفير إلى جملة مستهدفة **y**. على سبيل المثال، استخدم (تشو وآخرون، 2014أ) و(سوتسكيفر وآخرون، 2014) شبكتين عصبيتين تكراريتين (RNN) لتشفير جملة مصدر ذات طول متغير إلى متجه ذي طول ثابت ولفك تشفير المتجه إلى جملة مستهدفة ذات طول متغير.

على الرغم من كونه نهجاً جديداً تماماً، فقد أظهرت الترجمة الآلية العصبية بالفعل نتائج واعدة. أفاد سوتسكيفر وآخرون (2014) أن الترجمة الآلية العصبية القائمة على الشبكات العصبية التكرارية مع وحدات الذاكرة الطويلة قصيرة المدى (LSTM) تحقق أداءً قريباً من أحدث ما توصل إليه نظام الترجمة الآلية التقليدي القائم على العبارات في مهمة الترجمة من الإنجليزية إلى الفرنسية. إضافة مكونات عصبية إلى أنظمة الترجمة الموجودة، على سبيل المثال، لتسجيل أزواج العبارات في جدول العبارات (تشو وآخرون، 2014أ) أو لإعادة ترتيب الترجمات المرشحة (سوتسكيفر وآخرون، 2014)، سمحت بتجاوز مستوى الأداء المتقدم السابق.

**2.1 المشفر-مفكك الشفرة للشبكة العصبية التكرارية**

هنا، نصف بإيجاز الإطار الأساسي، الذي يُسمى المشفر-مفكك الشفرة للشبكة العصبية التكرارية، المقترح من قبل تشو وآخرون (2014أ) وسوتسكيفر وآخرون (2014) والذي نبني عليه معمارية جديدة تتعلم المحاذاة والترجمة في وقت واحد.

في إطار المشفر-مفكك الشفرة، يقرأ المشفر الجملة المدخلة، وهي تسلسل من المتجهات **x** = (x₁, ⋯, xₜₓ)، إلى متجه **c**. النهج الأكثر شيوعاً هو استخدام شبكة عصبية تكرارية بحيث

h_t = f(x_t, h_{t-1})     (1)

و

c = q({h₁, ⋯, hₜₓ}),

حيث h_t ∈ ℝⁿ هي حالة مخفية في الوقت t، و **c** هو متجه مولد من تسلسل الحالات المخفية. f و q هما دالتان غير خطيتان. استخدم سوتسكيفر وآخرون (2014) LSTM كـ f و q({h₁, ⋯, hₜ}) = hₜ، على سبيل المثال.

غالباً ما يتم تدريب مفكك الشفرة للتنبؤ بالكلمة التالية y_t' بمعطى متجه السياق **c** وجميع الكلمات المتنبأ بها سابقاً {y₁, ⋯, y_{t'-1}}. بعبارة أخرى، يحدد مفكك الشفرة احتمالاً على الترجمة **y** من خلال تحليل الاحتمال المشترك إلى احتمالات شرطية مرتبة:

p(**y**) = ∏ᵗ_{t=1} p(y_t | {y₁, ⋯, y_{t-1}}, **c**)     (2)

حيث **y** = (y₁, ⋯, y_Tᵧ). مع شبكة عصبية تكرارية، يتم نمذجة كل احتمال شرطي كـ

p(y_t | {y₁, ⋯, y_{t-1}}, **c**) = g(y_{t-1}, s_t, **c**)     (3)

حيث g هي دالة غير خطية، ومحتملة متعددة الطبقات، تخرج احتمال y_t، و s_t هي الحالة المخفية للشبكة العصبية التكرارية. تجدر الإشارة إلى أنه يمكن استخدام معماريات أخرى مثل هجين من شبكة عصبية تكرارية وشبكة عصبية التفافية عكسية (كالشبرينر وبلانسوم، 2013).

---

### Translation Notes

- **Figures referenced:** None
- **Key terms introduced:**
  - probabilistic perspective (منظور احتمالي)
  - conditional probability (الاحتمال الشرطي)
  - parameterized model (نموذج معلمي)
  - parallel training corpus (مدونة تدريب متوازية)
  - RNN Encoder-Decoder (المشفر-مفكك الشفرة للشبكة العصبية التكرارية)
  - hidden state (حالة مخفية)
  - LSTM units (وحدات الذاكرة الطويلة قصيرة المدى)
  - de-convolutional neural network (شبكة عصبية التفافية عكسية)
- **Equations:** Three main equations (1), (2), (3) describing the RNN encoder-decoder
- **Citations:** Multiple references throughout
- **Special handling:**
  - Mathematical notation preserved (h_t, **x**, **y**, ℝⁿ)
  - Explained arg max operation in Arabic context
  - Maintained technical precision for LSTM translation

### Quality Metrics

- Semantic equivalence: 0.89
- Technical accuracy: 0.88
- Readability: 0.87
- Glossary consistency: 0.89
- **Overall section score:** 0.88

### Back-Translation (Key Paragraph)

"From a probabilistic perspective, translation equals finding a target sentence **y** that maximizes the conditional probability of **y** given a source sentence **x**, i.e., arg max_y p(**y** | **x**). In neural machine translation, we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus."

**Validation:** ✅ Accurate preservation of technical and mathematical concepts.
