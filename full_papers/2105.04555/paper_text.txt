
================================================================================
PAGE 1
================================================================================
Customized Monte Carlo Tree Search for LLVM/Pollyâ€™s
Composable Loop Optimization Transformations
Jaehoon Koo, Prasanna Balaprakash, Michael Kruse, Xingfu Wu, Paul Hovland
Argonne National Laboratory, Lemont, IL 60439
{jkoo,pbalapra,michael.kruse,xingfu.wu,hovland}@anl.gov
Mary Hall
University of Utah, Salt Lake City, UT 84103
mhall@cs.utah.edu
ABSTRACT
Polly is the LLVM projectâ€™s polyhedral loop nest optimizer. Re-
cently, user-directed loop transformation pragmas wereproposed
based on LLVM/Clang and Polly. The search space exposed by the
transformationpragmasisatree,whereineachnoderepresentsa
specificcombinationoflooptransformationsthatcanbeapplied
to the code resulting from the parent nodeâ€™s loop transformations.
Wehavedevelopeda searchalgorithmbasedonMonte Carlotree
search(MCTS)tofindthebestcombinationoflooptransformations.
Ouralgorithmconsistsoftwophases:exploringlooptransforma-
tionsatdifferentdepthsofthetreetoidentifypromisingregionsin
thetreesearchspaceandexploitingthoseregionsbyperforming
alocal search.Moreover,a restartmechanism isusedto avoidthe
MCTS getting trapped in a local solution. The best and worst so-
lutionsaretransferredfromthepreviousphasesoftherestartsto
leveragethesearchhistory.Wecompareourapproachwithrandom,
greedy,andbreadth-firstsearchmethodsonPolyBenchkernelsand
ECP proxy applications. Experimental results show that our MCTS
algorithm finds pragma combinations with a speedup of 2.3x over
Pollyâ€™s heuristic optimizations on average.
1 INTRODUCTION
Most compute-intensive programs,in particular scientific applica-
tions in high-performance computing, spend a significant amount
ofexecution timein loops, thusmaking them theprime target for
performanceoptimizations.Commonstrategiesincludereplacing
them by vendor-provided library calls (such as for BLAS [ 32] or
FFT [14]). However, these are available only for a limited set of
kernels, which require the use of a specific data layout in memory
andmaynotbeavailableonallplatforms.Therefore,researchers
have inevitably sought to optimize the loops in the application
itself.
Unfortunately,manualoptimizationisatime-consumingprocess
that requires intimate knowledge about the target hardware and
oftenresultsinlessmaintainablecode.Codegeneratorspromiseso-
lutionsbyautomaticallyoptimizingthesynthesizedcode[ 18]from
adomain-specificproblemdescriptionbutspecializedforspecific
kindsofalgorithms,suchasLIFT[ 40]forBLASandSPIRAL[ 34].
Such code generators have performance-relevant choices to make
thataredifferentforeachplatform.Often,thechoiceisdetermined
byusing autotuning [6]:theprocessofgeneratingasearchspace
of possible implementations/configurations of a kernel or an appli-
cationand evaluatinga subsetof implementations/configurationsonatargetplatformthroughempiricalmeasurementstoidentify
the high-performing implementation/configuration.
The semantics of a program and its performance optimization
can be considered as two separate concerns. With the semantics
of an algorithm fixed, the goal of autotuning is to find a variant
of the program with the lowest time to solution on a given hard-
ware without changing how the algorithm works. Separation of
semanticsandoptimizationparametersisoneofthemotivationsof
Halide[37],whereinakernelandaniterationspacearespecified
in a domain-specific language. This is assigned a separate schedule
that determines the threads and loop nest that execute the ker-
nelovertheentireiterationspace.Theschedulecanbeoptimized
automatically [1, 35, 48] via machine learning (ML) methods.
Another approach is to use directives inside the source code
that describe the optimizations to apply, whereas the code itself
describes only the semantics. This realizes the separation of con-
cerns on any source code containing loops, without being specific
to a domain. In C/C++, pragmas are the most straightforward syn-
taxtoconveyloopoptimizationdirectives[ 16,30].Suchpragmas,
when added to a loop nest, tell the compiler to apply a specific
transformation to the loop, such as unrolling or parallelization.
Polly[22]istheLLVMsubprojectthataddsapolyhedralloopop-
timizertoLLVMâ€™soptimizationpipeline.Bydefault,itusesvarious
heuristicstooptimizealoopnest,includingPLuToâ€™s[ 8]memory
localityoptimizer,andgeneralizesmatrix-multiplicationrecogni-
tion and optimization [ 20], tiling, auto-parallelization, and GPU
offloading [ 23]. Recent work exposed Pollyâ€™s loop transformations
as source code directivesto the programmer[29] by parsing prag-
mas in the source code using Clang [ 33] and passing them to Polly.
These pragmas also build up a search space that can be used for
autotuning[ 31].Thesedirectivesarecomposable;inotherwords,
afterapplyingatransformation,moretransformationscanbeap-
pliedtotheresultsofatransformation.Thesetofadditionalloop
transformations that can be applied depends on the sequence of
prior loop transformations. Consequently, the search space for au-
totuning a sequence of loop transformations is tree-shaped [ 31]
where each node represents a sequence of loop transformations, or
configurations .Thetreesearchspaceispotentiallyunboundedin
depth(e.g.,thechoiceoftilesizecanbeanypositiveintegerandan-
otherlooptransformationcanalwaysbeadded).Consequently,the
numberofpotentiallyusefulconfigurationsistoolargetosearch
exhaustively.
Monte Carlo tree search (MCTS) [ 28], originally proposed to
solvegamesinartificialintelligence,isapromisingheuristicmethod
================================================================================
PAGE 2
================================================================================
to explore treelike search space. However, out-of-the-box MCTS
has several limitations for the treelike search space exposed by
Pollyâ€™sloop transformationpragmas. Toaddressthese limitations,
weintroduceacustomizedMCTStoexploretheconfigurationspace
oflooptransformationpragmas.Thecontributionsofthepaperare
threefold:
Wedevelopan autotuningframeworkto efficientlyexplore
a dynamically growing treelike search space of the newly
developed Clang loop optimization pragmas.
WedesignacustomizedMCTSmethodwithanewreward
mechanism,restartstrategy,andtransferlearningtoimprove
the search efficiency.
We show that the customized MCTS outperforms Pollyâ€™s
optimization heuristic in 16 out of 24 PolyBench kernels
andobtainsaspeedupof2.1onaverage.OnthreeExascale
Computing Project (ECP) proxyapplications, our MCTS sur-
passes Polly in all three and achieves a speedup of 3.7 on
average.
2 BACKGROUND
Inthissectionwefirstdescribelooptransformationpragmasim-
plemented in LLVM Clang/Polly and our treelike search space that
growsdynamically.Wethen providethe mathematical background
and explain the algorithmic mechanism of MCTS.
2.1 Loop transformation directives
Loop transformation directivesâ€”in our case in the form of pragma
annotations in the source codeâ€”instruct the compiler to apply a
specificoptimizationtoaloopinsteadofusingitsownprofitability
heuristics. If multiple transformations are applied to the same loop,
theorderinwhichtheyareappliedissignificant.Forexample,in
thefollowingexampletheloopisfirsttiledbyusingatilesizeof
32; then the tilingâ€™s floor loop (the outer loop) is parallelized.
#pragma clang loop parallelize_thread
#pragma clang loop sizes(32)
for(inti =0; i < n; ++i)
Body(i);
Theeffectofthesedirectivesbecomesmoreevidentwhenweexplic-
itlyexpandthetilingintoloopsasshownbelow.Italsoillustrates
that although the parallelize_thread textually appears before
thetiledirective,itisappliedsecondbecausethedirectivesapply
to what is on the next line.
#pragma clang loop parallelize_thread
for(inti1 =0; i1 < n; i1+= 32) /* floor */
for(inti2 = i1; i2 < i1+ 32&& i2 < n; ++i2) /* tile */
Body(i2);
Theimplementationdescribedin[ 29]consistsofaparsercom-
ponent for LLVM/Clang and a loop transformer with Polly, the
polyhedralloopoptimizer,asillustratedinFigure1.Itispublicly
available in GitHub.1Clang parses the pragma directives and at-
tachesthemasmetadatanodestothelooprepresentedinLLVM-IR.
In LLVMâ€™s optimization pipeline, these are seen by Polly. Before
applyingthedirectivestotheintermediaterepresentation(IR),it
first checks whether the codeâ€™ssemantics is preserved and rejects
1https://github.com/SOLLVE/llvm-project/tree/pragma-clang-loop
-polly-output-loopnest
#pragma â€¦
for (int i=...)
â€¦source.c IR
Assemblyloopnests.jsonCanonicalization passes
Loop optimization passes
Polly
LoopVectorize
Late Mid-End passes
Target backendLLVM
Lexer
ParserPreprocessor
Semantic Analyzer
IR GenerationClang
#pragma
#pragma
LoopmetadataFigure1: Clang/LLVMcompiler architecture
the transformation if it is not preserved. This step can be leveraged
withinautotuning tocheck looptransformations thatproducein-
correct code. The compiler has converted the input into an IR that
is intended for easier processing and has implemented analyses
required to detect violations to semantic constraints that are not
included in the transformation search space.
Asanalternativetouser-directedlooptransformations,Pollycan
useheuristicstooptimizealoopnest.TheheuristicusedbyPolly
is implemented in ISL [ 45] using PLuTO [ 8]. It tries to maximize
parallelismandminimizedistance.Additionally,Pollyappliestiling
and vectorization wherepossible and detects code that implements
matrix-matrix multiplication-like algorithms to which it applies a
dedicated optimization.
2.2 Search space
Most autotuning frameworks and algorithms [ 3,5,38] assume that
thetunableparameterscanbeexpressedinaflatlistandthatthe
searchspaceiscomposedofallpossibleparametercombinations.
Nevertheless, the search space of composable loop transformations
cannotbeexpressedinthisway.Onereasonisthatthesearchspace
is potentially infinite: Any transformation that results in at least
one output loop can be applied an arbitrary number of times. This
may often not make a lot of sense; for instance, partial unrolling
twice with a tile size of four is effectively the same as unrolling
with a factor of 16. We consider this domain knowledge for search
spacepruning.Thesecondreasonisthattheloopneststructures
changes after applying a transformation and therefore the set of
choicesischanging.Asaresult,tilesizesfromoneconfiguration
may be unrelated to the tile sizes of another configuration since
they apply to different loops.
In our trials, we use the same search space as in [ 31] with some
additional transformations and other improvements, such as the
ability to sparsely instantiate a configuration by a numeric id that
doesnotrequireinstantiatingallsiblingconfigurationsjusttoknow
how many there are. This search space takes the form of a tree, as
illustrated in Figure 2. The root node represents the configuration
with no loop transformation, in other words, the original program.
Everynodealsorepresentsaloopneststructuretodeterminewhich
transformationscouldbeapplied.Theloopneststructureoftheroot
node is determined by Clang, written to a JSON file and processed
bythesearchspacegenerator.Everychildnodethenrepresentsa
choice of adding one more transformation, which results in a new
configuration.
2
================================================================================
PAGE 3
================================================================================
tile
parallelizeparallelizefor (...)
Body;source.c
for (...)
for (...)
Body;
for (...)
#pragma clang loop parallelize_thread
for (...)
Body;#pragma clang loop parallelize_thread
for (...)
for (...)
Body;#pragma clang loop parallelize_thread
for (...)
Body;Figure 2: Example of a loop transformation search space.
The root represents the original loop without any transfor-
mations. It can be transformed by either applying paral-
lelization or tiling, resulting in two new configurations. A
parallelized loop is not considered transformable anymore,
but tiling results in two new loops that each can be paral-
lelized independently.
Each node/configuration represents a complete program that
canbecompiledandexecuted,anditsexecutiontimecanbemea-
sured.Theautotuningoptimizationgoalistofindtheconfiguration
withtheminimalexecutiontime,namely,thebestperformance.As
previouslymentioned,however,the compilermayrejectaconfig-
uration when it cannot ensure that all transformations preserve
theprogramâ€™ssemantics.Inthatcase,theconfigurationhastobe
discarded.
Thesearchspacegeneratorappliesthefollowingtransformations
toanode,someofwhichresultinmultiplelooptransformations
withdifferentsettings.
Loop tiling ( #pragma clang loop tile ): This applies to
multiple loops in a perfectly nested loop and applies all
possibletilesizesfromapreconfiguredset.Itresultsintwice
asmanyloopsasintheperfectloopnest.Theoptionalclause
peel(rectangular) instructsPollytobuildaseparateloop
forthepartialtiles,suchthatthecompletetilesdonotrequire
conditionals.
Loop interchange ( #pragma clang loop interchange ): A
new configuration is derived for each permutation of its
loops for each perfect loop nest.
Thread parallelization ( parallelize_thread ): This paral-
lelizesasingleloopusingOpenMP,creatingtheequivalentof
#pragma omp parallel for schedule(static) .Therea-
son for not using the OpenMP directive directly is that
it is not composable with other loop transformations. In
Clang/LLVM,OpenMPloweringisimplementedinthefront
end(seeFigure1), whereaslooptransformations areimple-
mented by Polly. It is not possible to directly apply
#pragma omp parallel for inthe mid-end. Furthermore,
the OpenMP implementation does not verify the legality of
the transformation.
Loopunrolling( #pragma clang loop unrolling ):Thisin-
volvesfullyunrollingalooporpartiallyunrollingbyafactor
from a predefined set.
Loop reversal ( #pragma clang loop reverse ): This exe-
cutes the iterations of loop in reverse order.
Arraypacking( #pragma clang loop pack ):Unliketheother
directives,thisisadatalayouttransformation.Itdetermines
the working set of array elements used in a loop and copies
Figure 3: Four phases of MCTS. The selection phase finds a
path from the root to the leaf node. The expansion phase
creates a child node from the leaf node. The simulation
phase conducts simulation from the expanded node until
it reaches a terminal node and produces an outcome. In
backpropagation, the simulation result is backpropagated
throughtheselectednodes,whichisusedforthesubsequent
selection.
them into a temporary array for the duration of the loop.
Pollytriestomaketheorderoftheelementsinthetempo-
raryarrayresembletheaccessorder.Whenarrayelements
are also written in the loop, the elements are also written
back to the original array. The transformation is most useful
either to make the loopâ€™sworking set fit into a closer cache
inthememoryhierarchyortotransposeamatrixtomake
its accesses consecutive within theloop.
2.3 Monte Carlo tree search
MonteCarlotree search(MCTS) isaheuristic searchmethod that
seeks to solve a class of computationally intractable sequential
decision-making problems, typically represented by trees. In these
trees,anoderepresentsastate(decision)inthesequentialdecision-
making process, and the directed edges represent sequential transi-
tions fromone state to another. MCTS is an iterativeapproach that
performsfour phasesat eachiteration: selection, expansion,simu-
lation,andbackpropagation(seeFigureFigure3foranillustration).
The selection phase traversesonly the part of the tree that was al-
ready visited. It starts from the root node and selects the next node
accordingtoatreepolicy.Ittypicallyendsinaleafnodeofavisited
tree.Theexpansionphaseexpandsthetreebyconsideringchildren
nodesoftheendingleafnodeofthepreviousphaseandselectsone
from among them. The simulation phase performs simulations for
thenewnodebyvisitingsubsequentnodesthatcanbereachedfrom
the newnode.Typically,this involvesseveralrandom walks from
the new node to various terminal nodes that have outcomes. Based
ontheseoutcomes,therewardforeachexploredrandomwalkis
computed starting from the root node. The backpropagation phase
propagates the path-specific reward back to the nodes in the given
path from the terminal node to the root node. Consequently, the
nodes that are common in the high reward branches get reinforced
andhaveahighprobabilityofgettingselectedinthesubsequent
iterations.
Thetree policy used in the selection phase plays a critical role in
balancingtheexplorationofthesearchspaceandtheexploitationof
the already-found promising regions in the search space. A widely
used policy is Upper Confidence Bounds applied to Trees (UCT) [ 4,
3
================================================================================
PAGE 4
================================================================================
28], where a child node E9is selected to maximize
*) =Â¯'9Â¸2s
2;= #
#9Â• (1)
where Â¯'9istheaveragerewardofthenode 9,#isthenumberof
times the current (parent) node has been visited, #9is the number
of times child 9has been visited, and is a constant to balance the
exploration and exploitation. The reward range is Â»0Â•1Â¼[10].
3 CUSTOMIZED MCTS FOR AUTOTUNING
LOOP TRANSFORMATION PRAGMAS
Although MCTS is designed to explore the treelike search space,
the vanilla version cannot be applied directly to our autotuning
problem.First,thereisnoconceptofplayingagameforoptimizing
loop transformation directives. While the reward is trivial in a
game setting with a win or loss, MCTS for autotuning problems
requirescarefulrewardengineering.Second,thevanillaMCTSis
not efficient if a good solution, a high-performing configuration
in our case, is located deep in the tree. Assume that we have a
kernelwheretherearenumerousconfigurationswithonedirective
locatedatthefirstdepthofthetree.However,thebestconfiguration
requiresmorethanonedirective.AsthepureMCTSexpandsanode
onebyonefromtheroot,itrequiresmanyiterationstoreachthe
best configuration located deep in the tree. Third,there is no limit
on the number of loop transformations to form a configuration
(node), which means a terminal node is unknown. In this case the
algorithmneedstodeterminewhentostoptoformaconfiguration.
Fourth, the vanilla MCTS can end up with a poor local solution.
Tocircumventtheseissues,wedevelopedacustomizedMCTS
for autotuning loop transformation pragmas. Our method consists
of two phases: random exploration of configurations at different
depths to find promisingregions, followed by targeted exploitation
ofthepromisingregions.ArestartstrategyisappliedtoMCTSto
avoidbeingstuckinalocalsolution.MCTSrepeatstheexploration
andexploitationaftereachrestartsothatconfigurationsatdifferent
depthscanbesearched.Moreover,itleveragesthesearchhistory
after the restart by transferring the high- and low-performing con-
figurations for reinforcement.
3.1 Moving average reward function
Wedesignourrewardfunctionsuchthatanevaluatedconfigura-
tion(sequenceofpragamsappliedtotheunoptimizedcode)ateach
iterationleadstoawinandaloss.Tothisend,weintroduceautility
function that measures the performance of the configuration and
a target function 5to determine a win or a loss over its perfor-
mance. We use the utility function that computes the speedup of a
configuration Eover an unoptimized code located on the root E0.
The target is computed by using the <-moving average, defined as
follows:
C=execution time of E0
execution time of EC
5CÂ¹<Âº=<0GÂ¹5< 1Â•CÃ•
8=C <Â¸18Â<Âºat iteration C, and the reward function is
'C=8>>> <
>>>:'penalty Â•ifÂ¤EC
1Â• ifCÂ¡5C
0Â• otherwise Â•(2)
whereÂ¤ECdenotes a failure program rejected by the compiler and
'penaltyisanegativeconstant.Thisrewardfunctionprogressively
increases the ability of MCTS to find configurations whose per-
formancesarebetterthantheaverageperformancesofthelast <
configurations.
3.2 Random walk to learn depth
Since the depth of the tree is unbounded, MCTS cannot stop at
a terminal node in a given iteration. Therefore, learning at what
depthtostopisanadditionalcomplexitythatwehavetoaddress
in dealing with the autotuning problem.
Tothatend,weincorporatearandomwalksearchwithinMCTS
to explore configurations at different depths and to find the best
depth to terminate the search for each iteration. This is achieved
as follows. First, we perform =number of random walks each with
randomly generated depth sampled uniformly between 1and3max.
Eachrandomwalkisaconfigurationthatcorrespondstoasequence
of pragmas. We compute their performances by applying them on
the unoptimized code and running them on the target hardware.
Thedepthoftheconfigurationwiththebestperformancethenis
selected as the termination depth for MCTS. We use the random
walknotonlyforlearningthedepthbutalsoforexploration.The
performancevaluesfromtheevaluationareusedtocomputethe
reward and backpropagation.
3.3 Restart mechanism to avoid getting stuck
in a local solution
Even though MCTS is known to find a relatively good solution,
it can get trapped in a local solution [ 44]. Moreover, the random
walkmethodtolearnthedepthcanexacerbatethisissuebecause
the depth once fixed cannot be changed. Specifically, if the random
walkdid not find agoodvalue for the depth,then MCTS is forced
to search with that depth value.
Toaddresstheseissues,weadoptarestartmechanism,wherewe
detect the convergence to a local solution and restart MCTS with a
new random walk. This cycle repeats until a computational budget
is exhausted. The convergence of the search is identified if there is
no improvement in the target value or the algorithm terminates at
the same configuration for more than a threshold.
Since the restart mechanism erases the memory of the MCTS,
it cannot leverage what it has seen in the past. To avoid this is-
sue, we transfer information from the search history as soon as
a new restart is performed. At each restart, we consider all the
configurations evaluated by the MCTS by speedup with respect
to the evaluated code. We compute lower and upper quantiles of
the speedup distribution given by U% and 100- U%, where Uis a
user-defined parameter. For example, when Uis set to 5, the lower
and upper quantiles are 5% and 95%, respectively. We take configu-
rationswithspeedupabove100- U%andreinforcethepathwiththe
positivereward.Similarly,wetaketheconfigurationswithspeedup
below U%andreinforcethepathwiththenegativereward.Itisnot
4
================================================================================
PAGE 5
================================================================================
guaranteed that all of the pragmas from the lower quantile lead to
poor performance. For example, a good pragma can be included in
bothlowerandupperquantiles.Therefore,wepenalizethelower
quantile configurations that do not share any common pragmas
withthe configurations that belong to the upper quantile.
Algorithm1 Customized MCTS
1:Input: 3max,budgets for search,convergencesettings, and U
2:whilestoppingcriterion not met do
3:whileno convergence is detected do
4: 3 RandomWalk (3max)
5: Transfer (U)
6: E MCTS(3)
7:end while
8:end while
Algorithm 1 shows the pseudo-code of the customized MCTS
method. The outer while loop (lines 2â€“8) runs until the stopping
criterionismet.Typically,thisisawall-clocktimeoratotalnumber
of evaluationsallowed. The inner while loop (lines 3â€“7) comprises
arandomwalktofindthedepth 3(RandomWalk (3max)),transfer
of configurations from lower and upper quantiles for negative and
positivereinforcement( Transfer (U)),andMCTSwiththedepth
3(MCTS(3)). Note that the transfer and the random walk are in-
dependentofeachotherandtheorderinwhichtheyareperformed
does not matter.
4 EXPERIMENTS
WeusethePolyBench4.2[ 47]benchmarktestsuiteandECPproxy
applications to evaluate our customized MCTS method. PolyBench
consistsof30numericalcomputationsextractedfromvariousap-
plicationdomains.Itcoverskernelsthatinclude19linearalgebra
computations,3image-processingapplications,6physicssimula-
tions,and2data-miningprocesses.Weselected24kernelswiththe
most levels of nested loops. They are as follows:
Linear Algebra:
- BLAS (7): gemm, gemver, gesummv, symm, syr2k, syrk,
trmm
- Kernels (6): 2mm, 3mm, atax, bicg, doitgen, mvt
- Solvers (3): durbin, lu, ludcmp
Medley (image processing) (2): floyd-warshall, nussinov
Physicssimulation(stencils)(5):adi,fdtd-2d,jacobi-1d,jacobi-
2d, seidel-2d
Data mining (1): covariance.
We use CoMD, miniAMR, and SW4lite from the ECP proxy ap-
plications suite. CoMD2is a reference implementation for algo-
rithms used in molecular dynamics; miniAMR3implements a basic
adaptive mesh refinement method that is often used in physical
simulations; and SW4lite4is a bare-bones version of the larger
SW4 application intended for testing performance optimizations in
SW4â€™s most important numerical kernels.
Because of loop distribution, we manually applied loop transfor-
mationtoincreasethenumberofperfectlynestedloopsandthus
2https://github.com/ECP-copa/CoMD
3https://github.com/Mantevo/miniAMR
4https://github.com/geodynamics/sw4litethenumberofpossibletransformationssuchastiling.Thatis,we
split independent computations in a loop body into two or more
separateloops.ThekernelsfromtheECPproxyapplicationsalso
neededtobecustomizedinordertomakethemtransformableby
Polly.
We compared our customized MCTS method with the following
search methods.
Pollycompile-time heuristics (O3P): This is a baseline with-
out autotuning. It uses Clangâ€™s highest optimization level -O3with
Pollyâ€™s optimization heuristic enabled. Additional command line
flags are -march=native -mllvm -polly-position=early -ml
lvm -polly-parallel -mllvm -polly-omp-backend=LLVM -ml
lvm -polly-scheduling=static thattune the machinecode for
the target and also enable Pollyâ€™s auto-parallelizer.
Random search (RS): It samples each configuration randomly,
wherein a depth is selected at random and a random walk is per-
formed for the selected depth in the search tree.
Breadth first (BF): This is the commonly used breadth-first
search method.
Globalgreedy(GG): Thisisagreedyalgorithmproposedin[ 31]
for treelike search spaces. For a given node, it evaluates all its
childrenandaddsthemtoapriorityqueue,startingwiththeroot
node. The queue is sorted with the most promising configurations
first to be expanded next. We refer the reader to [ 31] for a detailed
exposition of this method.
The experiments were conducted on a Linux machine with 2x
Intel Xeon Platinum 8180M CPU @ 2.50 GHz with 384 GB RAM
memory; GCC 9.2.0; and Clang 13.0.0 with Polly. The search space
includes six loop transformations: loop tiling, loop interchange,
threadparallelization,loopunrolling,loopreversal,andarraypack-
ing. The possible tile sizes are 2, 3, 4, 5, 8, 16, 32, 64, 128, and 256.
The choices for unroll factors are 2, 4, and 8. Each configuration
was executed five times, and the median of the five was selected as
the execution time for the configuration to minimize the impact of
noise. We used the execution time measured by PolyBench itself
usingthe -DPOLYBENCH_TIME preprocessoroption.Thismeasures
only the kernel execution time and excludes overhead such as exe-
cutablestartuptimeandarrayinitialization.Allthesearchmethods
exceptO3P wereallowed to runfor a wall-clock timeof six hours
or until 1,000 unique configurations were evaluated. For MCTS,
we used an exploration weight of 0.1 and Uof 0.05. Since the
first phase does exploration, a relatively small value of exploration
weight was selected such that the second phase focused on the
exploitation. In the search experiments, 3maxis set to five based
on a preliminary experiment on depth. Specifically, we found that
most of the configurations with the depth value greater than 5 fail
tocompileorresultincodetransformationerrors.Welimiteach
MCTS run to 300 evaluations. The convergence in the search is
detectedbylackofimprovementsfor50iterationsorvisitingthe
same configuration for 10 iterations in a row.
4.1 Comparison between search methods
Table 1 shows the comparison of different search methods. For a
givenkernel,thespeedupisgivenbytheratioofruntimesofthe
O3P and the best configuration obtained by the search method.
We observe that the MCTS obtains better performance than the
5
================================================================================
PAGE 6
================================================================================
Table 1: Performance comparison on 24 PolyBench kernels
KernelsSpeedup overO3P
BF GG RSMCTS
2mm 0.2206 0.2743 0.2716 1.4684
3mm 0.1619 0.1972 0.2044 0.3559
adi 0.1857 0.1799 0.1955 0.2071
atax 2.0037 3.3488 1.9356 3.5023
bicg 2.4986 2.5116 1.8525 1.8030
covariance 0.1668 0.2614 2.0257 2.5567
doitgen 0.1165 0.1163 0.8353 0.8135
durbin 1.0185 1.0211 1.0205 1.0135
fdtd-2d 0.6532 0.6534 0.6411 0.6503
floyd-warshall 2.7333 8.5391 8.1178 8.5425
gemm 0.7922 1.1433 0.9471 6.9786
gemver 0.6213 0.8255 0.6578 1.0221
gesummv 1.6231 1.6391 1.5628 3.2101
jacobi-1d 3.5876 3.7376 3.4982 3.6886
jacobi-2d 2.7010 2.6982 2.6490 2.6906
lu 0.4192 0.4248 0.4109 0.4202
ludcmp 0.9431 1.1567 1.1285 1.1534
mvt 0.8202 1.7041 1.1722 1.7395
nussinov 1.2574 1.2548 1.2577 1.2538
seidel-2d 0.8226 0.8221 0.8217 0.8225
symm 0.7379 0.7381 0.9046 0.7289
syr2k 0.2855 0.3019 0.3198 2.8777
syrk 0.4200 0.4227 0.4097 3.4533
trmm 0.1622 0.1827 0.1702 0.1647
O3P on 16 out of 24 kernels (speedup values greater than 1). The
observed speedups range from1.01 (durbin)to 8.54 (floyd-warshal).
The BF, GG, and RS outperform the O3P only on 8, 11, and 11,
kernels,respectively.TheaveragespeedupsoverO3Pon24kernels
are2.13,1.42,1.38,and1.04forMCTS,GG,RS,andBF,respectively.
Moreover, we observe that MCTS obtains the best speedup values
for10outof24kernels((boldfacedforeachkernel).TheGG,BF,and
RSmethodsobtainthebestspeedupvaluesfor4,1,and1kernels,
respectively. On 8 kernels, none of the search methods was able to
outperform O3P (italicized in the table).
Figure 4 shows the speedup (with respect to O3P) of the best
configurationfoundoverthenumberofevaluationsfordifferent
search methods. We observe that the customized MCTS finds high-
performing configurations within few iterations (100 to 200) when
compared with other search methods. Moreover, because of the
restart search strategy, the MCTS is not trapped in a local solution
as other search algorithms are (see gemm, 2mm, and covariance).
WeinvestigatedwhyPollyâ€™sheuristicwasperformingwellon
the eight kernels. We found out that there are transformations that
we currently cannot do with directives but that Polly can by using
its heuristics. These include unroll-and-jam, loop fusion, loop peel-
ingexceptforpeel(rectangular),wavefronting/skewing,indexset
splitting, and code motion. The Polly heuristics can also add anno-
tations that help the LoopVectorizer do a better loop optimization.
Figure 5 shows depths of the tree at which the best configu-
ration was found for the search methods. The MCTS finds thebestconfiguration mostlyat depth 2or 3,while othersearchalgo-
rithms find depth 1 or 2. Regarding directives of the best config-
uration,#pragma clang loop parallelize_thread is included
in the best configuration in most cases.
Table2:PerformancecomparisonofthreeECPproxyappli-
cations
ApplicationsSpeedup over O3P
BFGG RSMCTS
CoMD 1.38681.38541.38491.3883
miniAMR 1.03311.04020.99708.4163
SW4lite 1.19461.10901.10941.2038
Table2showsaperformancecomparisonof autotuningresults
onECPproxyapplications.SimilartoPolyBenchresults,wefind
thatourMCTSperformsbetterthanO3P,BF,GG,andRS,obtain-
ing the highest speedup over O3P, 1.38, 8.41, and 1.20 for CoMD,
miniAMR, and SW4lite, respectively.
To analyze the effectiveness of the search methods, we gathered
speedupsfromallthealgorithmsandcomputedacutoffbasedonthe
top 5% speedup values. We then counted the number of configura-
tionsthatareabovethiscutoffvalueoverthenumberofevaluations.
Figure6 showsthe results.Besides obtaining the highest speedup
configuration,MCTSfindsalargernumberofconfigurationsabove
the cutoff value. Moreover, upon further inspection of the best con-
figurations, we found that MCTS finds configurations that contain
differentpragmasforeachapplication,namely,unrollingandarray
packingfor CoMD,threadparallelization for miniAMR, and tiling
with different size selections for SW4lite.
4.2 Analysis
Weselectthegemmkernelforfurtheranalysis.First,weemploy
a decision tree regression method [ 9] to model the relationship
betweenthepragmasandtheobservedspeedups.Weusetheresult-
ingdecisiontreetounderstandthemostimportantpragmasforthe
gemmkernel.Specifically,thepragmasthatcontributetomaximum
variance in the speedups are placed at the top of the tree. The deci-
siontreeisimplementedand visualizedbyusingaPython library,
dtreeviz.5Figure 7 shows a decision tree, where we can observe
that the most important pragma is #pragma clang loop parall
elize_thread .Notethatgivenapragmainthetree,theright(left)
ofthetreemeanspresence(absence)ofthepragma.Inaddition,the
speedupwiththispragmaishigherintheleafnodesintheright
partofthedecisiontreethanintheleft.Inthefollowinglevelofthe
decisiontree,pragmasofdifferenttilesizesareselectedasimpor-
tant pragmas, while #pragma clang loop parallelize_thread
and#pragma clang loop tile_size(256) emerge as important
pragmas for several restarts.
Figure 8 shows the trajectory of the depth explored and the
speedup obtained over the number of evaluations. We observe
thatthesearchhasrepetitivephasesduetorestart.Eachphaseis
characterized by the random walk and the MCTS run. The random
walkresultsinconfigurationswithanequalnumberofdepths.The
5https://github.com/parrt/dtreeviz
6
================================================================================
PAGE 7
================================================================================
(a) gemm
 (b) 2mm
(c) durbin
 (d) floyd-warshall
(e) adi
 (f) covariance
Figure 4: Speedup of the best configuration over number of evaluations for different search methods on PolyBench kernels
MCTSexploitsthepromisingconfigurationfoundintherandom
walk by sampling more configurations nearby. The search reaches
a convergence, and a restart is performed. The effect of restart is
alsovisibleinthesecondphase.Whilethesearchstagnateswith
aspeedupof13xovertheunoptimizedcode,restarthelpsMCTS
find configurations with a speedup of 14x in the second phase.
We benchmarked Intelâ€™s Math Kernel Library (MKL) dgemm
routine.Itwasimplementedonthesamemachineasweusedinour
otherexperiments.TheexecutiontimesobtainedbyMKL,MCTS,
andO3Pare0.0056,0.0272,and0.1897,respectively.Eventhough
MCTSoutperforms O3P ongemm as wellas other kernels,MKLâ€™s
optimization obtains the fastest time for gemm since it is designed
to achieve the highest performance on the Intel machine.5 RELATED WORK
Several autotuning frameworks have been developed to support
autotuning regardless of the domain, such as cTuning-cc/Milepost
GCC[19],OpenTuner[ 3],ATF[38],ytopt[5],andmctree[ 31].The
autotuningframeworksdescribedin[ 17,39,42]aredevelopedfora
user-definedsequenceoftransformationsinwhichonlyasubsetof
transformationparameterscanbeautotuned.Therearealsodomain-
specific autotuning frameworks for image and array processing,
auch as HalideTuner [48], ProTuner [26], and the worksin [1, 35];
AutoTVM [ 12] and ReLeASE [ 2] for ML model compilation and
accelerators;andPATUS[ 13]andLIFT[ 24]forstencilcomputation.
Many approaches in the literature for solving loop optimization
problems use ML and reinforcement learning (RL) and, recently,
deep learning approaches. Early studies applied ML approaches to
solveacodeoptimizationproblem.Forexample,Stocketal.[ 41]
7
================================================================================
PAGE 8
================================================================================
Figure5:Depthofthebestconfigurationfoundbydifferent
search methods on the PolyBench kernels
appliedseveralMLmodelssuchasneuralnetworks,binarytrees,
SVM, and linear regression with hand-crafted features to solve
auto-vectorizationforlooppermutation,vectorizedloop,andun-
roll. Killian et al. [ 27] proposed a heuristics algorithm for auto-
vectorization based on SVM that provides optimized codes without
expertknowledge.Theauthorsinvestigatedhowtocombinevec-
torizationreportswithiterativecompilationandcodegeneration
to craft features. In their paper, SVM is used to predict the speedup
of a program given a sequence of optimization steps.
Recently, state-of-the-art deep learning models are being in-
troduced for loop optimization. In [ 15], Cummins et al. solved a
loop optimization problem by a deep learning model developed
basedonrecurrentneuralnetworkswithLongshort-termmemory.
The proposed model takes codes as texts for input and predicts
heterogeneousmappingforeitherGPUsorCPUsandthesizeof
thread coarsening factor 2f1Â•2Â• Â” Â” Â” Â• 32g. They verified their model
by benchmarking the previous method based on decision tree [ 21]
on several datasets such as NPB, PolyBench, Parboil, SHOC, and
Rodinia.In[ 2],Ahnetal.developedanRL-basedalgorithmtosolve
a compilation problem for deep neural networks built on top of
TVM [11]. Haj-Ali et al. [ 25] proposed a deep RL-based method for
loopoptimizationthatdeterminesvectorizedfactors.TheirDNN
policyagenttakescodeembeddingsasinputstateswheretheem-
beddings are computed by Code2Vec [ 43]. The agent determines
twovectorizedfactorssuchasvectorizationwidthandinterleave
counts, and run time is used for a reward. The researchers evalu-
ated the proposed algorithm by benchmarking Polly, decision tree,
feed-forward networks, and nearest-neighbor search. In [ 46], an
autotuningframeworkbasedonBayesianoptimization(BO)was
proposedtoexploretheparameterspacesearch.BOmodelssuch
as random forests, extra trees, gradient-boosted regression trees,
and Gaussian processes are selected as a learner in the proposed
framework. Performance of the framework is tested on six Poly-
Bench kernels with the latest LLVM Clang/Polly loop optimization
pragmas. However, these frameworks are limited to the search
spaceexpressedinavectorspace.Asearchtreeconsideredinthe
literaturedoesnotdirectlymaptothespaceofafixednumberof
knobs.
Recent literature solves loop optimization by defining search
spaceasatree,suchasin[ 1]and[26]fortheHalidescheduler,in[ 7]for Telamon,and in [ 31] for LLVM Clang/Polly loop optimization
pragmas.
Tree search algorithms including MCTS are introduced to op-
timize Halide [ 36] schedules. To tackle the limitation of a vector
search space for Halide studied in [ 48] and [35], Adams et al. [ 1]
proposedanautotuningframeworkbasedonbeamsearchovera
tree-shaped search space. Since each node in the search tree rep-
resents an intermediate schedule, a cost model based on neural
networks that output the dot product of a vector of hand-designed
termsandavectorofcoefficientsisusedtopredicttheruntime.The
authors verified that the proposed framework finds better configu-
rationsthanthe previousmethod,HalideTuner[ 48],does.Haj-Ali
etal.[26]furtherimprovedHalidescheduleautotuningbyapplying
MCTS,whichtacklesthenon-globalbehaviorofthepreviousbeam
search approach. The authors report that the schedules found by
MCTSareupto3.6timesfasterthanthosefoundbyexistingsearch
methods such as random search, greedy search with :of 1, and
the previous beam search. Since their problem space is only Halide
schedules,however,thesemethodsdonotapplytoarbitrarysource
C/C++ source codes as our schedules do.
6 CONCLUSIONS AND FUTURE WORK
Wedevelopedanautotuningframeworktoidentifyhigh-performing
pragmacombinationsforlooptransformations.BasedonLLVM/Clang
and Polly, our search space has a tree structure and grows dynami-
callysuchthatthespaceispotentiallyinfinite.Toefficientlyexplore
a large search space, we developed a customized Monte Carlo tree
search (MCTS) into the autotuning framework together with a
newrewardmechanism,restartstrategy,andtransferlearningto
improve the search efficiency. We evaluated the proposed frame-
workon24PolyBenchkernelsandthreeECPproxyapplications.
Intheexperiments,theproposedMCTSsurpassesPollyâ€™sheuris-
ticoptimizationon16PolyBenchkernelsandallthreeECPproxy
applications. In addition, our method outperforms other search
algorithmsforbreadth-first,globalgreedy,andrandomsearch.We
obtainaspeedupoverPollyâ€™soptimizationof2.30onaverageinthe
whole experiment, compared with the other methods that achieve
1.06, 1.40, and 1.35 for the breadth-first, global greedy, and random
search, respectively.
For our future work, we plan to further refine the tree search
algorithm as well as the search space. We plan to implement the
remaining loop transformations that the Polly heuristic can use
(unroll-and-jam,loopdistribution/fusion,etc.)butarecurrentlynot
partofthesearchspace.Withtheseimplemented,weexpectthat
autotuningwillbebetterthanPollyâ€™scompile-timeheuristic.We
currently do not prune the search space for infeasible transforma-
tions (such as reversing a loop twice) and identical configurations
that can be reached by different transformation sequences. The
latterwouldmakethesearchspaceadirectedacyclicgraphinstead
of a tree.
ACKNOWLEDGMENT
ThisresearchwassupportedbytheExascaleComputingProject(17-
SC-20-SC), a collaborative effort of the U.S. Department of Energy
OfficeofScienceandtheNationalNuclearSecurityAdministration.
8
================================================================================
PAGE 9
================================================================================
(a) CoMD
 (b) miniAMR
 (c) SW4lite
Figure 6: Number of configurations that are above this cutoff value over the number of evaluations
Figure 7: Decision tree visualizing relationships between pragmas and observed speedup for the gemm kernel
Figure 8: Trajectory of depth and speedup by the MCTS algorithm. Green and blue lines show the target value and the best
speedup obtained over the iterations.
9
================================================================================
PAGE 10
================================================================================
REFERENCES
[1]Adams, A. et al. Learning to optimize Halide with tree search and random
programs. ACM Trans. on Graphics 38 , 4 (2019), 1â€“12.
[2]Ahn,B.H.,Pilligundla,P.,andEsmaeilzadeh,H. Reinforcementlearningand
adaptivesamplingforoptimizedDNNcompilation. In Proc.ofthe2013IEEE/ACM
Int. Symp. on Code Generation and Optimization (Shenzhen, China, 2019), ICML
2019, IEEE, pp.1â€“10.
[3]Ansel, J., Kamil, S., Veeramachaneni, K., Ragan-Kelley, J., Bosboom, J.,
Oâ€™Reilly,U.-M.,andAmarasinghe,S. OpenTuner:Anextensibleframeworkfor
program autotuning. In 2014 23rd Int. Conf. on Parallel Architecture and Compila-
tionTechniques (Edmonton,AB,Canada,2014),PACT2014, IEEE,pp.303â€“â€“316.
[4]Auer,P.,Cesa-Bianchi,N.,andFischer,P. Finite-timeanalysisofthemulti-
armed bandit problem. Machine Learning 47 (May 2002), 235â€“256.
[5]Balaprakash,P.,Egele,R.,andHovland,P. ytopt.GitHubrepository.Argonne
National Laboratory.
[6]Balaprakash,P.etal. Autotuninginhigh-performancecomputingapplications.
Proc. of the IEEE 106 , 11 (2018), 2068â€“2083.
[7]Beaugnon,U.,ClÃ©ment,B.,Tollenaere,N.,andCohen,A. Ontherepresenta-
tionofpartiallyspecifiedimplementationsanditsapplicationtotheoptimization
of linear algebra kernels on GPU,2019.
[8]Bondhugula,U.,Hartono,A.,Ramanujam,J.,andSadayappan,P. Apractical
automaticpolyhedralparallelizerandlocalityoptimizer. In Proc.ofthe29thACM
SIGPLANConf.onProgrammingLanguageDesignandImplementation (NewYork,
NY,USA, 2008), PLDI â€™08, Association for Computing Machinery,p.101â€“113.
[9]Breiman, L., Friedman, J., Stone, C. J., and Olshen, R. Classification and
RegressionTrees . Chapman and Hall/CRC, 1984.
[10]Browne,C. et al. Asurveyof Monte Carlo tree searchmethods. IEEETrans.on
Computational Intelligence and AI in Games 4 ,1 (2012), 1â€“43.
[11]Chen,T.,Moreau,T.,Jiang,Z.,Zheng,L.,Yan,E.,Shen,H.,Cowan,M.,Wang,
L.,Hu,Y.,Ceze,L.,etal. fTVMg:Anautomatedend-to-endoptimizingcompiler
for deep learning. In 13thfUSENIXgSymp. on Operating Systems Design and
Implementation (OSDI18) (2018), pp. 578â€“594.
[12]Chen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L., Guestrin, C.,
and Krishnamurthy, A. Learning to optimize tensor programs. In Advances in
NeuralInformationProcessingSystems (2018),S.Bengio,H.Wallach,H.Larochelle,
K.Grauman,N.Cesa-Bianchi,andR.Garnett,Eds.,vol.31,CurranAssociates,
Inc.
[13]Christen,M.,Schenk,O.,andBurkhart,H. PATUS:Acodegenerationand
autotuning framework for parallel iterative stencil computations on modern
microarchitectures. In 2011 IEEE Int. Parallel Distributed Processing Symp. (2011),
pp.676â€“687.
[14]Cooley, J. W., and Tukey, J. W. An algorithm for the machine calculation of
complexFourier series. Mathematics of computation 19 , 90 (1965), 297â€“301.
[15]Cummins, C., Petoumenos, P., Wang, Z., and Leather, H. End-to-end deep
learning of optimization heuristics. In Proc. of 2017 26th Int. Conf. on Parallel
Architectures and Compilation Techniques (Portland, OR, USA, 2017), PACT 2017,
IEEE, pp.219â€“232.
[16]Donadio, S. et al. A Language for the Compact Representation of Multiple
Program Versions. In Proc. of the 18th Int. Workshop on Languages and Compilers
for Parallel Computing (LCPCâ€™05) (2006),Springer,pp.136â€“151.
[17]Farvardin, K., Finkel, H., Kruse, M., and Reppy, J. atJIT: A just-in-time au-
totuning compiler for C++. LLVM Developerâ€™s Meeting Technical Talk, Oct.
2018.
[18]Finkel, H. et al. Program Synthesis for Scientific Computing Report. Tech.
rep., Argonne National Laboratory, 04 2020. https://www.anl.gov/cels/program-
synthesis-for-scientific-computing-report.
[19]Fursin, G. et al. Milepost GCC: Machine learning enabled self-tuning compiler.
Int. Journal of Parallel Programming 39 ,3 (January 2011), 296â€“327.
[20]Gareev,R.,Grosser, T.,andKruse, M. High-performancegeneralizedtensor
operations: A compiler-oriented approach. ACM Trans. on Architecture and Code
Optimization (TACO)15 , 3 (2018), 1â€“27.
[21]Grewe,D.,Wang,Z.,andOâ€™Boyle,M.F.P. Portablemappingofdataparallel
programs to OpenCL for heterogeneous systems. In Proc. of the 2013 IEEE/ACM
Int.Symp.onCodeGenerationandOptimization (Shenzhen,China,2013),CGO
2013, IEEE, pp.1â€“10.
[22]Grosser,T.,GrÃ¶sslinger,A.,andLengauer,C. Pollyâ€“PerformingPolyhedral
OptimizationsonaLow-LevelIntermediateRepresentation. ParallelProcessing
Letters 22 , 04 (2012).
[23]Grosser, T., and Hoefler, T. Polly-ACC transparent compilation to hetero-
geneous hardware. In Proc. of the 2016 Int. Conf. on Supercomputing (2016),
pp.1â€“13.
[24]Hagedorn,B.,Stoltzfus,L.,Steuwer,M.,Gorlatch,S.,andDubach,C. High
performancestencilcodegenerationwithlift. In Proc.ofthe2018Int.Symp.on
CodeGenerationandOptimization (GA,USA,2018),CGO2018,ACM,pp.100â€“112.
[25]Haj-Ali, A., Ahmed, N. K., Willke, T., Shao, Y. S., Asanovic, K., and Stoica, I.
NeuroVectorizer: End-to-end vectorization with deep reinforcement learning. In
Proc. of the 18th ACM/IEEE Int. Symp. on Code Generation and Optimization (SanDiego, CA, USA, 2020), CGO 2020, ACM, pp. 242â€“â€“255.
[26]Haj-Ali, A., Genc, H., Huang, Q., Moses, W., Wawrzynek, J., AsanoviÄ‡, K.,
and Stoica, I. ProTuner: Tuning programswith Monte Carlo tree search,2020.
[27]Killian, W., Miceli, R., Park, E., Vega, M. A., and Cavazos, J. Performance
improvement in kernels by guiding compiler auto-vectorization heuristics. Part-
nership for Advanced Computing in Europe (PRACE) (2014), 1â€“11.
[28]Kocsis,L.,andSzepesvÃ¡ri,C. BanditbasedMonte-Carloplanning. In Proc.ofthe
SeventeenthEuropeanConf.onMachineLearning(ECML2006) (Berlin,Heidelberg,
Germany, 2006), J. FÃ¼rnkranz, T. Scheffer, and M. Spiliopoulou, Eds., vol. 4212 of
Lecture Notes in Computer Science , Springer,pp.282â€“293.
[29]Kruse, M., and Finkel, H. User-directed loop-transformations in Clang. In Proc.
ofthe5thWorkshopontheLLVMCompilerInfrastructureinHPC (Dallas,TX,USA,
2018), LLVM-HPC 2018, IEEE, pp. 1â€“10.
[30]Kruse, M., and Finkel, H. Design and use of loop-transformation prag-
mas. InOpenMP:ConqueringtheFullHardwareSpectrum (Cham,2019),X.Fan,
B. R. de Supinski, O. Sinnen, and N. Giacaman, Eds., Springer Int. Publishing,
pp. 125â€“139.
[31]Kruse, M., Finkel, H., and Wu, X. Autotuning search space for loop trans-
formations. In Proc. of 2020 IEEE/ACM 6th Workshop on the LLVM Compiler
Infrastructurein HPC (LLVM-HPC) andWorkshopon HierarchicalParallelism for
Exascale Computing (GA, USA, 2020), HiPar 2020, IEEE, pp. 12â€“22.
[32]Lawson, C. L., Hanson, R. J., Kincaid, D. R., and Krogh, F. T. Basic linear
algebrasubprogramsforFortranusage. ACMTrans.Math.Softw.5 ,3(Sept.1979),
308â€“323.
[33]LLVMContributors . Clang:AClanguagefamilyfrontendforLLVM. software
project. http://clang.llvm.org.
[34]Moura, J. M. F., Johnson, J., Johnson, R. W., Padua, D., Prasanna, V. K.,
PÃ¼schel, M., and Veloso, M. SPIRAL: Automatic implementation of signal
processing algorithms. In High Performance Extreme Computing (HPEC) (2000).
[35]Mullapudi, R. T., Adams, A., Sharlet, D., Ragan-Kelley, J., and Fatahalian,
K.Automaticallyscheduling Halide image processing pipelines. ACMTrans. on
Graphics 35 , 4 (2016), 1â€“12.
[36]Ragan-Kelley,J.,Barnes,C.,Adams,A.,Paris,S.,Durand,F.,andAmaras-
inghe,S. Halide:Alanguageandcompilerforoptimizingparallelism,locality,
and recomputation in image processing pipelines. Acm Sigplan Notices 48 , 6
(2013), 519â€“530.
[37]Ragan-Kelley, J. et al. Decoupling algorithms from schedules for easy opti-
mization of image processing pipelines. ACMTrans.Graph. 31 , 4 (July 2012).
[38]Rasch,A.,Haidl,M.,andGorlatch,S. ATF:Agenericauto-tuningframework.
In2017IEEE19thInt.Conf.onHighPerformanceComputingandCommunications;
IEEE15thInt.Conf.onSmartCity;IEEE3rdInt.Conf.onDataScienceandSystems
(Edmonton, AB, Canada, 2017), HPCC/SmartCity/DSS 2017, IEEE, pp.303â€“â€“316.
[39]Sreenivasan, V., Javali, R., Hall, M., Balaprakash, P., Scogland, T. R. W.,
and de Supinski, B. R. Autotuning search space for loop transformations. In In
OpenMP: Conquering the Full Hardware Spectrum (2019), IWOMP 2019, Springer,
Cham, pp. 50â€“60.
[40]Steuwer, M., Remmelg, T., and Dubach, C. LIFT: A functional data-parallel
IR for high-performance GPU code generation. In 2017 IEEE/ACMInt. Symp.on
Code Generation and Optimization (CGO) (2017), pp.74â€“85.
[41]Stock,K.,noelPouchet,L.,andSadayappan,P. Usingmachinelearningtoim-
proveautomaticvectorization. ACMTrans.onArchitectureandCodeOptimization
8, 4 (2012), 1â€“23.
[42]Tiwari,A.,Chen,C.,Chame,J.,Hall,M.,andHollingsworth,J.K. Ascalable
auto-tuningframeworkforcompileroptimization. In Proc.ofthe23rdIEEEInt.
Parallel And Distributed Computing Symp. (IPDPSâ€™09) (2009), IEEE.
[43]Uri,Zilberstein,M.,Levy,O.,andYahav,E. code2vec:Learningdistributed
representationsofcode. In Proc.ofthe ACMonProgrammingLanguages (2019),
POPL 2019, ACM, pp. 1â€“29.
[44]vandenHerik,H.J.,Kuipers,J.,Vermaseren,J.A.,andPlaat,A. Investigations
withMonteCarlotreesearchforfindingbettermultivariate schemes. In Proc.of
Int. Conf. on Agents and Artificial Intelligence (ICAART 2013) (Berlin, Heidelberg,
Germany,2014),vol.449of CommunicationsinComputerandInformationScience ,
Springer, pp. 3â€“20.
[45]Verdoolaege, S. isl: An integer set library for the polyhedral model. In Mathe-
maticalSoftwareâ€“ICMS2010 (Berlin,Heidelberg,2010),K.Fukuda,J.v.d.Hoeven,
M. Joswig, and N. Takayama, Eds., Springer Berlin Heidelberg, pp. 299â€“302.
[46]Wu, X. et al. Autotuning polybench benchmarks with LLVM Clang/Polly loop
optimizationpragmasusingBayesianoptimization. In Proc.of2020IEEE/ACMPer-
formance Modeling, Benchmarking and Simulation of High Performance Computer
Systems(GA, USA, 2020), PMBS 2020, IEEE, pp. 61â€“70.
[47]Yuki, T., and Pouchet, L.-N. PolyBench 4.2, 2016.
[48]Zingales, G. P. E. T. HalideTuner: Generating and tuning Halide schedules
with OpenTuner. Masterâ€™s thesis, Massachusetts Institute of Technology (MIT),
Cambridge, Massachusetts, USA, 2015.
10
================================================================================
PAGE 11
================================================================================
The submitted manuscript has been created by
UChicagoArgonne,LLC,OperatorofArgonneNa-
tionalLaboratory(â€œArgonneâ€).Argonne,aU.S.De-
partmentofEnergyOfficeofSciencelaboratory,is
operatedunderContractNo.DE-AC02-06CH11357.
The U.S. Government retains for itself, and others
acting on its behalf, a paid-up nonexclusive, irrev-
ocable worldwide license in said article to repro-
duce,preparederivativeworks,distributecopies
to the public, and perform publicly and display
publicly,byoronbehalfoftheGovernment.The
DepartmentofEnergywillprovidepublicaccess
to these results of federally sponsored research in
accordancewiththeDOEPublicAccessPlan.http:
//energy.gov/downloads/doe-public-access-plan
11