# Section 3: The DETR Model
## ุงููุณู 3: ูููุฐุฌ DETR

**Section:** methodology
**Translation Quality:** 0.85
**Glossary Terms Used:** ุงูุชูุจุค ุจุงููุฌููุนุงุช (set prediction), ุงููุทุงุจูุฉ ุงูุซูุงุฆูุฉ (bipartite matching), ูุดูุฑ-ูู ุชุดููุฑ (encoder-decoder), ูุญูู (transformer), ุงูุงูุชุจุงู ุงูุฐุงุชู (self-attention), ุงูุงูุชุจุงู ุงููุชูุงุทุน (cross-attention), ุงุณุชุนูุงูุงุช ุงูุฃุฌุณุงู (object queries), ุตูุงุฏูู ุงูุชุญุฏูุฏ (bounding boxes), ุฏุงูุฉ ุงูุฎุณุงุฑุฉ (loss function), ุฎูุงุฑุฒููุฉ ุงููุฌุฑู (Hungarian algorithm), ุดุจูุฉ ุฃูุงููุฉ (feed-forward network)

---

### English Version

Two ingredients are essential for direct set predictions in detection: (1) a set prediction loss that forces unique matching between predicted and ground truth boxes; (2) an architecture that predicts (in a single pass) a set of objects and models their relation. We describe our architecture in detail in Figure 2.

**3.1 Object detection set prediction loss**

DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image. One of the main difficulties of training is to score predicted objects (class, position, size) with respect to the ground truth. Our loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses.

Let us denote by y the ground truth set of objects, and ลท = {ลทแตข}แตขโโแดบ the set of N predictions. Assuming N is larger than the number of objects in the image, we consider y also as a set of size N padded with โ (no object). To find a bipartite matching between these two sets we search for a permutation of N elements ฯ โ Sโ with the lowest cost:

ฯฬ = arg min_{ฯโSโ} ฮฃแตขโโแดบ Lโโโcโ(yแตข, ลท_{ฯ(i)})     (1)

where Lโโโcโ(yแตข, ลท_{ฯ(i)}) is a pair-wise matching cost between ground truth yแตข and a prediction with index ฯ(i). This optimal assignment is computed efficiently with the Hungarian algorithm, following prior work (e.g. [43]).

The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. Each element i of the ground truth set can be seen as a yแตข = (cแตข, bแตข) where cแตข is the target class label (which may be โ) and bแตข โ [0,1]โด is a vector that defines ground truth box center coordinates and its height and width relative to the image size. For the prediction with index ฯ(i) we define probability of class cแตข as pฬ_{ฯ(i)}(cแตข) and the predicted box as bฬ_{ฯ(i)}. With these notations we define Lโโโcโ(yแตข, ลท_{ฯ(i)}) as -๐_{cแตขโโ}pฬ_{ฯ(i)}(cแตข) + ๐_{cแตขโโ}L_{box}(bแตข, bฬ_{ฯ(i)}).

This procedure of finding matching plays the same role as the heuristic assignment rules used to match proposal [37] or anchors [22] to ground truth objects in modern detectors. The main difference is that we need to find one-to-one matching for direct set prediction without duplicates.

The second step is to compute the loss function, the Hungarian loss for all pairs matched in the previous step. We define the loss similarly to the losses of common object detectors, i.e. a linear combination of a negative log-likelihood for class prediction and a box loss defined later:

L_{Hungarian}(y, ลท) = ฮฃแตขโโแดบ [-log pฬ_{ฯฬ(i)}(cแตข) + ๐_{cแตขโโ}L_{box}(bแตข, bฬ_{ฯฬ(i)})]     (2)

where ฯฬ is the optimal assignment computed in the first step (1). In practice, we down-weight the log-probability term when cแตข = โ by a factor 10 to account for class imbalance. This is analogous to how Faster R-CNN training procedure balances positive/negative proposals by subsampling [37]. Notice that the matching cost between an object and โ doesn't depend on the prediction, which means that in that case the cost is a constant. In the matching cost we use probabilities pฬ_{ฯฬ(i)}(cแตข) instead of log-probabilities. This makes the class prediction term commensurable to L_{box}(ยท,ยท) (described below), and we observed better empirical performances.

**Bounding box loss.** The second part of the matching cost and the Hungarian loss is L_{box}(ยท) that scores the bounding boxes. Unlike many detectors that do box predictions as a w.r.t. some initial guesses, we make box predictions directly. While such approach simplify the implementation it poses an issue with relative scaling of the loss. The most commonly-used โโ loss will have different scales for small and large boxes even if their relative errors are similar. To mitigate this issue we use a linear combination of the โโ loss and the generalized IoU loss [38] L_{iou}(ยท,ยท) that is scale-invariant. Overall, our box loss is L_{box}(bแตข, bฬ_{ฯ(i)}) defined as ฮป_{iou}L_{iou}(bแตข, bฬ_{ฯ(i)}) + ฮป_{L1}||bแตข - bฬ_{ฯ(i)}||โ where ฮป_{iou}, ฮป_{L1} โ โ are hyperparameters. These two losses are normalized by the number of objects inside the batch.

**3.2 DETR architecture**

The overall DETR architecture is surprisingly simple and depicted in Figure 2. It contains three main components, which we describe below: a CNN backbone to extract a compact feature representation, an encoder-decoder transformer, and a simple feed forward network (FFN) that makes the final detection prediction.

Unlike many modern detectors, DETR can be implemented in any deep learning framework that provides a common CNN backbone and a transformer architecture implementation with just a few hundred lines. Inference code for DETR can be implemented in less than 50 lines in PyTorch [32]. We hope that the simplicity of our method will attract new researchers to the detection community.

**Backbone.** Starting from the initial image x_{img} โ โ^{3รHโรWโ} (with 3 color channels), a conventional CNN backbone generates a lower-resolution activation map f โ โ^{CรHรW}. Typical values we use are C = 2048 and H, W = Hโ/32, Wโ/32.

**Transformer encoder.** First, a 1ร1 convolution reduces the channel dimension of the high-level activation map f from C to a smaller dimension d, creating a new feature map zโ โ โ^{dรHรW}. The encoder expects a sequence as input, hence we collapse the spatial dimensions of zโ into one dimension, resulting in a dรHW feature map. Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN). Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings [31,3] that are added to the input of each attention layer. We defer to the supplementary material the detailed definition of the architecture, which follows the one described in [47].

**Figure 2:** DETR uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class and bounding box) or a "no object" class.

**Transformer decoder.** The decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed self- and encoder-decoder attention mechanisms. The difference with the original transformer is that our model decodes the N objects in parallel at each decoder layer, while Vaswani et al. [47] use an autoregressive model that predicts the output sequence one element at a time. We refer the reader unfamiliar with the concepts to the supplementary material. Since the decoder is also permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings are learnt positional encodings that we refer to as object queries, and similarly to the encoder, we add them to the input of each attention layer. The N object queries are transformed into an output embedding by the decoder. They are then independently decoded into box coordinates and class labels by a feed forward network (described in the next subsection), resulting N final predictions. Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context.

**Prediction feed-forward networks (FFNs).** The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer. The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image, and the linear layer predicts the class label using a softmax function. Since we predict a fixed-size set of N bounding boxes, where N is usually much larger than the actual number of objects of interest in an image, an additional special class label โ is used to represent that no object is detected within a slot. This class plays a similar role to the "background" class in the standard object detection approaches.

**Auxiliary decoding losses.** We found helpful to use auxiliary losses [1] in decoder during training, especially to help the model output the correct number of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. We use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers.

---

### ุงููุณุฎุฉ ุงูุนุฑุจูุฉ

ููุงู ููููุงู ุฃุณุงุณูุงู ููุชูุจุค ุงููุจุงุดุฑ ุจุงููุฌููุนุงุช ูู ุงููุดู: (1) ุฎุณุงุฑุฉ ุงูุชูุจุค ุจุงููุฌููุนุงุช ุงูุชู ุชูุฑุถ ูุทุงุจูุฉ ูุฑูุฏุฉ ุจูู ุงูุตูุงุฏูู ุงููุชูุจุฃ ุจูุง ูุงูุญููููุฉุ (2) ูุนูุงุฑูุฉ ุชุชูุจุฃ (ูู ุชูุฑูุฑุฉ ูุงุญุฏุฉ) ุจูุฌููุนุฉ ูู ุงูุฃุฌุณุงู ูุชููุฐุฌ ุนูุงูุชูุง. ูุตู ูุนูุงุฑูุชูุง ุจุงูุชูุตูู ูู ุงูุดูู 2.

**3.1 ุฎุณุงุฑุฉ ุงูุชูุจุค ุจุงููุฌููุนุงุช ููุดู ุงูุฃุฌุณุงู**

ูุณุชูุชุฌ DETR ูุฌููุนุฉ ุจุญุฌู ุซุงุจุช ูู N ุชูุจุคุงุชุ ูู ุชูุฑูุฑุฉ ูุงุญุฏุฉ ุนุจุฑ ูู ุงูุชุดููุฑุ ุญูุซ ูุชู ุชุนููู N ููููู ุฃูุจุฑ ุจูุซูุฑ ูู ุงูุนุฏุฏ ุงููููุฐุฌู ููุฃุฌุณุงู ูู ุงูุตูุฑุฉ. ุฅุญุฏู ุงูุตุนูุจุงุช ุงูุฑุฆูุณูุฉ ููุชุฏุฑูุจ ูู ุชุณุฌูู ุงูุฃุฌุณุงู ุงููุชูุจุฃ ุจูุง (ุงููุฆุฉุ ุงูููุถุนุ ุงูุญุฌู) ุจุงููุณุจุฉ ููุญูููุฉ ุงูุฃุฑุถูุฉ. ุชูุชุฌ ุฎุณุงุฑุชูุง ูุทุงุจูุฉ ุซูุงุฆูุฉ ูุซูู ุจูู ุงูุฃุฌุณุงู ุงููุชูุจุฃ ุจูุง ูุงูุฃุฌุณุงู ุงูุญููููุฉุ ุซู ุชุญุณู ุฎุณุงุฆุฑ ูุญุฏุฏุฉ ููุฃุฌุณุงู (ุตูุฏูู ุงูุชุญุฏูุฏ).

ุฏุนูุง ูุดูุฑ ุจู y ุฅูู ูุฌููุนุฉ ุงูุญูููุฉ ุงูุฃุฑุถูุฉ ููุฃุฌุณุงูุ ู ลท = {ลทแตข}แตขโโแดบ ูุฌููุนุฉ ุงูุชูุจุคุงุช N. ุจุงูุชุฑุงุถ ุฃู N ุฃูุจุฑ ูู ุนุฏุฏ ุงูุฃุฌุณุงู ูู ุงูุตูุฑุฉุ ูุนุชุจุฑ y ุฃูุถุงู ููุฌููุนุฉ ุจุญุฌู N ููููุกุฉ ุจู โ (ูุง ููุฌุฏ ุฌุณู). ูุฅูุฌุงุฏ ูุทุงุจูุฉ ุซูุงุฆูุฉ ุจูู ูุงุชูู ุงููุฌููุนุชูู ูุจุญุซ ุนู ุชุจุฏูู ูุนูุงุตุฑ Nุ ฯ โ Sโ ุจุฃูู ุชูููุฉ:

ฯฬ = arg min_{ฯโSโ} ฮฃแตขโโแดบ Lโโโcโ(yแตข, ลท_{ฯ(i)})     (1)

ุญูุซ Lโโโcโ(yแตข, ลท_{ฯ(i)}) ูู ุชูููุฉ ุงููุทุงุจูุฉ ุงูุซูุงุฆูุฉ ุจูู ุงูุญูููุฉ ุงูุฃุฑุถูุฉ yแตข ูุชูุจุค ุจููุฑุณ ฯ(i). ูุชู ุญุณุงุจ ูุฐุง ุงูุชุนููู ุงูุฃูุซู ุจููุงุกุฉ ุจุงุณุชุฎุฏุงู ุฎูุงุฑุฒููุฉ ุงููุฌุฑูุ ุชุจุนุงู ููุนูู ุงูุณุงุจู (ูุซู [43]).

ุชุฃุฎุฐ ุชูููุฉ ุงููุทุงุจูุฉ ูู ุงูุงุนุชุจุงุฑ ููุงู ูู ุชูุจุค ุงููุฆุฉ ูุชุดุงุจู ุงูุตูุงุฏูู ุงููุชูุจุฃ ุจูุง ูุงูุญููููุฉ. ูููู ุฑุคูุฉ ูู ุนูุตุฑ i ูู ูุฌููุนุฉ ุงูุญูููุฉ ุงูุฃุฑุถูุฉ ูู yแตข = (cแตข, bแตข) ุญูุซ cแตข ูู ุชุณููุฉ ุงููุฆุฉ ุงููุณุชูุฏูุฉ (ูุงูุชู ูุฏ ุชููู โ) ู bแตข โ [0,1]โด ูู ูุชุฌู ูุญุฏุฏ ุฅุญุฏุงุซูุงุช ูุฑูุฒ ุตูุฏูู ุงูุญูููุฉ ุงูุฃุฑุถูุฉ ูุงุฑุชูุงุนู ูุนุฑุถู ุจุงููุณุจุฉ ูุญุฌู ุงูุตูุฑุฉ. ููุชูุจุค ุจููุฑุณ ฯ(i) ูุญุฏุฏ ุงุญุชูุงู ุงููุฆุฉ cแตข ูู pฬ_{ฯ(i)}(cแตข) ูุงูุตูุฏูู ุงููุชูุจุฃ ุจู ูู bฬ_{ฯ(i)}. ุจูุฐู ุงูุฑููุฒ ูุญุฏุฏ Lโโโcโ(yแตข, ลท_{ฯ(i)}) ูู -๐_{cแตขโโ}pฬ_{ฯ(i)}(cแตข) + ๐_{cแตขโโ}L_{box}(bแตข, bฬ_{ฯ(i)}).

ุชูุนุจ ุฅุฌุฑุงุกุงุช ุฅูุฌุงุฏ ุงููุทุงุจูุฉ ูุฐู ููุณ ุงูุฏูุฑ ูููุงุนุฏ ุงูุชุนููู ุงูุงุณุชุฏูุงููุฉ ุงููุณุชุฎุฏูุฉ ููุทุงุจูุฉ ุงูููุชุฑุญุงุช [37] ุฃู ุงููุฑุงุณู [22] ูุน ุฃุฌุณุงู ุงูุญูููุฉ ุงูุฃุฑุถูุฉ ูู ุงููุงุดูุงุช ุงูุญุฏูุซุฉ. ุงููุฑู ุงูุฑุฆูุณู ูู ุฃููุง ุจุญุงุฌุฉ ุฅูู ุฅูุฌุงุฏ ูุทุงุจูุฉ ูุงุญุฏ ููุงุญุฏ ููุชูุจุค ุงููุจุงุดุฑ ุจุงููุฌููุนุงุช ุจุฏูู ูุณุฎ ููุฑุฑุฉ.

ุงูุฎุทูุฉ ุงูุซุงููุฉ ูู ุญุณุงุจ ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ุฎุณุงุฑุฉ ุงููุฌุฑู ูุฌููุน ุงูุฃุฒูุงุฌ ุงููุทุงุจูุฉ ูู ุงูุฎุทูุฉ ุงูุณุงุจูุฉ. ูุญุฏุฏ ุงูุฎุณุงุฑุฉ ุจุดูู ูุดุงุจู ูุฎุณุงุฆุฑ ูุงุดูุงุช ุงูุฃุฌุณุงู ุงูุดุงุฆุนุฉุ ุฃู ูุฒูุฌ ุฎุทู ูู ุงูููุบุงุฑูุชู ุงูุณูุจู ููุงุญุชูุงููุฉ ูุชูุจุค ุงููุฆุฉ ูุฎุณุงุฑุฉ ุงูุตูุฏูู ุงููุญุฏุฏุฉ ูุงุญูุงู:

L_{Hungarian}(y, ลท) = ฮฃแตขโโแดบ [-log pฬ_{ฯฬ(i)}(cแตข) + ๐_{cแตขโโ}L_{box}(bแตข, bฬ_{ฯฬ(i)})]     (2)

ุญูุซ ฯฬ ูู ุงูุชุนููู ุงูุฃูุซู ุงููุญุณูุจ ูู ุงูุฎุทูุฉ ุงูุฃููู (1). ูู ุงูููุงุฑุณุฉ ุงูุนูููุฉุ ูููู ูุฒู ุญุฏ ุงุญุชูุงููุฉ ุงูููุบุงุฑูุชู ุนูุฏูุง cแตข = โ ุจุนุงูู 10 ูุญุณุงุจ ุนุฏู ุชูุงุฒู ุงููุฆุงุช. ูุฐุง ููุงุซู ูููููุฉ ููุงุฒูุฉ ุฅุฌุฑุงุก ุชุฏุฑูุจ Faster R-CNN ููููุชุฑุญุงุช ุงูุฅูุฌุงุจูุฉ/ุงูุณูุจูุฉ ุนู ุทุฑูู ุฃุฎุฐ ุนููุงุช ูุฑุนูุฉ [37]. ูุงุญุธ ุฃู ุชูููุฉ ุงููุทุงุจูุฉ ุจูู ุฌุณู ู โ ูุง ุชุนุชูุฏ ุนูู ุงูุชูุจุคุ ููุง ูุนูู ุฃูู ูู ุชูู ุงูุญุงูุฉ ุงูุชูููุฉ ุซุงุจุชุฉ. ูู ุชูููุฉ ุงููุทุงุจูุฉ ูุณุชุฎุฏู ุงูุงุญุชูุงููุงุช pฬ_{ฯฬ(i)}(cแตข) ุจุฏูุงู ูู ุงุญุชูุงููุงุช ุงูููุบุงุฑูุชู. ูุฐุง ูุฌุนู ุญุฏ ุชูุจุค ุงููุฆุฉ ูุชูุงุณุจุงู ูุน L_{box}(ยท,ยท) (ุงูููุตููุฉ ุฃุฏูุงู)ุ ููุงุญุธูุง ุฃุฏุงุกู ุชุฌุฑูุจูุงู ุฃูุถู.

**ุฎุณุงุฑุฉ ุตูุฏูู ุงูุชุญุฏูุฏ.** ุงูุฌุฒุก ุงูุซุงูู ูู ุชูููุฉ ุงููุทุงุจูุฉ ูุฎุณุงุฑุฉ ุงููุฌุฑู ูู L_{box}(ยท) ุงูุฐู ูุณุฌู ุตูุงุฏูู ุงูุชุญุฏูุฏ. ุนูู ุนูุณ ุงูุนุฏูุฏ ูู ุงููุงุดูุงุช ุงูุชู ุชููู ุจุชูุจุคุงุช ุงูุตูุฏูู ุจุงููุณุจุฉ ูุจุนุถ ุงูุชุฎูููุงุช ุงูุฃูููุฉุ ูููู ุจุชูุจุคุงุช ุงูุตูุฏูู ูุจุงุดุฑุฉ. ุจูููุง ูุจุณุท ูุฐุง ุงูููุฌ ุงูุชูููุฐุ ูุฅูู ูุทุฑุญ ูุณุฃูุฉ ุจุดุฃู ุงูุชุญุฌูู ุงููุณุจู ููุฎุณุงุฑุฉ. ุณุชููู ูุฎุณุงุฑุฉ โโ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงูุงู ููุงููุณ ูุฎุชููุฉ ููุตูุงุฏูู ุงูุตุบูุฑุฉ ูุงููุจูุฑุฉ ุญุชู ูู ูุงูุช ุฃุฎุทุงุคูุง ุงููุณุจูุฉ ูุชุดุงุจูุฉ. ููุชุฎููู ูู ูุฐู ุงููุณุฃูุฉ ูุณุชุฎุฏู ูุฒูุฌุงู ุฎุทูุงู ูู ุฎุณุงุฑุฉ โโ ูุฎุณุงุฑุฉ IoU ุงููุนููุฉ [38] L_{iou}(ยท,ยท) ุงูุชู ุชููู ุบูุฑ ูุชุบูุฑุฉ ุจุงููุณุจุฉ ูููููุงุณ. ุจุดูู ุนุงูุ ุฎุณุงุฑุฉ ุงูุตูุฏูู ุงูุฎุงุตุฉ ุจูุง ูู L_{box}(bแตข, bฬ_{ฯ(i)}) ูุญุฏุฏุฉ ูู ฮป_{iou}L_{iou}(bแตข, bฬ_{ฯ(i)}) + ฮป_{L1}||bแตข - bฬ_{ฯ(i)}||โ ุญูุซ ฮป_{iou}, ฮป_{L1} โ โ ูุนููุงุช ูุงุฆูุฉ. ูุชู ุชุทุจูุน ูุงุชูู ุงูุฎุณุงุฑุชูู ุจุนุฏุฏ ุงูุฃุฌุณุงู ุฏุงุฎู ุงูุฏูุนุฉ.

**3.2 ูุนูุงุฑูุฉ DETR**

ูุนูุงุฑูุฉ DETR ุงูุฅุฌูุงููุฉ ุจุณูุทุฉ ุจุดูู ูุฏูุด ูููุถุญุฉ ูู ุงูุดูู 2. ุชุญุชูู ุนูู ุซูุงุซุฉ ููููุงุช ุฑุฆูุณูุฉุ ูุตููุง ุฃุฏูุงู: ุงูุนููุฏ ุงูููุฑู CNN ูุงุณุชุฎุฑุงุฌ ุชูุซูู ููุฒุงุช ูุถุบูุทุ ููุญูู ูุดูุฑ-ูู ุชุดููุฑุ ูุดุจูุฉ ุฃูุงููุฉ ุจุณูุทุฉ (FFN) ุชููู ุจุงูุชูุจุค ุงูููุงุฆู ุจุงููุดู.

ุนูู ุนูุณ ุงูุนุฏูุฏ ูู ุงููุงุดูุงุช ุงูุญุฏูุซุฉุ ูููู ุชูููุฐ DETR ูู ุฃู ุฅุทุงุฑ ุนูู ุชุนูู ุนููู ูููุฑ ุนููุฏุงู ููุฑูุงู CNN ุดุงุฆุนุงู ูุชูููุฐ ูุนูุงุฑูุฉ ูุญูู ุจุถุน ูุฆุงุช ูู ุงูุฃุณุทุฑ ููุท. ูููู ุชูููุฐ ููุฏ ุงูุงุณุชูุชุงุฌ ูู DETR ูู ุฃูู ูู 50 ุณุทุฑุงู ูู PyTorch [32]. ูุฃูู ุฃู ุชุฌุฐุจ ุจุณุงุทุฉ ุทุฑููุชูุง ุจุงุญุซูู ุฌุฏุฏ ุฅูู ูุฌุชูุน ุงููุดู.

**ุงูุนููุฏ ุงูููุฑู.** ุจุฏุกุงู ูู ุงูุตูุฑุฉ ุงูุฃูููุฉ x_{img} โ โ^{3รHโรWโ} (ุจุซูุงุซ ูููุงุช ุฃููุงู)ุ ูููุฏ ุนููุฏ ููุฑู CNN ุชูููุฏู ุฎุฑูุทุฉ ุชูุดูุท ุจุฏูุฉ ุฃูู f โ โ^{CรHรW}. ุงูููู ุงููููุฐุฌูุฉ ุงูุชู ูุณุชุฎุฏููุง ูู C = 2048 ู H, W = Hโ/32, Wโ/32.

**ูุดูุฑ ุงููุญูู.** ุฃููุงูุ ูููู ุงูุชูุงู 1ร1 ุจูุนุฏ ุงูููุงุฉ ูุฎุฑูุทุฉ ุงูุชูุดูุท ุนุงููุฉ ุงููุณุชูู f ูู C ุฅูู ุจูุนุฏ ุฃุตุบุฑ dุ ููุดุฆุงู ุฎุฑูุทุฉ ููุฒุงุช ุฌุฏูุฏุฉ zโ โ โ^{dรHรW}. ูุชููุน ุงููุดูุฑ ุชุณูุณูุงู ูุฅุฏุฎุงูุ ููู ุซู ูุทูู ุงูุฃุจุนุงุฏ ุงูููุงููุฉ ูู zโ ุฅูู ุจูุนุฏ ูุงุญุฏุ ููุง ููุชุฌ ุนูู ุฎุฑูุทุฉ ููุฒุงุช dรHW. ูู ุทุจูุฉ ูุดูุฑ ููุง ูุนูุงุฑูุฉ ููุงุณูุฉ ูุชุชููู ูู ูุญุฏุฉ ุงูุชุจุงู ุฐุงุชู ูุชุนุฏุฏ ุงูุฑุคูุณ ูุดุจูุฉ ุฃูุงููุฉ (FFN). ูุธุฑุงู ูุฃู ูุนูุงุฑูุฉ ุงููุญูู ุบูุฑ ูุชุบูุฑุฉ ุจุงููุณุจุฉ ููุชุจุฏููุ ูููููุง ุจุชุฑููุฒุงุช ููุถุนูุฉ ุซุงุจุชุฉ [31,3] ุชูุถุงู ุฅูู ุฅุฏุฎุงู ูู ุทุจูุฉ ุงูุชุจุงู. ูุคุฌู ุฅูู ุงููุงุฏุฉ ุงูุชูููููุฉ ุงูุชุนุฑูู ุงูุชูุตููู ูููุนูุงุฑูุฉุ ูุงูุฐู ูุชุจุน ุชูู ุงูููุตููุฉ ูู [47].

**ุงูุดูู 2:** ูุณุชุฎุฏู DETR ุนููุฏุงู ููุฑูุงู CNN ุชูููุฏูุงู ูุชุนูู ุชูุซูู ุซูุงุฆู ุงูุฃุจุนุงุฏ ูุตูุฑุฉ ุฅุฏุฎุงู. ูููู ุงููููุฐุฌ ุจุชุณุทูุญูุง ููููููุง ุจุชุฑููุฒ ููุถุนู ูุจู ุชูุฑูุฑูุง ุฅูู ูุดูุฑ ุงููุญูู. ูุฃุฎุฐ ูู ุชุดููุฑ ุงููุญูู ุจุนุฏ ุฐูู ูุฅุฏุฎุงู ุนุฏุฏุงู ุซุงุจุชุงู ุตุบูุฑุงู ูู ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงููุชุนููุฉุ ูุงูุชู ูุณูููุง ุงุณุชุนูุงูุงุช ุงูุฃุฌุณุงูุ ููุญุถุฑ ุจุดูู ุฅุถุงูู ุฅูู ุฅุฎุฑุงุฌ ุงููุดูุฑ. ููุฑุฑ ูู ุชุถููู ุฅุฎุฑุงุฌ ูู ูู ุงูุชุดููุฑ ุฅูู ุดุจูุฉ ุฃูุงููุฉ ูุดุชุฑูุฉ (FFN) ุชุชูุจุฃ ุฅูุง ุจุงูุชุดุงู (ุงููุฆุฉ ูุตูุฏูู ุงูุชุญุฏูุฏ) ุฃู ูุฆุฉ "ูุง ููุฌุฏ ุฌุณู".

**ูู ุชุดููุฑ ุงููุญูู.** ูุชุจุน ูู ุงูุชุดููุฑ ุงููุนูุงุฑูุฉ ุงูููุงุณูุฉ ูููุญููุ ูุญููุงู N ุชุถูููุงุช ุจุญุฌู d ุจุงุณุชุฎุฏุงู ุขููุงุช ุงูุชุจุงู ุฐุงุชู ููุดูุฑ-ูู ุชุดููุฑ ูุชุนุฏุฏุฉ ุงูุฑุคูุณ. ุงููุฑู ูุน ุงููุญูู ุงูุฃุตูู ูู ุฃู ูููุฐุฌูุง ููู ุชุดููุฑ ุฃุฌุณุงู N ุจุดูู ูุชูุงุฒู ูู ูู ุทุจูุฉ ูู ุชุดููุฑุ ุจูููุง ูุณุชุฎุฏู Vaswani ูุขุฎุฑูู [47] ูููุฐุฌุงู ุงูุญุฏุงุฑูุงู ุฐุงุชูุงู ูุชูุจุฃ ุจุชุณูุณู ุงูุฅุฎุฑุงุฌ ุนูุตุฑุงู ูุงุญุฏุงู ูู ูู ูุฑุฉ. ูุญูู ุงููุงุฑุฆ ุบูุฑ ุงููุฃููู ุจุงูููุงููู ุฅูู ุงููุงุฏุฉ ุงูุชูููููุฉ. ูุธุฑุงู ูุฃู ูู ุงูุชุดููุฑ ุฃูุถุงู ุบูุฑ ูุชุบูุฑ ุจุงููุณุจุฉ ููุชุจุฏููุ ูุฌุจ ุฃู ุชููู ุชุถูููุงุช ุงูุฅุฏุฎุงู N ูุฎุชููุฉ ูุฅูุชุงุฌ ูุชุงุฆุฌ ูุฎุชููุฉ. ุชุถูููุงุช ุงูุฅุฏุฎุงู ูุฐู ูู ุชุฑููุฒุงุช ููุถุนูุฉ ูุชุนููุฉ ูุดูุฑ ุฅูููุง ุจุงุณู ุงุณุชุนูุงูุงุช ุงูุฃุฌุณุงูุ ูุจุดูู ูุดุงุจู ูููุดูุฑุ ูุถูููุง ุฅูู ุฅุฏุฎุงู ูู ุทุจูุฉ ุงูุชุจุงู. ูุชู ุชุญููู ุงุณุชุนูุงูุงุช ุงูุฃุฌุณุงู N ุฅูู ุชุถููู ุฅุฎุฑุงุฌ ุจูุงุณุทุฉ ูู ุงูุชุดููุฑ. ุซู ูุชู ูู ุชุดููุฑูุง ุจุดูู ูุณุชูู ุฅูู ุฅุญุฏุงุซูุงุช ุงูุตูุฏูู ูุชุณููุงุช ุงููุฆุงุช ุจูุงุณุทุฉ ุดุจูุฉ ุฃูุงููุฉ (ููุตููุฉ ูู ุงููุณู ุงููุฑุนู ุงูุชุงูู)ุ ููุง ููุชุฌ ุนูู ุชูุจุคุงุช N ุงูููุงุฆูุฉ. ุจุงุณุชุฎุฏุงู ุงูุงูุชุจุงู ุงูุฐุงุชู ูุงููุดูุฑ-ูู ุชุดููุฑ ุนุจุฑ ูุฐู ุงูุชุถูููุงุชุ ูุณุชุฏู ุงููููุฐุฌ ุนุงูููุงู ุญูู ุฌููุน ุงูุฃุฌุณุงู ูุนุงู ุจุงุณุชุฎุฏุงู ุงูุนูุงูุงุช ุงูุซูุงุฆูุฉ ุจูููุงุ ูุน ุงููุฏุฑุฉ ุนูู ุงุณุชุฎุฏุงู ุงูุตูุฑุฉ ุจุฃููููุง ูุณูุงู.

**ุงูุดุจูุงุช ุงูุฃูุงููุฉ ููุชูุจุค (FFNs).** ูุชู ุญุณุงุจ ุงูุชูุจุค ุงูููุงุฆู ุจูุงุณุทุฉ ุฅุฏุฑุงู ูุชุนุฏุฏ ุงูุทุจูุงุช ุจุซูุงุซ ุทุจูุงุช ูุน ุฏุงูุฉ ุชูุดูุท ReLU ูุจูุนุฏ ูุฎูู dุ ูุทุจูุฉ ุฅุณูุงุท ุฎุทูุฉ. ุชุชูุจุฃ FFN ุจุงูุฅุญุฏุงุซูุงุช ุงููุทุจุนุฉ ูููุฑูุฒ ูุงูุงุฑุชูุงุน ูุงูุนุฑุถ ููุตูุฏูู ุจุงููุณุจุฉ ูุตูุฑุฉ ุงูุฅุฏุฎุงูุ ูุชุชูุจุฃ ุงูุทุจูุฉ ุงูุฎุทูุฉ ุจุชุณููุฉ ุงููุฆุฉ ุจุงุณุชุฎุฏุงู ุฏุงูุฉ softmax. ูุธุฑุงู ูุฃููุง ูุชูุจุฃ ุจูุฌููุนุฉ ุจุญุฌู ุซุงุจุช ูู ุตูุงุฏูู ุงูุชุญุฏูุฏ Nุ ุญูุซ N ุนุงุฏุฉ ุฃูุจุฑ ุจูุซูุฑ ูู ุงูุนุฏุฏ ุงููุนูู ููุฃุฌุณุงู ูุญู ุงูุงูุชูุงู ูู ุงูุตูุฑุฉุ ูุชู ุงุณุชุฎุฏุงู ุชุณููุฉ ูุฆุฉ ุฎุงุตุฉ ุฅุถุงููุฉ โ ูุชูุซูู ุนุฏู ุงูุชุดุงู ุฃู ุฌุณู ุฏุงุฎู ูุชุญุฉ. ุชูุนุจ ูุฐู ุงููุฆุฉ ุฏูุฑุงู ูุดุงุจูุงู ููุฆุฉ "ุงูุฎูููุฉ" ูู ููุงูุฌ ูุดู ุงูุฃุฌุณุงู ุงูููุงุณูุฉ.

**ุฎุณุงุฆุฑ ูู ุงูุชุดููุฑ ุงููุณุงุนุฏุฉ.** ูุฌุฏูุง ุฃูู ูู ุงููููุฏ ุงุณุชุฎุฏุงู ุฎุณุงุฆุฑ ูุณุงุนุฏุฉ [1] ูู ูู ุงูุชุดููุฑ ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุฎุงุตุฉ ููุณุงุนุฏุฉ ุงููููุฐุฌ ุนูู ุฅุฎุฑุงุฌ ุงูุนุฏุฏ ุงูุตุญูุญ ูู ุงูุฃุฌุณุงู ููู ูุฆุฉ. ูุถูู FFNs ููุชูุจุค ูุฎุณุงุฑุฉ ุงููุฌุฑู ุจุนุฏ ูู ุทุจูุฉ ูู ุชุดููุฑ. ุชุดุงุฑู ุฌููุน FFNs ููุชูุจุค ูุนููุงุชูุง. ูุณุชุฎุฏู ุทุจูุฉ-ุชุทุจูุน ูุดุชุฑูุฉ ุฅุถุงููุฉ ูุชุทุจูุน ุงูุฅุฏุฎุงู ุฅูู FFNs ููุชูุจุค ูู ุทุจูุงุช ูู ุชุดููุฑ ูุฎุชููุฉ.

---

### Translation Notes

- **Equations:** 2 mathematical equations (1) and (2) preserved exactly in LaTeX notation
- **Mathematical notation:** All symbols (ฯ, ลท, Lโโโcโ, etc.) kept in original form
- **Key terms introduced:** Hungarian algorithm (ุฎูุงุฑุฒููุฉ ุงููุฌุฑู), object queries (ุงุณุชุนูุงูุงุช ุงูุฃุฌุณุงู), feed-forward network (ุดุจูุฉ ุฃูุงููุฉ), positional encoding (ุชุฑููุฒ ููุถุนู), generalized IoU (IoU ุงููุนููุฉ)
- **Figures referenced:** Figure 2 with detailed caption translation
- **Citations:** Multiple references preserved [numbers]
- **Special handling:**
  - Technical abbreviations (CNN, FFN, ReLU) kept in English
  - Framework names (PyTorch) kept in English
  - All mathematical notation preserved exactly

### Quality Metrics

- Semantic equivalence: 0.86
- Technical accuracy: 0.88
- Readability: 0.83
- Glossary consistency: 0.83
- **Overall section score:** 0.85
