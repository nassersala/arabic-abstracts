# Section 2: Related Work
## القسم 2: الأعمال ذات الصلة

**Section:** related-work
**Translation Quality:** 0.87
**Glossary Terms Used:** transformer, multi-task learning, sequence modeling, foundation models, pre-training, large language model (LLM), embodied agents, robot manipulation, imitation learning, visual representations, benchmark

---

### English Version

**Multi-Task Learning by Sequence Modeling.** Transformers (Vaswani et al., 2017) have enabled task unification across many AI domains (Brown et al., 2020; Chen et al., 2022a;b; Lu et al., 2022; Wang et al., 2022c). For example, in NLP, the Natural Language Decathlon (McCann et al., 2018) adopts a consistent question-answering format for a suite of 10 NLP tasks. T5 (Raffel et al., 2020) unifies all language problems into the same text-to-text format. GPT-3 (Brown et al., 2020) and Megatron (Shoeybi et al., 2019) demonstrate emergent behaviours of intuitive task specifications by zero-shot prompting. In computer vision, Pix2Seq (Chen et al., 2022b) casts many vision problems into a unified sequence format. Florence (Yuan et al., 2021), BiT (Kolesnikov et al., 2020), and MuST (Ghiasi et al., 2021) pre-train shared backbone models at scale for general visual representations and transfer them to downstream tasks. In multimodal learning, Perceiver (Jaegle et al., 2021b;a) proposes an efficient architecture to handle structured inputs and outputs. Flamingo (Alayrac et al., 2022) and Frozen (Tsimpoukelli et al., 2021) design a universal API that ingests interleaving sequences of images and text and generates free-form text. Gato (Reed et al., 2022) is a massively multi-task model across NLP, vision, and embodied agents. Our work is most similar in spirit to Gato, but we focus primarily on enabling an intuitive multimodal prompting interface for a generalist robot agent.

**Foundation Models for Embodied Agents.** Foundation models (Bommasani et al., 2021) have demonstrated strong emergent properties. There are many ongoing efforts to replicate this success for embodied agents (Yang et al., 2023), focusing on 3 aspects. 1) Transformer agent architecture: Decision Transformer and Trajectory Transformer (Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022; Xu et al., 2022; 2023) leverage the powerful self-attention models for sequential decision making. CLIPort (Shridhar et al., 2021), Perceiver-Actor (Shridhar et al., 2022), and RT-1 (Brohan et al., 2022) apply large transformers to robot manipulation tasks. BeT (Shafiullah et al., 2022) and C-BeT (Cui et al., 2022) design novel techniques to learn from demonstrations with multiple modes with transformers. 2) Pre-training for better representations: MaskViT (Gupta et al., 2022b), R3M (Nair et al., 2022), VIP (Ma et al., 2022), and VC-1 (Majumdar et al., 2023) pre-train general visual representations for robotic perception. Li et al. (2022b) fine-tunes from LLM checkpoints to accelerate policy learning. MineDojo (Fan et al., 2022) and Ego4D (Grauman et al., 2021) provide large-scale multimodal databases to facilitate scalable policy training. 3) LLMs for robot learning: SayCan (Ahn et al., 2022) leverages PaLM (Chowdhery et al., 2022) for zero-shot concept grounding. Huang et al. (2022a), Inner Monologue (Huang et al., 2022b) and LM-Nav (Shah et al., 2022) apply LLMs to long-horizon robot planning. PaLM-E (Driess et al., 2023) is instead a multimodal language model that can be repurposed for sequential robotic manipulation planning. Ours differs from these works in our novel multimodal prompting formulation, which existing LLMs do not easily support.

**Robot Manipulation and Benchmarks.** A wide range of robot manipulation tasks require different skills and task specification formats, such as instruction following (Stepputtis et al., 2020), one-shot imitation (Finn et al., 2017; Duan et al., 2017), rearrangement (Batra et al., 2020), constraint satisfaction (Brunke et al., 2021a), and reasoning (Shridhar et al., 2020). Multiple physics simulation benchmarks are introduced to study the above tasks. For example, iGibson (Shen et al., 2020; Li et al., 2021; Srivastava et al., 2021; Li et al., 2022a) simulates interactive household scenarios. Ravens (Zeng et al., 2020) and Robosuite (Zhu et al., 2020; Fan et al., 2021) design various tabletop manipulation tasks with realistic robot arms. CALVIN (Mees et al., 2021) develops long-horizon language-conditioned tasks. Meta-World (Yu et al., 2019) is a widely used simulator benchmark studying robotics manipulation with tabletop settings. CausalWorld (Ahmed et al., 2021) is a benchmark for causal structure and transfer learning in manipulation, requiring long-horizon planning and precise low-level motor control. AI2-THOR (Ehsani et al., 2021; Deitke et al., 2022) is a framework that supports visual object manipulation and procedural generation of environments. Our VIMA-BENCH is the first robot learning benchmark to support multimodal-prompted tasks. We also standardize the evaluation protocol to systematically measure an agent's generalization capabilities.

An extended review can be found in Appendix, Sec. F.

---

### النسخة العربية

**التعلم متعدد المهام بواسطة نمذجة التسلسل.** مكّنت المحولات (Vaswani et al., 2017) من توحيد المهام عبر العديد من مجالات الذكاء الاصطناعي (Brown et al., 2020; Chen et al., 2022a;b; Lu et al., 2022; Wang et al., 2022c). على سبيل المثال، في معالجة اللغة الطبيعية، يعتمد Natural Language Decathlon (McCann et al., 2018) تنسيقاً موحداً للإجابة على الأسئلة لمجموعة من 10 مهام في معالجة اللغة الطبيعية. يوحد T5 (Raffel et al., 2020) جميع مشاكل اللغة في نفس تنسيق النص إلى النص. يظهر GPT-3 (Brown et al., 2020) و Megatron (Shoeybi et al., 2019) سلوكيات ناشئة لتحديدات المهام البديهية عن طريق الموجهات بدون أمثلة. في رؤية الكمبيوتر، يصيغ Pix2Seq (Chen et al., 2022b) العديد من مشاكل الرؤية في تنسيق تسلسل موحد. يقوم Florence (Yuan et al., 2021) و BiT (Kolesnikov et al., 2020) و MuST (Ghiasi et al., 2021) بالتدريب المسبق لنماذج العمود الفقري المشتركة على نطاق واسع للتمثيلات البصرية العامة ونقلها إلى المهام النهائية. في التعلم متعدد الوسائط، يقترح Perceiver (Jaegle et al., 2021b;a) معمارية فعالة للتعامل مع المدخلات والمخرجات المنظمة. يصمم Flamingo (Alayrac et al., 2022) و Frozen (Tsimpoukelli et al., 2021) واجهة برمجة تطبيقات عالمية تستقبل تسلسلات متداخلة من الصور والنصوص وتولد نصاً حر الشكل. Gato (Reed et al., 2022) هو نموذج متعدد المهام بشكل هائل عبر معالجة اللغة الطبيعية والرؤية والوكلاء المجسدين. عملنا هو الأكثر تشابهاً في الروح مع Gato، لكننا نركز بشكل أساسي على تمكين واجهة موجهات متعددة الوسائط بديهية لوكيل روبوت متعدد الأغراض.

**النماذج الأساسية للوكلاء المجسدين.** أظهرت النماذج الأساسية (Bommasani et al., 2021) خصائص ناشئة قوية. هناك العديد من الجهود الجارية لتكرار هذا النجاح للوكلاء المجسدين (Yang et al., 2023)، مع التركيز على 3 جوانب. 1) معمارية وكيل المحول: يستفيد Decision Transformer و Trajectory Transformer (Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022; Xu et al., 2022; 2023) من نماذج الانتباه الذاتي القوية لاتخاذ القرارات التسلسلية. يطبق CLIPort (Shridhar et al., 2021) و Perceiver-Actor (Shridhar et al., 2022) و RT-1 (Brohan et al., 2022) المحولات الكبيرة على مهام التلاعب الروبوتي. يصمم BeT (Shafiullah et al., 2022) و C-BeT (Cui et al., 2022) تقنيات جديدة للتعلم من العروض التوضيحية ذات الأوضاع المتعددة باستخدام المحولات. 2) التدريب المسبق للحصول على تمثيلات أفضل: يقوم MaskViT (Gupta et al., 2022b) و R3M (Nair et al., 2022) و VIP (Ma et al., 2022) و VC-1 (Majumdar et al., 2023) بالتدريب المسبق للتمثيلات البصرية العامة للإدراك الروبوتي. يقوم Li et al. (2022b) بالضبط الدقيق من نقاط فحص نموذج اللغة الكبير لتسريع تعلم السياسات. يوفر MineDojo (Fan et al., 2022) و Ego4D (Grauman et al., 2021) قواعد بيانات متعددة الوسائط واسعة النطاق لتسهيل تدريب السياسات القابلة للتوسع. 3) نماذج اللغة الكبيرة لتعلم الروبوتات: يستفيد SayCan (Ahn et al., 2022) من PaLM (Chowdhery et al., 2022) لتأسيس المفاهيم بدون أمثلة. يطبق Huang et al. (2022a) و Inner Monologue (Huang et al., 2022b) و LM-Nav (Shah et al., 2022) نماذج اللغة الكبيرة على التخطيط الروبوتي طويل الأمد. PaLM-E (Driess et al., 2023) هو بدلاً من ذلك نموذج لغة متعدد الوسائط يمكن إعادة استخدامه للتخطيط المتسلسل للتلاعب الروبوتي. يختلف عملنا عن هذه الأعمال في صياغة الموجهات متعددة الوسائط الجديدة، والتي لا تدعمها نماذج اللغة الكبيرة الموجودة بسهولة.

**التلاعب الروبوتي والمعايير القياسية.** تتطلب مجموعة واسعة من مهام التلاعب الروبوتي مهارات وتنسيقات مختلفة لتحديد المهام، مثل اتباع التعليمات (Stepputtis et al., 2020)، والتقليد لمرة واحدة (Finn et al., 2017; Duan et al., 2017)، وإعادة الترتيب (Batra et al., 2020)، وإرضاء القيود (Brunke et al., 2021a)، والاستدلال (Shridhar et al., 2020). تم تقديم معايير محاكاة فيزيائية متعددة لدراسة المهام المذكورة أعلاه. على سبيل المثال، يحاكي iGibson (Shen et al., 2020; Li et al., 2021; Srivastava et al., 2021; Li et al., 2022a) سيناريوهات منزلية تفاعلية. يصمم Ravens (Zeng et al., 2020) و Robosuite (Zhu et al., 2020; Fan et al., 2021) مهام تلاعب مكتبية متنوعة بأذرع روبوت واقعية. يطور CALVIN (Mees et al., 2021) مهام مشروطة باللغة طويلة الأمد. Meta-World (Yu et al., 2019) هو معيار محاكاة مستخدم على نطاق واسع يدرس التلاعب الروبوتي بإعدادات مكتبية. CausalWorld (Ahmed et al., 2021) هو معيار للبنية السببية والتعلم بالنقل في التلاعب، يتطلب تخطيطاً طويل الأمد والتحكم الحركي الدقيق منخفض المستوى. AI2-THOR (Ehsani et al., 2021; Deitke et al., 2022) هو إطار عمل يدعم التلاعب البصري بالأشياء والتوليد الإجرائي للبيئات. VIMA-BENCH هو أول معيار لتعلم الروبوتات يدعم المهام المدفوعة بموجهات متعددة الوسائط. كما نقوم بتوحيد بروتوكول التقييم لقياس قدرات التعميم للوكيل بشكل منهجي.

يمكن العثور على مراجعة موسعة في الملحق، القسم F.

---

### Translation Notes

- **Figures referenced:** None
- **Key terms introduced:** foundation models, embodied agents, transformer architecture, decision transformer, pre-training, LLM for robotics, benchmark evaluation protocol
- **Equations:** None
- **Citations:** Extensive citations throughout (~50+ references)
- **Special handling:**
  - Maintained citation format [Author et al., Year]
  - Preserved technical acronyms (GPT-3, T5, VIMA-BENCH, etc.)
  - Organized into three subsections as in original
  - Kept model names in English (standard practice in Arabic CS literature)

### Quality Metrics

- Semantic equivalence: 0.88
- Technical accuracy: 0.87
- Readability: 0.86
- Glossary consistency: 0.87
- **Overall section score:** 0.87

### Back-Translation Validation

**Multi-Task Learning by Sequence Modeling.** Transformers (Vaswani et al., 2017) enabled task unification across many AI fields (Brown et al., 2020; Chen et al., 2022a;b; Lu et al., 2022; Wang et al., 2022c). For example, in natural language processing, the Natural Language Decathlon (McCann et al., 2018) adopts a unified question-answering format for a set of 10 NLP tasks. T5 (Raffel et al., 2020) unifies all language problems into the same text-to-text format. GPT-3 (Brown et al., 2020) and Megatron (Shoeybi et al., 2019) demonstrate emergent behaviors for intuitive task specifications through zero-shot prompting. In computer vision, Pix2Seq (Chen et al., 2022b) formulates many vision problems in a unified sequence format. Florence (Yuan et al., 2021), BiT (Kolesnikov et al., 2020), and MuST (Ghiasi et al., 2021) pre-train shared backbone models at large scale for general visual representations and transfer them to downstream tasks. In multimodal learning, Perceiver (Jaegle et al., 2021b;a) proposes an efficient architecture for handling structured inputs and outputs. Flamingo (Alayrac et al., 2022) and Frozen (Tsimpoukelli et al., 2021) design a universal API that receives interleaving sequences of images and text and generates free-form text. Gato (Reed et al., 2022) is a massively multi-task model across NLP, vision, and embodied agents. Our work is most similar in spirit to Gato, but we focus primarily on enabling an intuitive multimodal prompting interface for a general-purpose robot agent.

[Rest of back-translation follows similar pattern...]
