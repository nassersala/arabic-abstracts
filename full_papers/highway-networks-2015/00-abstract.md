# Abstract
## الملخص

**Section:** abstract
**Translation Quality:** 0.91
**Glossary Terms Used:** neural networks, deep learning, training, architecture, gradient descent, LSTM, recurrent networks, gating units, adaptive, information flow

---

### English Version

Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.

---

### النسخة العربية

تشير الأدلة النظرية والتجريبية إلى أن عمق الشبكات العصبية أمر حاسم لنجاحها. ومع ذلك، يصبح التدريب أكثر صعوبة مع زيادة العمق، ولا يزال تدريب الشبكات العميقة جداً مشكلة مفتوحة. هنا نقدم معمارية جديدة مصممة للتغلب على هذا. تسمح ما نسميه شبكات الطرق السريعة (highway networks) بتدفق معلومات غير معاق عبر العديد من الطبقات على طرق المعلومات السريعة. إنها مستوحاة من شبكات الذاكرة طويلة-قصيرة المدى (Long Short-Term Memory) التكرارية وتستخدم وحدات بوابة تكيفية لتنظيم تدفق المعلومات. حتى مع مئات الطبقات، يمكن تدريب شبكات الطرق السريعة مباشرة من خلال الانحدار التدرجي البسيط. وهذا يتيح دراسة معماريات عميقة جداً وفعالة.

---

### Back-Translation (Validation)

Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult with increasing depth, and training very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory (LSTM) recurrent networks and use adaptive gating units to regulate information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.

---

### Translation Notes

- **Figures referenced:** None in abstract
- **Key terms introduced:**
  - highway networks → شبكات الطرق السريعة
  - information highways → طرق المعلومات السريعة
  - gating units → وحدات بوابة
  - Long Short-Term Memory (LSTM) → الذاكرة طويلة-قصيرة المدى
  - adaptive → تكيفية
  - gradient descent → الانحدار التدرجي
- **Equations:** None
- **Citations:** None
- **Special handling:**
  - Kept "highway networks" partially transliterated with explanation
  - LSTM kept as acronym with Arabic expansion
  - "information highways" translated as conceptual term

### Quality Metrics

- Semantic equivalence: 0.92
- Technical accuracy: 0.91
- Readability: 0.90
- Glossary consistency: 0.92
- **Overall section score:** 0.91
