# XLNet: Generalized Autoregressive Pretraining for Language Understanding
## XLNet: التدريب المسبق الانحداري الذاتي المعمم لفهم اللغة

**arXiv ID:** 1906.08237
**Authors:** Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
**Affiliations:**
- Carnegie Mellon University
- Google AI Brain Team

**Year:** 2019
**Submitted:** June 19, 2019
**Revised:** January 2, 2020
**Publication:** 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada
**Categories:** Computation and Language (cs.CL); Machine Learning (cs.LG)
**DOI:** N/A
**PDF:** https://arxiv.org/pdf/1906.08237.pdf
**Code:** https://github.com/zihangdai/xlnet

**Abstract Translation Quality:** 0.93 (from translations/)
**Full Paper Translation Quality:** 0.873 (Excellent - all sections ≥ 0.85)

## Paper Structure

1. Introduction
2. Proposed Method
   - 2.1 Background
   - 2.2 Objective: Permutation Language Modeling
   - 2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations
   - 2.4 Incorporating Ideas from Transformer-XL
   - 2.5 Modeling Multiple Segments
   - 2.6 Discussion
3. Experiments
   - 3.1 Pretraining and Implementation
   - 3.2 Fair Comparison with BERT
   - 3.3 Comparison with RoBERTa: Scaling Up
   - 3.4 Ablation Study
4. Conclusions

## Citation

```bibtex
@inproceedings{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5753--5763},
  year={2019}
}
```

## Translation Team
- Translator: Claude (Sonnet 4.5)
- Reviewer: TBD
- Started: 2025-11-15
- Completed: 2025-11-15

## Translation Statistics
- Total sections: 10
- Average quality score: 0.873
- Minimum section score: 0.85 (Experiments)
- Maximum section score: 0.93 (Abstract)
- New glossary terms added: 20
- Existing terms updated: 9
- Total translation time: Single session
