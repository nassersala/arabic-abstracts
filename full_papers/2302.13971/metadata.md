# LLaMA: Open and Efficient Foundation Language Models
## LLaMA: نماذج اللغة الأساسية المفتوحة والفعّالة

**arXiv ID:** 2302.13971

**Authors:** Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample

**Year:** 2023

**Publication:** arXiv preprint (Meta AI)

**Categories:** cs.CL (Computation and Language)

**DOI:** N/A

**PDF:** https://arxiv.org/pdf/2302.13971.pdf

**Abstract Translation Quality:** 0.96 (from translations/)

**Full Paper Translation Quality:** TBD

## Citation

```bibtex
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
```

## Translation Team
- Translator: Claude (Sonnet 4.5)
- Reviewer: TBD
- Started: 2025-11-17
- Completed: TBD

## Paper Significance

This is a **HIGHLY CITED foundational paper** in Large Language Models (LLMs):
- Introduces LLaMA family (7B to 65B parameters)
- Demonstrates that state-of-the-art models can be trained on **publicly available datasets only**
- LLaMA-13B outperforms GPT-3 (175B) on most benchmarks
- LLaMA-65B competitive with Chinchilla-70B and PaLM-540B
- Released all models to research community
- Major impact on open-source LLM development
- Foundation for many subsequent models (Alpaca, Vicuna, etc.)
