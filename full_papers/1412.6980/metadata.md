# Adam: A Method for Stochastic Optimization
## آدم: طريقة للتحسين العشوائي

**arXiv ID:** 1412.6980
**Authors:** Diederik P. Kingma, Jimmy Ba
**Year:** 2014 (Published at ICLR 2015)
**Publication:** 3rd International Conference for Learning Representations, San Diego, 2015
**Categories:** cs.LG (Machine Learning)
**DOI:** N/A
**PDF:** https://arxiv.org/pdf/1412.6980.pdf

**Abstract Translation Quality:** 0.92 (from translations/)
**Full Paper Translation Quality:** 0.88

## Citation

```bibtex
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
```

## Translation Team
- Translator: Claude (Sonnet 4.5) - Session 2025-11-15
- Reviewer: TBD
- Started: 2025-11-15
- Completed: 2025-11-15

## Paper Overview

This foundational paper introduces Adam (Adaptive Moment Estimation), which has become one of the most widely-used optimization algorithms in deep learning. The paper presents:

- An adaptive learning rate optimization algorithm combining advantages of AdaGrad and RMSProp
- Theoretical convergence analysis with regret bounds
- Comprehensive experimental validation across multiple tasks
- AdaMax variant based on infinity norm

## Translation Notes

This paper contains:
- Mathematical algorithms with pseudocode
- Convergence proofs and theoretical analysis
- Multiple experimental results and figures
- Technical terminology requiring careful translation for Arabic ML community
