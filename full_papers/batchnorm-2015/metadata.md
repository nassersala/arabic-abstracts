# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
## تطبيع الحزمة: تسريع تدريب الشبكات العميقة من خلال تقليل التحول التباين الداخلي

**arXiv ID:** 1502.03167
**Authors:** Sergey Ioffe, Christian Szegedy
**Year:** 2015
**Publication:** Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)
**Pages:** 448-456
**Categories:** Machine Learning (cs.LG)
**DOI:** N/A
**PDF:** https://arxiv.org/pdf/1502.03167.pdf

**Abstract Translation Quality:** 0.90 (from translations/1502.03167.md)
**Full Paper Translation Quality:** 0.876

## Citation

```bibtex
@inproceedings{ioffe2015batch,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning},
  pages={448--456},
  year={2015},
  volume={37},
  series={ICML'15}
}
```

## Paper Significance

This paper introduced Batch Normalization, one of the most influential techniques in deep learning. It addresses the internal covariate shift problem during training and has become a standard component in modern neural network architectures. The paper achieved state-of-the-art results on ImageNet classification and demonstrated 14x faster training convergence.

## Translation Team
- Translator: Claude (Sonnet 4.5)
- Reviewer: TBD
- Started: 2025-11-15
- Completed: 2025-11-15
