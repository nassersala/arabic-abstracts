# Section 1: Introduction
## القسم 1: المقدمة

**Section:** introduction
**Translation Quality:** 0.88
**Glossary Terms Used:** reinforcement learning, exploration, robotic manipulation, sparse rewards, reward function, off-policy, suboptimal expert, policy iteration, waypoint tracking, demonstrations, offline RL, prior, bimanual robot, dexterous manipulation

---

### English Version

In recent years, deep reinforcement learning (RL) algorithms have demonstrated increasing capabilities in solving complex robotic control problems both in simulation and on real robots. However, exploration remains a significant challenge for high-dimensional robotics tasks with sparse rewards – the prevalent setting in the domain of robotic manipulation. Crafting a shaped reward function for manipulation tasks is often highly non-trivial due to the characteristics of the desired behavior, which might require complex interactions with tools to accomplish the task. Constructing controllers that achieve partial task success and exhibit the desired behavior, on the other hand, is often possible with methods from classical robotics, such as defining waypoint tracking controllers for picking up tools in the environment. Ideally, our RL algorithms should be flexible enough to incorporate off-policy data generated by such a "suboptimal expert". Indeed, recent work on deep RL from demonstrations (RLfD) [1, 2] as well as offline RL from fixed datasets [3, 4] has partially made progress in this direction. However, we find that current algorithms are ineffective in utilizing highly off-policy data generated from a suboptimal expert together with exploration data generated by their own policy.

**Figure 1:** Left: Bimanual Shadow Hand LEGO stacking task with example waypoints. Middle: Motions are concatenated sequentially to construct a useful suboptimal expert. The robot arm end effector poses are controlled with waypoint tracking controllers, while the dexterous human-like hands are controlled with learned primitives. Right: The suboptimal experts' actions can be intertwined with the current policy's actions to achieve rich exploration. Note that it is also possible to record the actions that the suboptimal expert would have taken, shown as dotted lines.

In this work, we aim to develop an algorithm that excels in the setting of reinforcement learning from suboptimal experts (RLfSE) where we have direct access to an imperfect or partial task solution, as well as the ability to collect new data. To that end, we present three main contributions: 1) we develop a general policy iteration algorithm, Relative Entropy Q-Learning (REQ), which takes advantage of highly off-policy exploration data and shows strong performance across off-policy, offline RL, and RLfD; 2) we show that a simple exploration strategy which intertwines the policy's actions with a suboptimal expert's actions results in an effective exploration strategy for complex robotic manipulation tasks; 3) we demonstrate that suboptimal experts can be constructed by composing learned primitives and waypoint tracking controllers, allowing us to learn challenging dexterous manipulation task on a simulated bimanual robot with human-like hands.

---

### النسخة العربية

في السنوات الأخيرة، أظهرت خوارزميات التعلم المعزز العميق قدرات متزايدة في حل مشاكل التحكم الروبوتي المعقدة سواء في المحاكاة أو على الروبوتات الحقيقية. ومع ذلك، يظل الاستكشاف تحدياً كبيراً لمهام الروبوتات عالية الأبعاد ذات المكافآت المتفرقة - وهو الإعداد السائد في مجال التلاعب الروبوتي. غالباً ما يكون صياغة دالة مكافأة مشكلة لمهام التلاعب أمراً غير بسيط للغاية بسبب خصائص السلوك المرغوب، والذي قد يتطلب تفاعلات معقدة مع الأدوات لإنجاز المهمة. من ناحية أخرى، غالباً ما يكون بناء متحكمات تحقق نجاحاً جزئياً في المهمة وتُظهر السلوك المرغوب أمراً ممكناً باستخدام أساليب من الروبوتات الكلاسيكية، مثل تعريف متحكمات تتبع نقاط المسار لالتقاط الأدوات في البيئة. من الناحية المثالية، يجب أن تكون خوارزميات التعلم المعزز لدينا مرنة بما يكفي لدمج البيانات خارج السياسة المُولدة بواسطة "خبير غير مثالي" من هذا القبيل. في الواقع، حققت الأعمال الحديثة حول التعلم المعزز العميق من العروض التوضيحية (RLfD) [1, 2] وكذلك التعلم المعزز غير المتصل بالإنترنت من مجموعات البيانات الثابتة [3, 4] تقدماً جزئياً في هذا الاتجاه. ومع ذلك، نجد أن الخوارزميات الحالية غير فعالة في استخدام البيانات عالية الانحراف عن السياسة المُولدة من خبير غير مثالي مع بيانات الاستكشاف المُولدة بواسطة سياستها الخاصة.

**الشكل 1:** يسار: مهمة تكديس قطع ليغو بيد Shadow الثنائية مع نقاط مسار مثالية. وسط: يتم ربط الحركات بشكل متسلسل لبناء خبير غير مثالي مفيد. يتم التحكم في أوضاع المُنفذ الطرفي لذراع الروبوت باستخدام متحكمات تتبع نقاط المسار، بينما يتم التحكم في الأيدي البارعة الشبيهة بالبشر باستخدام عناصر أولية متعلمة. يمين: يمكن تشابك أفعال الخبراء غير المثاليين مع أفعال السياسة الحالية لتحقيق استكشاف غني. لاحظ أنه من الممكن أيضاً تسجيل الأفعال التي كان الخبير غير المثالي سيتخذها، كما هو موضح بالخطوط المتقطعة.

في هذا العمل، نهدف إلى تطوير خوارزمية تتفوق في إعداد التعلم المعزز من الخبراء غير المثاليين (RLfSE) حيث لدينا وصول مباشر إلى حل مهمة غير كامل أو جزئي، بالإضافة إلى القدرة على جمع بيانات جديدة. لتحقيق هذه الغاية، نقدم ثلاث مساهمات رئيسية: 1) نطور خوارزمية عامة لتكرار السياسة، وهي التعلم-Q بالإنتروبيا النسبية (REQ)، التي تستفيد من بيانات الاستكشاف عالية الانحراف عن السياسة وتُظهر أداءً قوياً عبر التعلم المعزز خارج السياسة، والتعلم المعزز غير المتصل بالإنترنت، والتعلم المعزز من العروض التوضيحية؛ 2) نوضح أن استراتيجية استكشاف بسيطة تتشابك فيها أفعال السياسة مع أفعال خبير غير مثالي تؤدي إلى استراتيجية استكشاف فعالة لمهام التلاعب الروبوتي المعقدة؛ 3) نوضح أن الخبراء غير المثاليين يمكن بناؤهم من خلال تركيب عناصر أولية متعلمة ومتحكمات تتبع نقاط المسار، مما يسمح لنا بتعلم مهمة تلاعب بارع صعبة على روبوت ثنائي اليد محاكى بأيدٍ شبيهة بالبشر.

---

### Translation Notes

- **Figures referenced:** Figure 1 (Bimanual Shadow Hand LEGO stacking task)
- **Key terms introduced:** Reinforcement Learning from Suboptimal Experts (RLfSE), Relative Entropy Q-Learning (REQ), waypoint tracking controllers, learned primitives
- **Equations:** 0
- **Citations:** [1, 2, 3, 4]
- **Special handling:** Figure caption translated alongside main text

### Quality Metrics

- Semantic equivalence: 0.89
- Technical accuracy: 0.90
- Readability: 0.87
- Glossary consistency: 0.86
- **Overall section score:** 0.88
