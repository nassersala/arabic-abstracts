We evaluate our algorithms, REQ and REQfSE, in several different reinforcement learning settings
and demonstrate that the same algorithm performs well in all regimes. First, in section 6.1 we
consider reinforcement learning from demonstrations (RLfD) and reinforcement learning from sub-
optimal experts (RLfSE) for the manipulation tasks shown in Figure 2. Next, in section 6.2, we
evaluate REQ for (off-policy) reinforcement learning (on the DeepMind Control Suite benchmark
[36]) as well as in a pure ofﬂine reinforcement learning setting. Finally, we demonstrate REQfSE’s
potential to solve challenging dexterous manipulation tasks by controlling a simulated bimanual
robot with human-like hands. Here, we construct a suboptimal expert by composing waypoint con-
trollers with learned primitives (trained via REQ) which we then employ in the context of REQfSE.
Hyperparameters for all algorithms can be found in the Appendix A.9.
6.1 Reinforcement Learning from Demonstrations and Suboptimal Experts
REQ bears similarity to both recent ofﬂine RL algorithms as well as to conventional policy iteration
algorithms such as MPO. We thus hypothesize that REQ is well suited to take advantage of highly
off-policy data but may also be able to make effective use of near on-policy data. We compare REQ
to existing algorithms for learning from demonstrations in complex manipulation environments in
both the RLfD and RLfSE settings. Speciﬁcally, we compare REQ to DDPGfD [1, 37] and MPOfD
[20], which both make use of demonstration data that is added to the replay buffer. Additionally, we
perform an ablation on the importance of the REQ policy evaluation by replacing the importance
weighted formulation from Equation 2 with the standard TD(0) operator [5] which effectively re-
covers CRR-bin [3], a strong ofﬂine RL baseline, but with a trust-region in the policy improvement
and without distributional Q-learning.2We refer to this as CRRfSE in our experiments. Figure 3
shows that especially in the bimanual environments, training with REQ provides a signiﬁcant perfor-
mance gain compared to other algorithms. We speculate that as the state-action space grows, using
highly off-policy data for policy evaluation becomes signiﬁcantly more challenging, similar to the
difﬁculties encountered in ofﬂine RL with high state-action spaces. Given that most off-policy RL
algorithms – unlike REQ or CRR – are designed to consider only the actions from their respective
policyfor policy improvement, ignoring potentially good actions in the dataset can make them
signiﬁcantly less effective. This is evident in our results where CRR performs well compared to
DDPG and MPO but signiﬁcantly underperforms REQ. We highlight that CRR’s difference in speed
can be solely attributed to the policy evaluation step: CRR performs policy evaluation with respect
to what we consider to be the prior policy, but REQ can sharpen the prior to obtain better Q-values
(and in turn a better policy), greatly improving learning speed.
RLfSE also signiﬁcantly outperforms RLfD. We speculate that at the beginning of learning, there
may be a large difference in the state distribution between the demonstrations and on-policy ex-
perience. This can make it difﬁcult to exploit the expert actions for policy evaluation and policy
improvement. Executing suboptimal experts from states visited by the policy (as happens during
“intertwining”) makes expert’s action available for states close to the policy’s state distribution, thus
rendering the data more effective. Intertwining also has the effect of broadening the state-visitation
distribution, allowing for a more robust value learning. Furthermore, as expected, REQfSE signif-
2Note also that CRR is equivalent to AWAC [4] when exponential weighting rather than binary weighting
is used, a setting that we found to not perform better in our experiments.
6Figure 3: RLfD and RLfSE results on the tasks shown in Figure 2. The dotted line represents the
performance of the suboptimal expert and the solid line shows the performance of the best RLfD
agent over CRRfD, MPOfD, REQfD and DDPGfD. In both the bimanual insertion and the cleanup
tasks the performance of RLfD agents is zero. For the bimanual Shadow LEGO task, we only
evaluate REQfSE as this task is signiﬁcantly harder than the ﬁrst three. Additional comparisons of
RLfSE, RLfD, BC, DAgger [16] and Residual RL [22] are available in the Appendix A.6.
icantly outperforms the suboptimal experts alone. Additional ablations of the REQ algorithm can
be found in Appendix A.8. Finally, it is also worth observing the resulting behaviors of the policies
optimized with REQfSE (footage available in the supplementary video). For example, the bimanual
cleanup task shown in Figure 2 features two robotic arms in a workspace containing a trashcan,
brush, dustpan and some balls. The goal is to pick up both the brush and the dustpan to sweep up the
balls and place it in the trashcan. We ﬁnd that when using a suboptimal expert with a low success
rate to guide exploration (via intertwining) this difﬁcult task can be learned successfully using a
sparse reward that only indicates task success but provides no additional information, e.g. tool use.
This suggests that REQfSE is well-suited to tasks where humans can provide additional cues for
desired behavior or intermediary solution steps that are difﬁcult to capture with a reward function.
6.2 Off-Policy and Ofﬂine Reinforcement Learning
To better understand the REQ algorithm, we evaluate it in the off-policy and ofﬂine RL settings.
These settings can be interpreted as two ends of a spectrum: one end with access to data generated
solely by the optimized policy, and the other where only ofﬂine data is available, with RLfSE and
RLfD somewhere in the middle. Below we compare REQ to specialized algorithms for each setting.
We hypothesize that in the standard off-policy RL setting REQ can perform signiﬁcantly better than
the closely related ofﬂine RL algorithm CRR. Recall that CRR considers the data itself to be the
prior and hence performs policy evaluation for what we refer to as prior. In contrast REQ uses the
optimal KL-constrained policy in each iteration to estimate the value (Equation 2). As a result, we
would expect CRR to learn slowly when the data is bad or random. In contrast, REQ’s importance
weighted policy can ﬁlter out the good actions for each state (subject to the KL constraint) and should
result in evaluation of a better policy and thus faster learning. To test this hypothesis, we evaluate
REQ and CRR as well as two state of the art off-policy RL algorithms (MPO [7] and SAC [28]) on
the DeepMind Control Suite [36]. We again use the variant of CRR-bin [3] which differs from REQ
only in the policy evaluation step. The results are shown in Figure 4. REQ’s policy evaluation indeed
results in a signiﬁcant improvement upon CRR. REQ’s performance is also comparable to state of
the art off-policy RL algorithms, although it learns more slowly for the humanoid environments.
Figure 4: Online off-policy RL results on the DeepMind Control Suite [36]. MPO and SAC results
matches [38] and [39] respectively. Note that SAC sometimes becomes unstable when run for longer.
7BC D4PG ABM CRR-exp CRR-bin REQ REQ-exp
Cartpole 3866 85513 79830 66422 8607 85510 74129
Finger Turn 26139 76424 56625 71438 75531 72041 70547
Walker Stand 386692946 68913 79730 88113 90127 81017
Walker Walk 41733 93919 84615 90112 9363 9289 91213
Cheetah Run 40756 308121 30432 57779 45320 43821 52148
Fish Swim 4668 28177 52719 51721 58523 59241 58636
Insert Ball 38512 15454 4094 62524 65442 63854 60222
Insert Peg 32431 712 34512 38736 36528 39741 38921
Humanoid Run 3822 11 3026 5866 41210 40824 59615
Table 1: Ofﬂine RL results on the DeepMind Control Suite [36]. The reference numbers for base-
lines are from Novikov et al. [3].
We further hypothesize that REQ would also be competitive when we consider training from a ﬁxed
ofﬂine dataset. To this end, we compare REQ to the results of Novikov et al. [3] for the Control
Suite tasks, using the same network architecture and evaluation criteria. As can be seen in Table 1,
REQ performs comparatively to CRR when the same advantage transformation is used (CRR-bin).
Interestingly, the improved policy evaluation has less impact in this setting. We hypothesize that this
is due to the fact that we need to stay close to the actions contained in the data to avoid problems with
Q-value overestimation [3, 9]. When using another variant, REQ-exp, which uses the same exponen-
tial advantage transformation as CRR-exp, we again observe comparable performance. Overall, the
results in off-policy and ofﬂine RL highlight the generality of the REQ algorithm. Although it does
not completely outperform specialized algorithms for either setting, it simultaneously performs well
in both. It also sheds light on important algorithmic components for learning efﬁciently and learning
from highly off-policy data. The former is achieved by considering a KL-constrained optimal policy
in the algorithm, like many of the algorithms for off-policy RL [7, 8]. The latter is achieved through
considering the actions from the dataset, rather than the actions from the policy being optimized as
proposed in recent ofﬂine RL algorithms [3, 9].
6.3 Reinforcement Learning from Composition of Learned Primitives and Controllers
Finally, to demonstrate REQ’s potential on challenging manipulation tasks, we evaluate REQfSE
on the bimanual Shadow LEGO task. The goal in this task is to stack two LEGO blocks using
a bimanual arm setup with two Shadow Hands. This problem is extremely challenging not only
due to its high-dimensional state-action space (state size of 176 and action size of 52), but also
because simple waypoint tracking controllers are not sufﬁcient to provide reference behavior for the
full task. Therefore, we ﬁrst learn two primitive policies using REQ in an off-policy RL setting.
The ﬁrst primitive is trained to orient the green LEGO to an upwards facing position (while in
the basket) with the left hand. The second primitive is trained to orient the red LEGO block to a
downwards facing position with the right hand. Both primitives are trained “from-scratch” using a
shaped reward function as described in the appendix A.1. We then construct a suboptimal “expert”
for this task by composing the waypoint tracking controllers for reaching the blocks with the learned
primitives to orient the blocks, followed by a third phase in which the LEGO blocks are joined
together (again using simple waypoint tracking controllers). The results can be seen in Figure 3
where REQfSE is able to achieve higher performance than the suboptimal expert (footage available
in the supplementary video).