In recent years, deep reinforcement learning (RL) algorithms have demonstrated increasing capabil-
ities in solving complex robotic control problems both in simulation and on real robots. However,
exploration remains a signiﬁcant challenge for high-dimensional robotics tasks with sparse rewards
– the prevalent setting in the domain of robotic manipulation. Crafting a shaped reward function for
manipulation tasks is often highly non-trivial due to the characteristics of the desired behavior, which
might require complex interactions with tools to accomplish the task. Constructing controllers that
achieve partial task success and exhibit the desired behavior, on the other hand, is often possible with
methods from classical robotics, such as deﬁning waypoint tracking controllers for picking up tools
in the environment. Ideally, our RL algorithms should be ﬂexible enough to incorporate off-policy
data generated by such a “suboptimal expert”. Indeed, recent work on deep RL from demonstrations
(RLfD) [1, 2] as well as ofﬂine RL from ﬁxed datasets [3, 4] has partially made progress in this
direction. However, we ﬁnd that current algorithms are ineffective in utilizing highly off-policy data
generated from a suboptimal expert together with exploration data generated by their own policy.arXiv:2010.08587v2  [cs.RO]  5 Jan 2021Figure 1: Left: Bimanual Shadow Hand LEGO stacking task with example waypoints. Middle:
Motions are concatenated sequentially to construct a useful suboptimal expert. The robot arm end
effector poses are controlled with waypoint tracking controllers, while the dexterous human-like
hands are controlled with learned primitives. Right: The suboptimal experts’ actions can be inter-
twined with the current policy’s actions to achieve rich exploration. Note that it is also possible to
record the actions that the suboptimal expert would have taken, shown as dotted lines.
In this work, we aim to develop an algorithm that excels in the setting of reinforcement learning from
suboptimal experts (RLfSE) where we have direct access to an imperfect or partial task solution, as
well as the ability to collect new data. To that end, we present three main contributions: 1) we
develop a general policy iteration algorithm, Relative Entropy Q-Learning (REQ), which takes ad-
vantage of highly off-policy exploration data and shows strong performance across off-policy, ofﬂine
RL, and RLfD; 2) we show that a simple exploration strategy which intertwines the policy’s actions
with a suboptimal expert’s actions results in an effective exploration strategy for complex robotic
manipulation tasks; 3) we demonstrate that suboptimal experts can be constructed by composing
learned primitives and waypoint tracking controllers, allowing us to learn challenging dexterous
manipulation task on a simulated bimanual robot with human-like hands.