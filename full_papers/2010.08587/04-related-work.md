# Section 5: Related Work
## القسم 5: الأعمال ذات الصلة

**Section:** related work
**Translation Quality:** 0.87
**Glossary Terms Used:** reinforcement learning, demonstrations, imitation learning, off-policy, actor-critic, Q-learning, KL regularization, offline RL, trust region, policy optimization

---

### English Version

**Reinforcement Learning from Demonstrations** Our approach can be seen as a form of imitation learning from imperfect experts. Similar to DDPGfD [1], DQNfD [19] and MPOfD [20] we use off-policy data generated by experts in the replay buffer to augment reinforcement learning. However, our reinforcement learning from suboptimal experts [21, 22] setup allows us to both gather a mixture of data from the current policy and the suboptimal expert, as well as label any states with what the suboptimal expert would have done. Querying expert actions in states visited by the policy bears similarity to DAgger [16].

**Off-Policy Reinforcement Learning** When no experts are used and all data is collected by following the policy $\pi$, our algorithm bears similarity to a number of recent actor-critic and Q-learning algorithms. In the limit of $\epsilon \to \infty$ it can be seen as a form of deep Q-learning [11, 23] where the maximization of the Q-function is performed approximately via an amortized proposal – an idea that has previously been considered in the literature using a mixture of a uniform and a learned proposal fitted to the best current action for each state [24]. Our method can also directly be related to actor-critic algorithms that implement constrained policy updates – which have seen resurgence in recent years [6, 7, 8, 25, 26]. In particular REQ can be compared to MPO [7] which optimizes the objective from Equation 1, but uses an expected constraint and chooses the prior to be the policy $\pi_{i-1}$ from the last iteration – i.e. it performs an EM-style policy optimization with a trust-region constraint. In addition MPO performs a projection step to obtain a parametric representation of $\pi_i$ in each iteration, whereas REQ only learns the prior and represents the policy via importance sampling.

**KL Regularized Reinforcement Learning** Whereas REQ performs constrained updates with respect to the maximum reward objective, many recent algorithms directly optimize a regularized objective that trades off reward with the divergence to a fixed [27, 28, 29, 30] or learned [31, 32, 33] reference distribution. When the reference distribution is learned it plays a similar role to our prior. Learned reference distributions have so far mainly been considered for transfer and multi-task learning [31, 32, 33, 34].

**Offline Reinforcement Learning** Our algorithm can also be seen as a simplification of a recent offline RL algorithm, ABM+MPO [9] – where we only learn the prior and represent the optimal policy implicitly. While this may seem limiting, we find that removing the need for two policies – which also makes our algorithm closely related to [3] – results in better performance in practice. A similar effect was also observed in Novikov et al. [3], Nair et al. [4] where the authors found that CRR/AWAC outperformed ABM in high-dimensions. Note that the main difference between REQ and CRR is that we perform policy evaluation on the constrained optimal policy and treat the learned policy as the prior – CRR instead considers the behavior distribution (the dataset) as the prior. As the experiments will show, this allows us to obtain significantly better learning speed when new data can be collected.

**Figure 2:** Simulated manipulation environments in the MuJoCo simulator [35]. From left to right: single arm stacking, bimanual insertion, bimanual cleanup and bimanual Shadow hand LEGO assembly. The environments are described in greater detail in Appendix A.1.

---

### النسخة العربية

**التعلم المعزز من العروض التوضيحية** يمكن اعتبار نهجنا شكلاً من أشكال التعلم بالتقليد من خبراء غير كاملين. على غرار DDPGfD [1] و DQNfD [19] و MPOfD [20] نستخدم البيانات خارج السياسة المُولدة بواسطة الخبراء في المخزن المؤقت لإعادة التشغيل لتعزيز التعلم المعزز. ومع ذلك، فإن إعدادنا للتعلم المعزز من الخبراء غير المثاليين [21، 22] يسمح لنا بجمع مزيج من البيانات من السياسة الحالية والخبير غير المثالي، بالإضافة إلى تسمية أي حالات بما كان الخبير غير المثالي سيفعله. يتشابه الاستعلام عن أفعال الخبراء في الحالات التي تزورها السياسة مع DAgger [16].

**التعلم المعزز خارج السياسة** عندما لا يتم استخدام خبراء ويتم جمع جميع البيانات باتباع السياسة $\pi$، تتشابه خوارزميتنا مع عدد من خوارزميات الفاعل-الناقد والتعلم-Q الحديثة. في حدود $\epsilon \to \infty$ يمكن اعتبارها شكلاً من أشكال التعلم-Q العميق [11، 23] حيث يتم تنفيذ تعظيم دالة Q تقريباً عبر اقتراح مُستهلك – وهي فكرة تم النظر فيها سابقاً في الأدبيات باستخدام مزيج من اقتراح موحد واقتراح متعلم يتم ملاءمته لأفضل فعل حالي لكل حالة [24]. يمكن أيضاً ربط طريقتنا مباشرة بخوارزميات الفاعل-الناقد التي تنفذ تحديثات السياسة المقيدة – والتي شهدت انتعاشاً في السنوات الأخيرة [6، 7، 8، 25، 26]. على وجه الخصوص، يمكن مقارنة REQ بـ MPO [7] التي تحسن الهدف من المعادلة 1، ولكنها تستخدم قيداً متوقعاً وتختار المُسبَق ليكون السياسة $\pi_{i-1}$ من التكرار الأخير – أي أنها تنفذ تحسين سياسة بنمط EM مع قيد منطقة الثقة. بالإضافة إلى ذلك، تنفذ MPO خطوة إسقاط للحصول على تمثيل بارامتري لـ $\pi_i$ في كل تكرار، بينما تتعلم REQ المُسبَق فقط وتمثل السياسة عبر أخذ العينات بالأهمية.

**التعلم المعزز المنظم بـ KL** بينما تنفذ REQ تحديثات مقيدة فيما يتعلق بهدف المكافأة الأقصى، تحسن العديد من الخوارزميات الحديثة مباشرة هدفاً منظماً يوازن بين المكافأة والتباعد إلى توزيع مرجعي ثابت [27، 28، 29، 30] أو متعلم [31، 32، 33]. عندما يتم تعلم التوزيع المرجعي، فإنه يلعب دوراً مشابهاً لمُسبَقنا. تم النظر حتى الآن في التوزيعات المرجعية المتعلمة بشكل رئيسي للتعلم بالنقل والتعلم متعدد المهام [31، 32، 33، 34].

**التعلم المعزز غير المتصل بالإنترنت** يمكن أيضاً اعتبار خوارزميتنا تبسيطاً لخوارزمية تعلم معزز غير متصل بالإنترنت حديثة، ABM+MPO [9] – حيث نتعلم المُسبَق فقط ونمثل السياسة المثلى ضمنياً. بينما قد يبدو هذا محدوداً، نجد أن إزالة الحاجة إلى سياستين – مما يجعل خوارزميتنا أيضاً مرتبطة ارتباطاً وثيقاً بـ [3] – يؤدي إلى أداء أفضل في الممارسة. لوحظ تأثير مشابه أيضاً في Novikov وآخرون [3]، Nair وآخرون [4] حيث وجد المؤلفون أن CRR/AWAC تفوق على ABM في الأبعاد العالية. لاحظ أن الفرق الرئيسي بين REQ و CRR هو أننا نجري تقييم السياسة على السياسة المثلى المقيدة ونعامل السياسة المتعلمة كمُسبَق – بدلاً من ذلك، تعتبر CRR توزيع السلوك (مجموعة البيانات) كمُسبَق. كما ستظهر التجارب، يتيح لنا ذلك الحصول على سرعة تعلم أفضل بكثير عندما يمكن جمع بيانات جديدة.

**الشكل 2:** بيئات التلاعب المحاكاة في محاكي MuJoCo [35]. من اليسار إلى اليمين: تكديس الذراع الواحدة، الإدخال الثنائي، التنظيف الثنائي، وتجميع ليغو بيد Shadow الثنائية. تُوصف البيئات بمزيد من التفصيل في الملحق A.1.

---

### Translation Notes

- **Figures referenced:** Figure 2 (MuJoCo manipulation environments)
- **Key terms introduced:** DDPGfD, DQNfD, MPOfD, DAgger, EM-style optimization, ABM+MPO, CRR/AWAC
- **Equations:** 0
- **Citations:** [1, 3, 4, 6-9, 11, 16, 19-35]
- **Special handling:** Algorithm comparisons and relationships clearly explained

### Quality Metrics

- Semantic equivalence: 0.88
- Technical accuracy: 0.89
- Readability: 0.86
- Glossary consistency: 0.85
- **Overall section score:** 0.87
