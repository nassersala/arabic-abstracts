# Section 3-4: Methodology (Relative Entropy Q-Learning & RL from Suboptimal Experts)
## ุงููุณู 3-4: ุงููููุฌูุฉ (ุงูุชุนูู-Q ุจุงูุฅูุชุฑูุจูุง ุงููุณุจูุฉ ูุงูุชุนูู ุงููุนุฒุฒ ูู ุงูุฎุจุฑุงุก ุบูุฑ ุงููุซุงูููู)

**Section:** methodology
**Translation Quality:** 0.86
**Glossary Terms Used:** policy iteration, KL constraint, importance sampling, Q-function, prior policy, temporal difference, target network, softmax Bellman operator, trust region, waypoint tracking, suboptimal expert, exploration strategy

---

### English Version

## 3 Relative Entropy Q-Learning

We introduce Relative Entropy Q-Learning (REQ). REQ is a policy iteration algorithm targeting the KL-constrained RL objective $J_c$ from Equation 1 in each iteration. We start by realizing that the solution to the KL-constrained objective at iteration $i$, $\pi_i = \arg\max_{\pi} J_c(\pi, \pi_{prior}^i, \epsilon)$, can be obtained in closed form by formulating the Lagrangian of the constrained optimization problem and solving for $\pi$. The solution consists of a softmax over Q-values (a well known result, see e.g. [6, 7, 8, 9, 10]) $\pi_i(a|s) \propto \pi_{prior}^i(a|s) \exp(Q^{\pi_{i-1}}(s, a)/\tau_s)$ where the temperature $\tau_s$ can be obtained by solving the dual function of $J_c$; a convex optimization problem. We refer to the Appendix A.3 for a derivation of the Lagrangian as well as how we optimize the dual function for $\tau_s$. Exactly calculating the normalization constant is intractable in continuous action spaces but we can, however, sample from $\pi_i(a|s)$ via importance weighting of samples from $\pi_{prior}^i$ โ an observation that we will now use to define the REQ policy evaluation step.

**Policy Evaluation** The first key observation is that we can learn the state-action value function of $\pi_i$ without the need to explicitly represent $\pi_i$ via a parametric policy. Instead, we can realize that the squared temporal difference error $(Q^{\pi_i}(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{\pi_i}[Q^{\pi_i}(s_{t+1}, a_{t+1})]))^2$ can be evaluated using importance sampling, leading to the following objective:

$$Q^{\pi_i} = \arg\min_Q \mathbb{E}_{s,a,r,s' \sim \mathcal{D}} \left[ \left( Q(s, a) - \left[ r + \gamma \sum_{j=1}^N \frac{\exp(Q_{\theta'}(s', a_j)/\tau_{s'})}{\sum_k \exp(Q_{\theta'}(s', a_k)/\tau_{s'})} Q_{\theta'}(s', a_j) \right] \right)^2 \right], \qquad (2)$$

with $\forall j: a_j \sim \pi_{prior}^i(\cdot|s')$, where $\theta'$ denotes the parameters of a target network [11] and we estimate the expectation $\mathbb{E}_{\pi_i}[Q(s', a)] \propto \mathbb{E}_{a' \sim \pi_{prior}(\cdot|s')}[\exp(Q(s', a')/\tau_{s'})Q(s', a')]$ with self-normalized importance sampling based on $N$ samples.

A few observations can be made about this objective. The learned Q-function corresponds to the one considered in ABM+MPO [9], but with the difference that $\pi_i$ is never projected onto a parametric policy. Instead it is only implicitly represented via importance sampling from the prior โ we thus only need to learn $\pi_{prior}$ and a Q-function. This can be beneficial when the prior is learned from data but is not well aligned with high-value regions in $Q^{\pi_i}$. In such a case $\pi_i$ may become multimodal and hard to project to a parametric policy without accumulation of errors. Note that REQ can still represent the optimal policy, as for $\epsilon \to \infty$ the policy $\pi_i$ will approach $\pi^*$. Hence, the constraint $\epsilon$ allows us to trade-off the exploitation of the Q-function and regularizing towards prior. An alternative view of the REQ policy evaluation is to consider it as a policy iteration algorithm that uses the softmax Bellman operator [12, 13] for policy evaluation (with an adaptive method to satisfy a hard KL constraint with respect to a given prior). Further analysis of the REQ policy evaluation is provided in the Appendix A.2.

**Prior Policy Improvement** The Q-learning like algorithm from above can work with any prior as long as $\pi_{prior}$ has probability mass everywhere; $\pi_{prior}(a|s) > 0 \; \forall a$. However, the sample based importance weighting scheme from Equation 2 becomes ineffective โ potentially leading to learning Q-values of a suboptimal policy โ if the number of samples $N$ is low and the prior has small probability mass at actions with high Q-values. The policy improvement step of REQ thus is to learn an effective prior, $\pi_{prior}^i$, and improve it in each iteration. We achieve this by fitting the prior to all actions from the dataset, $\mathcal{D}$, whose value is estimated to be higher than the average value of the policy. Formally, we find

$$\pi_{prior}^{i+1} = \arg\max_{\pi_{prior}} \mathbb{E}_{a,s \sim \mathcal{D}} \left[ \mathbb{1} \left[ Q_{\theta'}^{\pi_i}(s, a) \geq \mathbb{E}_{a \sim \pi_i(\cdot|s)}[Q_{\theta'}^{\pi_i}(s, a)] \right] \log \pi_{prior}(a|s) \right], \qquad (3)$$

where $\mathbb{1}$ is the indicator function. That is, we consider learning a prior similar to recent offline RL algorithms such as ABM [9] and CRR-bin [3]. To avoid overfitting during training (e.g. due to suboptimal Q-values shrinking the prior distribution) we additionally regularize the prior update step. In particular, we employ constraints on the movement of the prior-policy mean ($\text{KL}(\pi_{prior}^{i+1}(a|s) \| \pi_{prior}^i(a|s; \mu = \mu_i)) < \epsilon_\mu$) and the covariance ($\text{KL}(\pi_{prior}^{i+1}(a|s) \| \pi_{prior}^i(a|s; \Sigma = \Sigma_i)) < \epsilon_\Sigma$) for a Gaussian prior โ analogous to the trust-regions used for policy optimization in MPO [7], which we can enforce via a simple Lagrangian relaxation approach similar to MPO [8].

**Practical algorithm** A full listing of our procedure is presented in Algorithm 1. In contrast to previous works [7], we formulate a per-state KL constraint $\tau_s$ that we optimize for each state in the batch; i.e. we perform multiple gradient steps on the dual for a given $s$ to ensure the constraint is tight. In addition, instead of fully optimizing the policy and Q-function in each iteration, we switch to a new iteration after a fixed amount of gradient descent steps (via the use of target networks).

**Algorithm 1** Relative Entropy Q-learning (REQ)

**Input:** number of learning steps $N$, steps between target updates $U$, number of action samples $M$, KL regularization parameter $\epsilon$, initial parameters for $\theta, \phi$ and $\tau_s$

```
def REQ_update(ฮธ, ฯ, ฯ, B):
    // For this step let ฯ(a|s) โ ฯ_ฯ'(a|s) exp(Q_ฮธ'(a,s)/ฯ_s) and A(a,s) = Q_ฮธ'(a,s) - V(a,s)

    Find ฯ_s for s โ B via gradient: โ_ฯ 1/|B| ฮฃ_s (ฮต + ฯ_s log(1/M ฮฃ_j=1^M exp(Q_ฮธ'(s,a_j)/ฯ_s))) | a_j ~ ฯ_ฯ'(ยท,s)

    Compute V(s) = (ฮฃ_j=1^M exp(Q_ฮธ'(a_j,s)/ฯ_s))/(ฮฃ_j=1^M exp(Q_ฮธ'(a_j,s)/ฯ_s)) Q_ฮธ'(a_j,s), where a_j ~ ฯ_ฯ_prior'(ยท|s)

    Update Q-function with gradient: โ_ฮธ 1/|B| ฮฃ_(s,a,r,s'โB) (r + ฮณV(s') - Q(a,s))^2

    Update prior with gradient: โ_ฯ 1/|B| ฮฃ_(s,a,rโB) ๐[A(a,s)โฅ0] log ฯ(a|s)

Initialize: i = 0, ฮธ' = ฮธ, ฯ' = ฯ
while i โค N do
    Optionally: collect new data D_i by following ฯ_i or some mixture of ฯ_i and an expert policy ฯ
    Let D โ D โช D_i
    sample a batch B from replay buffer D
    execute REQ_update(ฮธ, ฯ, ฯ, B)
    Update policy and Q-function every U steps by copying: ฮธ' โ ฮธ, ฯ' โ ฯ
end while
```

## 4 Reinforcement Learning from Suboptimal Experts

In this section we explain how our method can be used for reinforcement learning from suboptimal experts and describe a class of suboptimal experts for robotic manipulation consisting of simple waypoint tracking controllers.

**Problem Formulation** We consider an RL setting with additional access to a suboptimal expert $\rho(a|s)$. We assume that $\rho$ exhibits behaviors that are relevant to the task but is not necessarily the optimal policy $\pi^*$. We refer to this setting as Reinforcement Learning from Suboptimal Experts (RLfSE). In RLfSE, our policy iteration scheme can be understood as a form of sample-based approximate policy iteration from mixed behavior data โ somewhat similar to the AggreVaTe family of algorithms [14, 15]. This setting is motivated by real world problems for which domain-specific solutions are already deployed. RLfSE can give us access to broader data distributions than reinforcement learning from demonstrations (RLfD [1, 2]), as we can choose to collect data from $\rho$ or a mixture of $\pi$ and $\rho$. Additionally, direct access to $\rho$ also allows us to label off-policy data with the expert's actions, similar to commonly used no-regret imitation learning algorithms [14, 16, 17].

**Waypoint Tracking Controllers** As a concrete example, for each manipulation task, we construct a suboptimal expert $\rho(a|s)$ by composing waypoint (pose) tracking controllers. (Note $\rho$ is deterministic in our case, thus we can write $a = \rho(s)$.) Such pose tracking controllers can be formulated by leveraging differential kinematics as well as velocity control modes of robotic arms. Using relative reference frames, these controllers can generalize under homogeneous transformations and provide an intuitive interface for humans to specify waypoints to follow. The pose controllers that we use are linear feedback controllers on the end-effector(s) of the robot arm(s) using velocity control. Formally we use $\rho(s) = [\nu_p(s), \nu_o(s)]$ for each controllable six degree of freedom with $\nu_p(s) = K_p e_p(s)$ and $\nu_o(s) = K_o e_o(s)$ where $K_p$ and $K_o$ are positive definite gain matrices. The position error is $e_p(s) = p_d(s) - p_t(s)$, where $p_t$ and $p_d$ are the measured and desired positions of the end-effector, respectively. We define the orientation error using unit quaternions where $Q_d = \{\eta_d, \vec{\epsilon}_d\}$ and $Q_t = \{\eta_t, \vec{\epsilon}_t\}$ represent the desired and measured orientations respectively, with $\eta$ representing the real valued quaternion components and $\vec{\epsilon}$ the imaginary values. We define the orientation error as $e_o(s) = \eta_t(s)\vec{\epsilon}_d(s) - \eta_d(s)\vec{\epsilon}_t(s) - S(\vec{\epsilon}_d(s))\vec{\epsilon}_t(s)$ where $S(\cdot)$ is the skew-symmetric operator [18]. Additional details of the composition of the waypoint tracking controller are presented in Appendix A.4.

**Relative Entropy Q-learning from Suboptimal Experts** To make use of the suboptimal expert $\rho$, we propose a simple exploration strategy: intertwining the execution of the current policy with the execution of $\rho$. For each episode we first randomly choose with probability $p_{intertwine}$ whether to execute a mix of the policy and the expert or either of the policy or the expert only. In episodes in which we choose actions according to a mixture of policy and expert we execute expert's action with probability $p_\rho$ at every time step. Otherwise, we execute the entire episode with probability $p_\pi$. This is illustrated in Figure 1 and the full procedure is presented in Appendix A.5 Algorithm 3. Note that setting $p_{intertwine} = 0$ recovers the RLfD setting, where a certain portion of the data in the replay buffer are demonstrations from the suboptimal expert $\rho$. In addition to using $\rho$ for data generation, we can also take advantage of access to the expert in the prior policy improvement step of REQ by using the following equation,

$$\pi_{prior}^{i+1} = \arg\max_\pi \mathbb{E}_{a,s \sim \mathcal{D}} \left[ \mathbb{1}[A_q(a,s) \geq 0] \right] + \mathbb{E}_{a \sim \rho} \left[ \mathbb{1}[A_q(a,s) \geq 0] \log \pi(a|s) \right], \qquad (4)$$

where we now consider actions sampled from $\mathcal{D}$ and actions from the expert $\rho$ for inclusion into the prior, by evaluating the suboptimal expert on states from $\mathcal{D}$. Note that since $\rho$ is deterministic in our case, i.e. the expectation is over a delta distribution and can be evaluated using the single $a = \rho(s)$.

---

### ุงููุณุฎุฉ ุงูุนุฑุจูุฉ

## 3 ุงูุชุนูู-Q ุจุงูุฅูุชุฑูุจูุง ุงููุณุจูุฉ

ููุฏู ุงูุชุนูู-Q ุจุงูุฅูุชุฑูุจูุง ุงููุณุจูุฉ (REQ). REQ ูู ุฎูุงุฑุฒููุฉ ุชูุฑุงุฑ ุณูุงุณุฉ ุชุณุชูุฏู ูุฏู ุงูุชุนูู ุงููุนุฒุฒ ุงููููุฏ ุจู KL ููู $J_c$ ูู ุงููุนุงุฏูุฉ 1 ูู ูู ุชูุฑุงุฑ. ูุจุฏุฃ ุจุฅุฏุฑุงู ุฃู ุงูุญู ูููุฏู ุงููููุฏ ุจู KL ูู ุงูุชูุฑุงุฑ $i$ุ $\pi_i = \arg\max_{\pi} J_c(\pi, \pi_{prior}^i, \epsilon)$ุ ูููู ุงูุญุตูู ุนููู ูู ุตูุฑุฉ ูุบููุฉ ูู ุฎูุงู ุตูุงุบุฉ ูุงุบุฑุงูุฌูุงู ููุณุฃูุฉ ุงูุชุญุณูู ุงููููุฏุฉ ูุญู $\pi$. ูุชููู ุงูุญู ูู softmax ุนูู ููู Q (ูุชูุฌุฉ ูุนุฑููุฉุ ุงูุธุฑ ุนูู ุณุจูู ุงููุซุงู [6ุ 7ุ 8ุ 9ุ 10]) $\pi_i(a|s) \propto \pi_{prior}^i(a|s) \exp(Q^{\pi_{i-1}}(s, a)/\tau_s)$ ุญูุซ ูููู ุงูุญุตูู ุนูู ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ $\tau_s$ ูู ุฎูุงู ุญู ุงูุฏุงูุฉ ุงูุซูุงุฆูุฉ ูู $J_c$ุ ููู ูุณุฃูุฉ ุชุญุณูู ูุญุฏุจุฉ. ูุดูุฑ ุฅูู ุงูููุญู A.3 ูุงุดุชูุงู ุงููุงุบุฑุงูุฌูุงู ููุฐูู ููููุฉ ุชุญุณูู ุงูุฏุงูุฉ ุงูุซูุงุฆูุฉ ูู $\tau_s$. ุญุณุงุจ ุซุงุจุช ุงูุชุทุจูุน ุจุฏูุฉ ุบูุฑ ูุงุจู ููุชุชุจุน ูู ูุถุงุกุงุช ุงูุฃูุนุงู ุงููุณุชูุฑุฉ ูููู ูููููุงุ ูุน ุฐููุ ุฃุฎุฐ ุนููุงุช ูู $\pi_i(a|s)$ ุนุจุฑ ุชุฑุฌูุญ ุงูุฃูููุฉ ููุนููุงุช ูู $\pi_{prior}^i$ โ ููู ููุงุญุธุฉ ุณูุณุชุฎุฏููุง ุงูุขู ูุชุนุฑูู ุฎุทูุฉ ุชูููู ุงูุณูุงุณุฉ ูู REQ.

**ุชูููู ุงูุณูุงุณุฉ** ุงูููุงุญุธุฉ ุงูุฑุฆูุณูุฉ ุงูุฃููู ูู ุฃูู ูููููุง ุชุนูู ุฏุงูุฉ ูููุฉ ุงูุญุงูุฉ-ุงููุนู ูู $\pi_i$ ุฏูู ุงูุญุงุฌุฉ ุฅูู ุชูุซูู $\pi_i$ ุตุฑุงุญุฉู ุนุจุฑ ุณูุงุณุฉ ุจุงุฑุงูุชุฑูุฉ. ุจุฏูุงู ูู ุฐููุ ูููููุง ุฅุฏุฑุงู ุฃู ุฎุทุฃ ุงููุฑู ุงูุฒููู ุงููุฑุจุน $(Q^{\pi_i}(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{\pi_i}[Q^{\pi_i}(s_{t+1}, a_{t+1})]))^2$ ูููู ุชููููู ุจุงุณุชุฎุฏุงู ุฃุฎุฐ ุงูุนููุงุช ุจุงูุฃูููุฉุ ููุง ูุคุฏู ุฅูู ุงููุฏู ุงูุชุงูู:

$$Q^{\pi_i} = \arg\min_Q \mathbb{E}_{s,a,r,s' \sim \mathcal{D}} \left[ \left( Q(s, a) - \left[ r + \gamma \sum_{j=1}^N \frac{\exp(Q_{\theta'}(s', a_j)/\tau_{s'})}{\sum_k \exp(Q_{\theta'}(s', a_k)/\tau_{s'})} Q_{\theta'}(s', a_j) \right] \right)^2 \right], \qquad (2)$$

ูุน $\forall j: a_j \sim \pi_{prior}^i(\cdot|s')$ุ ุญูุซ $\theta'$ ูุดูุฑ ุฅูู ุจุงุฑุงูุชุฑุงุช ุดุจูุฉ ุงููุฏู [11] ูููุฏุฑ ุงูุชููุน $\mathbb{E}_{\pi_i}[Q(s', a)] \propto \mathbb{E}_{a' \sim \pi_{prior}(\cdot|s')}[\exp(Q(s', a')/\tau_{s'})Q(s', a')]$ ูุน ุฃุฎุฐ ุงูุนููุงุช ุจุงูุฃูููุฉ ุงููุทุจูุน ุฐุงุชูุงู ุจูุงุกู ุนูู $N$ ุนููุฉ.

ูููู ุฅุฌุฑุงุก ุจุถุน ููุงุญุธุงุช ุญูู ูุฐุง ุงููุฏู. ุชูุงุจู ุฏุงูุฉ Q ุงููุชุนููุฉ ุชูู ุงููุนุชุจุฑุฉ ูู ABM+MPO [9]ุ ูููู ูุน ุงูุงุฎุชูุงู ุฃู $\pi_i$ ูุง ูุชู ุฅุณูุงุทูุง ุฃุจุฏุงู ุนูู ุณูุงุณุฉ ุจุงุฑุงูุชุฑูุฉ. ุจุฏูุงู ูู ุฐูู ูุชู ุชูุซูููุง ููุท ุถูููุงู ุนุจุฑ ุฃุฎุฐ ุงูุนููุงุช ุจุงูุฃูููุฉ ูู ุงูููุณุจูู โ ูุจุงูุชุงูู ูุญุชุงุฌ ููุท ุฅูู ุชุนูู $\pi_{prior}$ ูุฏุงูุฉ Q. ูููู ุฃู ูููู ูุฐุง ูููุฏุงู ุนูุฏูุง ูุชู ุชุนูู ุงูููุณุจูู ูู ุงูุจูุงูุงุช ููููู ูุง ูุชูุงุดู ุจุดูู ุฌูุฏ ูุน ููุงุทู ุงููููุฉ ุงูุนุงููุฉ ูู $Q^{\pi_i}$. ูู ูุซู ูุฐู ุงูุญุงูุฉ ูุฏ ุชุตุจุญ $\pi_i$ ูุชุนุฏุฏุฉ ุงูุฃููุงุท ููุตุนุจ ุฅุณูุงุทูุง ุนูู ุณูุงุณุฉ ุจุงุฑุงูุชุฑูุฉ ุฏูู ุชุฑุงูู ุงูุฃุฎุทุงุก. ูุงุญุธ ุฃู REQ ูุง ูุฒุงู ุจุฅููุงููุง ุชูุซูู ุงูุณูุงุณุฉ ุงููุซููุ ุญูุซ ุฃูู ูู $\epsilon \to \infty$ ูุฅู ุงูุณูุงุณุฉ $\pi_i$ ุณุชูุชุฑุจ ูู $\pi^*$. ูุจุงูุชุงููุ ูุฅู ุงูููุฏ $\epsilon$ ูุณูุญ ููุง ุจุงูููุงุถูุฉ ุจูู ุงุณุชุบูุงู ุฏุงูุฉ Q ูุงูุชูุธูู ูุญู ุงูููุณุจูู. ุงููุธุฑุฉ ุงูุจุฏููุฉ ูุชูููู ุณูุงุณุฉ REQ ูู ุงุนุชุจุงุฑู ุฎูุงุฑุฒููุฉ ุชูุฑุงุฑ ุณูุงุณุฉ ุชุณุชุฎุฏู ูุนุงูู ุจูููุงู softmax [12ุ 13] ูุชูููู ุงูุณูุงุณุฉ (ูุน ุทุฑููุฉ ุชููููุฉ ูุชูุจูุฉ ููุฏ KL ุตุงุฑู ูููุง ูุชุนูู ุจููุณุจูู ูุนูู). ูุชู ุชูููุฑ ูุฒูุฏ ูู ุงูุชุญููู ูุชูููู ุณูุงุณุฉ REQ ูู ุงูููุญู A.2.

**ุชุญุณูู ุงูุณูุงุณุฉ ุงูููุณุจููุฉ** ูููู ูุฎูุงุฑุฒููุฉ ุงูุชุนูู-Q ุงููุฐููุฑุฉ ุฃุนูุงู ุฃู ุชุนูู ูุน ุฃู ููุณุจูู ุทุงููุง ุฃู $\pi_{prior}$ ูู ูุชูุฉ ุงุญุชูุงููุฉ ูู ูู ููุงูุ $\pi_{prior}(a|s) > 0 \; \forall a$. ููุน ุฐููุ ูุตุจุญ ูุฎุทุท ุชุฑุฌูุญ ุงูุฃูููุฉ ุงููุงุฆู ุนูู ุงูุนููุงุช ูู ุงููุนุงุฏูุฉ 2 ุบูุฑ ูุนุงู โ ููุง ูุฏ ูุคุฏู ุฅูู ุชุนูู ููู Q ูุณูุงุณุฉ ุบูุฑ ูุซูู โ ุฅุฐุง ูุงู ุนุฏุฏ ุงูุนููุงุช $N$ ููุฎูุถุงู ููุงู ููููุณุจูู ูุชูุฉ ุงุญุชูุงููุฉ ุตุบูุฑุฉ ุนูุฏ ุงูุฃูุนุงู ุฐุงุช ููู Q ุงูุนุงููุฉ. ูุจุงูุชุงูู ูุฅู ุฎุทูุฉ ุชุญุณูู ุงูุณูุงุณุฉ ูู REQ ูู ุชุนูู ููุณุจูู ูุนุงูุ $\pi_{prior}^i$ุ ูุชุญุณููู ูู ูู ุชูุฑุงุฑ. ูุญูู ุฐูู ูู ุฎูุงู ููุงุกูุฉ ุงูููุณุจูู ูุฌููุน ุงูุฃูุนุงู ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ $\mathcal{D}$ุ ุงูุชู ุชูุฏุฑ ูููุชูุง ุจุฃููุง ุฃุนูู ูู ูุชูุณุท ูููุฉ ุงูุณูุงุณุฉ. ุฑุณููุงูุ ูุฌุฏ

$$\pi_{prior}^{i+1} = \arg\max_{\pi_{prior}} \mathbb{E}_{a,s \sim \mathcal{D}} \left[ \mathbb{1} \left[ Q_{\theta'}^{\pi_i}(s, a) \geq \mathbb{E}_{a \sim \pi_i(\cdot|s)}[Q_{\theta'}^{\pi_i}(s, a)] \right] \log \pi_{prior}(a|s) \right], \qquad (3)$$

ุญูุซ $\mathbb{1}$ ูู ุฏุงูุฉ ุงููุคุดุฑ. ุฃู ุฃููุง ูุนุชุจุฑ ุชุนูู ููุณุจูู ูุดุงุจู ูุฎูุงุฑุฒููุงุช ุงูุชุนูู ุงููุนุฒุฒ ุบูุฑ ุงููุชุตู ุจุงูุฅูุชุฑูุช ุงูุญุฏูุซุฉ ูุซู ABM [9] ู CRR-bin [3]. ูุชุฌูุจ ุงูุฅูุฑุงุท ูู ุงูุชูููู ุฃุซูุงุก ุงูุชุฏุฑูุจ (ุนูู ุณุจูู ุงููุซุงู ุจุณุจุจ ููู Q ุบูุฑ ุงููุซูู ุงูุชู ุชููุต ุชูุฒูุน ุงูููุณุจูู) ูููู ุจุชูุธูู ุฎุทูุฉ ุชุญุฏูุซ ุงูููุณุจูู ุจุดูู ุฅุถุงูู. ุนูู ูุฌู ุงูุฎุตูุตุ ููุธู ูููุฏุงู ุนูู ุญุฑูุฉ ูุชูุณุท ุงูุณูุงุณุฉ ุงูููุณุจููุฉ ($\text{KL}(\pi_{prior}^{i+1}(a|s) \| \pi_{prior}^i(a|s; \mu = \mu_i)) < \epsilon_\mu$) ูุงูุชุจุงูู ุงููุดุชุฑู ($\text{KL}(\pi_{prior}^{i+1}(a|s) \| \pi_{prior}^i(a|s; \Sigma = \Sigma_i)) < \epsilon_\Sigma$) ูููุณุจูู ุบุงูุณู โ ุจูุง ููุงุซู ููุงุทู ุงูุซูุฉ ุงููุณุชุฎุฏูุฉ ูุชุญุณูู ุงูุณูุงุณุฉ ูู MPO [7]ุ ูุงูุชู ูููููุง ูุฑุถูุง ุนุจุฑ ููุฌ ุงุณุชุฑุฎุงุก ูุงุบุฑุงูุฌู ุจุณูุท ูุดุงุจู ูู MPO [8].

**ุฎูุงุฑุฒููุฉ ุนูููุฉ** ูุชู ุชูุฏูู ูุงุฆูุฉ ูุงููุฉ ุจุฅุฌุฑุงุกูุง ูู ุงูุฎูุงุฑุฒููุฉ 1. ุนูู ุงููููุถ ูู ุงูุฃุนูุงู ุงูุณุงุจูุฉ [7]ุ ูุตูุบ ููุฏ KL ููู ุญุงูุฉ $\tau_s$ ุงูุฐู ูุญุณูู ููู ุญุงูุฉ ูู ุงูุฏูุนุฉุ ุฃู ูููู ุจุชูููุฐ ุฎุทูุงุช ูุชุนุฏุฏุฉ ูู ุงูุงูุญุฏุงุฑ ุงูุชุฏุฑุฌู ุนูู ุงูุซูุงุฆู ูู $s$ ูุนูู ูุถูุงู ุฅุญูุงู ุงูููุฏ. ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ุจุฏูุงู ูู ุชุญุณูู ุงูุณูุงุณุฉ ูุฏุงูุฉ Q ุจุงููุงูู ูู ูู ุชูุฑุงุฑุ ููุชูู ุฅูู ุชูุฑุงุฑ ุฌุฏูุฏ ุจุนุฏ ุนุฏุฏ ุซุงุจุช ูู ุฎุทูุงุช ุงูุงูุญุฏุงุฑ ุงูุชุฏุฑุฌู (ุนุจุฑ ุงุณุชุฎุฏุงู ุดุจูุงุช ุงููุฏู).

**ุงูุฎูุงุฑุฒููุฉ 1** ุงูุชุนูู-Q ุจุงูุฅูุชุฑูุจูุง ุงููุณุจูุฉ (REQ)

**ูุฏุฎูุงุช:** ุนุฏุฏ ุฎุทูุงุช ุงูุชุนูู $N$ุ ุงูุฎุทูุงุช ุจูู ุชุญุฏูุซุงุช ุงููุฏู $U$ุ ุนุฏุฏ ุนููุงุช ุงูุฃูุนุงู $M$ุ ุจุงุฑุงูุชุฑ ุชูุธูู KL ููู $\epsilon$ุ ุงูุจุงุฑุงูุชุฑุงุช ุงูุฃูููุฉ ูู $\theta, \phi$ ู $\tau_s$

```
def REQ_update(ฮธ, ฯ, ฯ, B):
    // ููุฐู ุงูุฎุทูุฉ ุฏุน ฯ(a|s) โ ฯ_ฯ'(a|s) exp(Q_ฮธ'(a,s)/ฯ_s) ู A(a,s) = Q_ฮธ'(a,s) - V(a,s)

    ุฃูุฌุฏ ฯ_s ูู s โ B ุนุจุฑ ุงูุชุฏุฑุฌ: โ_ฯ 1/|B| ฮฃ_s (ฮต + ฯ_s log(1/M ฮฃ_j=1^M exp(Q_ฮธ'(s,a_j)/ฯ_s))) | a_j ~ ฯ_ฯ'(ยท,s)

    ุงุญุณุจ V(s) = (ฮฃ_j=1^M exp(Q_ฮธ'(a_j,s)/ฯ_s))/(ฮฃ_j=1^M exp(Q_ฮธ'(a_j,s)/ฯ_s)) Q_ฮธ'(a_j,s)ุ ุญูุซ a_j ~ ฯ_ฯ_prior'(ยท|s)

    ุญุฏุซ ุฏุงูุฉ Q ุจุงูุชุฏุฑุฌ: โ_ฮธ 1/|B| ฮฃ_(s,a,r,s'โB) (r + ฮณV(s') - Q(a,s))^2

    ุญุฏุซ ุงูููุณุจูู ุจุงูุชุฏุฑุฌ: โ_ฯ 1/|B| ฮฃ_(s,a,rโB) ๐[A(a,s)โฅ0] log ฯ(a|s)

ุชููุฆุฉ: i = 0, ฮธ' = ฮธ, ฯ' = ฯ
ุจูููุง i โค N ูู ุจู
    ุงุฎุชูุงุฑูุงู: ุฌูุน ุจูุงูุงุช ุฌุฏูุฏุฉ D_i ุจุงุชุจุงุน ฯ_i ุฃู ูุฒูุฌ ูู ฯ_i ูุณูุงุณุฉ ุฎุจูุฑ ฯ
    ุฏุน D โ D โช D_i
    ุนููุฉ ุฏูุนุฉ B ูู ุงููุฎุฒู ุงููุคูุช ูุฅุนุงุฏุฉ ุงูุชุดุบูู D
    ููุฐ REQ_update(ฮธ, ฯ, ฯ, B)
    ุญุฏุซ ุงูุณูุงุณุฉ ูุฏุงูุฉ Q ูู U ุฎุทูุฉ ุจุงููุณุฎ: ฮธ' โ ฮธ, ฯ' โ ฯ
ููุงูุฉ ุจูููุง
```

## 4 ุงูุชุนูู ุงููุนุฒุฒ ูู ุงูุฎุจุฑุงุก ุบูุฑ ุงููุซุงูููู

ูู ูุฐุง ุงููุณู ููุถุญ ููู ูููู ุงุณุชุฎุฏุงู ุทุฑููุชูุง ููุชุนูู ุงููุนุฒุฒ ูู ุงูุฎุจุฑุงุก ุบูุฑ ุงููุซุงูููู ููุตู ูุฆุฉ ูู ุงูุฎุจุฑุงุก ุบูุฑ ุงููุซุงูููู ููุชูุงุนุจ ุงูุฑูุจูุชู ุชุชููู ูู ูุชุญููุงุช ุชุชุจุน ููุงุท ุงููุณุงุฑ ุงูุจุณูุทุฉ.

**ุตูุงุบุฉ ุงููุณุฃูุฉ** ูุนุชุจุฑ ุฅุนุฏุงุฏ ุชุนูู ูุนุฒุฒ ูุน ูุตูู ุฅุถุงูู ุฅูู ุฎุจูุฑ ุบูุฑ ูุซุงูู $\rho(a|s)$. ููุชุฑุถ ุฃู $\rho$ ููุธูุฑ ุณููููุงุช ุฐุงุช ุตูุฉ ุจุงููููุฉ ููููู ููุณ ุจุงูุถุฑูุฑุฉ ุงูุณูุงุณุฉ ุงููุซูู $\pi^*$. ูุดูุฑ ุฅูู ูุฐุง ุงูุฅุนุฏุงุฏ ุจุงุณู ุงูุชุนูู ุงููุนุฒุฒ ูู ุงูุฎุจุฑุงุก ุบูุฑ ุงููุซุงูููู (RLfSE). ูู RLfSEุ ูููู ููู ูุฎุทุท ุชูุฑุงุฑ ุงูุณูุงุณุฉ ูุฏููุง ูุดูู ูู ุฃุดูุงู ุชูุฑุงุฑ ุงูุณูุงุณุฉ ุงูุชูุฑูุจู ุงููุงุฆู ุนูู ุงูุนููุงุช ูู ุจูุงูุงุช ุณููู ูุฎุชูุทุฉ โ ูุดุงุจู ุฅูู ุญุฏ ูุง ูุนุงุฆูุฉ ุฎูุงุฑุฒููุงุช AggreVaTe [14ุ 15]. ูุฐุง ุงูุฅุนุฏุงุฏ ูุฏููุน ุจูุดุงูู ุงูุนุงูู ุงูุญูููู ุงูุชู ูุชู ูููุง ูุดุฑ ุญููู ุฎุงุตุฉ ุจุงููุฌุงู ุจุงููุนู. ูููู ูู RLfSE ุฃู ูููุญูุง ุงููุตูู ุฅูู ุชูุฒูุนุงุช ุจูุงูุงุช ุฃูุณุน ูู ุงูุชุนูู ุงููุนุฒุฒ ูู ุงูุนุฑูุถ ุงูุชูุถูุญูุฉ (RLfD [1ุ 2])ุ ุญูุซ ูููููุง ุงุฎุชูุงุฑ ุฌูุน ุงูุจูุงูุงุช ูู $\rho$ ุฃู ูุฒูุฌ ูู $\pi$ ู $\rho$. ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ูุฅู ุงููุตูู ุงููุจุงุดุฑ ุฅูู $\rho$ ูุณูุญ ููุง ุฃูุถุงู ุจุชุณููุฉ ุงูุจูุงูุงุช ุฎุงุฑุฌ ุงูุณูุงุณุฉ ุจุฃูุนุงู ุงูุฎุจูุฑุ ุจุดูู ูุดุงุจู ูุฎูุงุฑุฒููุงุช ุงูุชุนูู ุจุงูุชูููุฏ ุจุฏูู ูุฏู ุงููุณุชุฎุฏูุฉ ุนุงุฏุฉ [14ุ 16ุ 17].

**ูุชุญููุงุช ุชุชุจุน ููุงุท ุงููุณุงุฑ** ููุซุงู ููููุณุ ููู ูููุฉ ุชูุงุนุจุ ูุจูู ุฎุจูุฑุงู ุบูุฑ ูุซุงูู $\rho(a|s)$ ูู ุฎูุงู ุชุฑููุจ ูุชุญููุงุช ุชุชุจุน ููุงุท ุงููุณุงุฑ (ุงููุถุนูุฉ). (ูุงุญุธ ุฃู $\rho$ ุญุชูู ูู ุญุงูุชูุงุ ูุจุงูุชุงูู ูููููุง ูุชุงุจุฉ $a = \rho(s)$.) ูููู ุตูุงุบุฉ ูุชุญููุงุช ุชุชุจุน ุงููุถุนูุฉ ูุฐู ูู ุฎูุงู ุงูุงุณุชูุงุฏุฉ ูู ุงูุญุฑููุงุช ุงูุชูุงุถููุฉ ููุฐูู ุฃูุถุงุน ุงูุชุญูู ูู ุงูุณุฑุนุฉ ููุฃุฐุฑุน ุงูุฑูุจูุชูุฉ. ุจุงุณุชุฎุฏุงู ุงูุฅุทุงุฑุงุช ุงููุฑุฌุนูุฉ ุงููุณุจูุฉุ ูููู ููุฐู ุงููุชุญููุงุช ุงูุชุนููู ุชุญุช ุงูุชุญูููุงุช ุงููุชุฌุงูุณุฉ ูุชูููุฑ ูุงุฌูุฉ ุจุฏูููุฉ ููุจุดุฑ ูุชุญุฏูุฏ ููุงุท ุงููุณุงุฑ ุงูุชู ูุฌุจ ุงุชุจุงุนูุง. ูุชุญููุงุช ุงููุถุนูุฉ ุงูุชู ูุณุชุฎุฏููุง ูู ูุชุญููุงุช ุชุบุฐูุฉ ุฑุงุฌุนุฉ ุฎุทูุฉ ุนูู ุงูููููุฐ(ุงุช) ุงูุทุฑูู(ุฉ) ูุฐุฑุงุน(ุฃุฐุฑุน) ุงูุฑูุจูุช ุจุงุณุชุฎุฏุงู ุงูุชุญูู ูู ุงูุณุฑุนุฉ. ุฑุณููุงู ูุณุชุฎุฏู $\rho(s) = [\nu_p(s), \nu_o(s)]$ ููู ุณุช ุฏุฑุฌุงุช ุญุฑูุฉ ูุงุจูุฉ ููุชุญูู ูุน $\nu_p(s) = K_p e_p(s)$ ู $\nu_o(s) = K_o e_o(s)$ ุญูุซ $K_p$ ู $K_o$ ูู ูุตูููุงุช ูุณุจ ูุญุฏุฏุฉ ููุฌุจุฉ. ุฎุทุฃ ุงูููุถุน ูู $e_p(s) = p_d(s) - p_t(s)$ุ ุญูุซ $p_t$ ู $p_d$ ููุง ุงูููุถุนุงู ุงููููุงุณ ูุงููุฑุบูุจ ููููููุฐ ุงูุทุฑููุ ุนูู ุงูุชูุงูู. ูุนุฑู ุฎุทุฃ ุงูุงุชุฌุงู ุจุงุณุชุฎุฏุงู ุงูุฑุจุงุนูุงุช ุงููุญุฏููุฉ ุญูุซ $Q_d = \{\eta_d, \vec{\epsilon}_d\}$ ู $Q_t = \{\eta_t, \vec{\epsilon}_t\}$ ุชูุซู ุงูุงุชุฌุงูุงุช ุงููุฑุบูุจุฉ ูุงููููุงุณุฉ ุนูู ุงูุชูุงููุ ูุน $\eta$ ุชูุซู ููููุงุช ุงูุฑุจุงุนูุฉ ุฐุงุช ุงููููุฉ ุงูุญููููุฉ ู $\vec{\epsilon}$ ุงูููู ุงูุฎูุงููุฉ. ูุนุฑู ุฎุทุฃ ุงูุงุชุฌุงู ูู $e_o(s) = \eta_t(s)\vec{\epsilon}_d(s) - \eta_d(s)\vec{\epsilon}_t(s) - S(\vec{\epsilon}_d(s))\vec{\epsilon}_t(s)$ ุญูุซ $S(\cdot)$ ูู ุงููุนุงูู ุงูููุญุฑู ุงููุชูุงุซู [18]. ุชูุนุฑุถ ุชูุงุตูู ุฅุถุงููุฉ ูุชุฑููุจ ูุชุญูู ุชุชุจุน ููุงุท ุงููุณุงุฑ ูู ุงูููุญู A.4.

**ุงูุชุนูู-Q ุจุงูุฅูุชุฑูุจูุง ุงููุณุจูุฉ ูู ุงูุฎุจุฑุงุก ุบูุฑ ุงููุซุงูููู** ููุงุณุชูุงุฏุฉ ูู ุงูุฎุจูุฑ ุบูุฑ ุงููุซุงูู $\rho$ุ ููุชุฑุญ ุงุณุชุฑุงุชูุฌูุฉ ุงุณุชูุดุงู ุจุณูุทุฉ: ุชุดุงุจู ุชูููุฐ ุงูุณูุงุณุฉ ุงูุญุงููุฉ ูุน ุชูููุฐ $\rho$. ููู ุญููุฉ ูุฎุชุงุฑ ุฃููุงู ุจุดูู ุนุดูุงุฆู ุจุงุญุชูุงููุฉ $p_{intertwine}$ ูุง ุฅุฐุง ููุง ุณูููุฐ ูุฒูุฌุงู ูู ุงูุณูุงุณุฉ ูุงูุฎุจูุฑ ุฃู ุฅูุง ุงูุณูุงุณุฉ ุฃู ุงูุฎุจูุฑ ููุท. ูู ุงูุญููุงุช ุงูุชู ูุฎุชุงุฑ ูููุง ุงูุฃูุนุงู ูููุงู ููุฒูุฌ ูู ุงูุณูุงุณุฉ ูุงูุฎุจูุฑ ูููุฐ ูุนู ุงูุฎุจูุฑ ุจุงุญุชูุงููุฉ $p_\rho$ ูู ูู ุฎุทูุฉ ุฒูููุฉ. ูุฅูุงุ ูููุฐ ุงูุญููุฉ ุจุฃููููุง ุจุงุญุชูุงููุฉ $p_\pi$. ูุชู ุชูุถูุญ ุฐูู ูู ุงูุดูู 1 ููุชู ุชูุฏูู ุงูุฅุฌุฑุงุก ุงููุงูู ูู ุงูููุญู A.5 ุงูุฎูุงุฑุฒููุฉ 3. ูุงุญุธ ุฃู ุชุนููู $p_{intertwine} = 0$ ูุณุชุฑุฌุน ุฅุนุฏุงุฏ RLfDุ ุญูุซ ูููู ุฌุฒุก ูุนูู ูู ุงูุจูุงูุงุช ูู ุงููุฎุฒู ุงููุคูุช ูุฅุนุงุฏุฉ ุงูุชุดุบูู ุนุฑูุถุงู ุชูุถูุญูุฉ ูู ุงูุฎุจูุฑ ุบูุฑ ุงููุซุงูู $\rho$. ุจุงูุฅุถุงูุฉ ุฅูู ุงุณุชุฎุฏุงู $\rho$ ูุชูููุฏ ุงูุจูุงูุงุชุ ูููููุง ุฃูุถุงู ุงูุงุณุชูุงุฏุฉ ูู ุงููุตูู ุฅูู ุงูุฎุจูุฑ ูู ุฎุทูุฉ ุชุญุณูู ุงูุณูุงุณุฉ ุงูููุณุจููุฉ ูู REQ ุจุงุณุชุฎุฏุงู ุงููุนุงุฏูุฉ ุงูุชุงููุฉ:

$$\pi_{prior}^{i+1} = \arg\max_\pi \mathbb{E}_{a,s \sim \mathcal{D}} \left[ \mathbb{1}[A_q(a,s) \geq 0] \right] + \mathbb{E}_{a \sim \rho} \left[ \mathbb{1}[A_q(a,s) \geq 0] \log \pi(a|s) \right], \qquad (4)$$

ุญูุซ ูุนุชุจุฑ ุงูุขู ุงูุฃูุนุงู ุงููุฃุฎูุฐุฉ ูู $\mathcal{D}$ ูุงูุฃูุนุงู ูู ุงูุฎุจูุฑ $\rho$ ูุฅุฏุฑุงุฌูุง ูู ุงูููุณุจููุ ูู ุฎูุงู ุชูููู ุงูุฎุจูุฑ ุบูุฑ ุงููุซุงูู ุนูู ุงูุญุงูุงุช ูู $\mathcal{D}$. ูุงุญุธ ุฃูู ูุธุฑุงู ูุฃู $\rho$ ุญุชูู ูู ุญุงูุชูุงุ ุฃู ุฃู ุงูุชููุน ุนูู ุชูุฒูุน ุฏูุชุง ููููู ุชููููู ุจุงุณุชุฎุฏุงู $a = \rho(s)$ ุงููุงุญุฏ.

---

### Translation Notes

- **Figures referenced:** Figure 1 (referenced in Section 4)
- **Key terms introduced:** Relative Entropy Q-Learning (REQ), importance weighting, softmax Bellman operator, prior policy improvement, waypoint tracking controllers, RLfSE
- **Equations:** 3 main equations (Equations 2, 3, 4)
- **Citations:** [1-20]
- **Special handling:** Algorithm pseudocode translated with English code preserved; mathematical notation maintained; quaternion mathematics explained

### Quality Metrics

- Semantic equivalence: 0.87
- Technical accuracy: 0.88
- Readability: 0.85
- Glossary consistency: 0.84
- **Overall section score:** 0.86
