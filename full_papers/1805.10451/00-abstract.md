# Abstract
## الملخص

**Section:** abstract
**Translation Quality:** 0.90
**Glossary Terms Used:** deep learning, machine learning, neural network, manifold, probability distribution, latent space, optimal mass transportation

---

### English Version

Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning.

In this work, we give a geometric view to understand deep learning: we show that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it.

We further introduce the concepts of rectified linear complexity for deep neural network measuring its learning capability, rectified linear complexity of an embedding manifold describing the difficulty to be learned. Then we show for any deep neural network with fixed architecture, there exists a manifold that cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory to control the probability distribution in the latent space.

---

### النسخة العربية

التعلم العميق هو التقنية السائدة للعديد من مهام تعلم الآلة، بما في ذلك التعرف على الصور، والترجمة الآلية، والتعرف على الكلام، وما إلى ذلك. لقد تفوق على الأساليب التقليدية في مختلف المجالات وحقق نجاحات كبيرة. لسوء الحظ، لا يزال الفهم حول كيفية عمله غير واضح. من الأهمية المركزية وضع الأساس النظري للتعلم العميق.

في هذا العمل، نقدم منظوراً هندسياً لفهم التعلم العميق: نظهر أن المبدأ الأساسي المسؤول عن النجاح هو بنية المتعدد في البيانات، أي أن البيانات الطبيعية عالية الأبعاد تتركز بالقرب من متعدد منخفض الأبعاد، التعلم العميق يتعلم المتعدد وتوزيع الاحتمالات عليه.

نقدم أيضاً مفاهيم التعقيد الخطي المصحح للشبكة العصبية العميقة لقياس قدرتها على التعلم، والتعقيد الخطي المصحح لمتعدد التضمين الذي يصف صعوبة تعلمه. ثم نظهر أنه لأي شبكة عصبية عميقة ذات بنية ثابتة، يوجد متعدد لا يمكن للشبكة تعلمه. أخيراً، نقترح تطبيق نظرية النقل الأمثل للكتلة للتحكم في توزيع الاحتمالات في الفضاء الكامن.

---

### Translation Notes

- **Key Terms:**
  - manifold → متعدد (manifold in differential geometry)
  - rectified linear complexity → التعقيد الخطي المصحح
  - embedding manifold → متعدد التضمين
  - optimal mass transportation → النقل الأمثل للكتلة
  - latent space → الفضاء الكامن

- **Technical Accuracy:** All mathematical concepts preserved
- **Citations:** None in abstract
- **Equations:** None in abstract

### Quality Metrics

- Semantic equivalence: 0.92
- Technical accuracy: 0.90
- Readability: 0.89
- Glossary consistency: 0.90
- **Overall section score:** 0.90
