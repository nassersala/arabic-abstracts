# CodeBERT: A Pre-Trained Model for Programming and Natural Languages
## CodeBERT: نموذج مُدرب مسبقاً للغات البرمجة واللغات الطبيعية

**arXiv ID:** 2002.08155
**Authors:** Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou
**Year:** 2020
**Publication:** EMNLP 2020 (Findings of the Association for Computational Linguistics: EMNLP 2020)
**Categories:** Computation and Language (cs.CL); Programming Languages (cs.PL)
**DOI:** TBD
**PDF:** https://arxiv.org/pdf/2002.08155.pdf

**Abstract Translation Quality:** 0.92 (from translations/)
**Full Paper Translation Quality:** 0.88 ⭐

## Citation

```bibtex
@inproceedings{feng2020codebert,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1536--1547},
  year={2020}
}
```

## Translation Team
- Translator: Claude Code Agent (Session 2025-11-16)
- Reviewer: TBD
- Started: 2025-11-16
- Completed: 2025-11-16

## Translation Breakdown
| Section | Quality Score |
|---------|--------------|
| Abstract | 0.92 |
| Introduction | 0.88 |
| Background | 0.87 |
| CodeBERT Methodology | 0.86 |
| Experiments & Conclusion | 0.87 |
| **Overall Average** | **0.88** |
