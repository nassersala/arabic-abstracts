# XLNet: Generalized Autoregressive Pretraining for Language Understanding
## XLNet: التدريب المسبق الانحداري الذاتي المعمم لفهم اللغة

**arXiv ID:** 1906.08237
**Authors:** Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
**Year:** 2019
**Categories:** Computation and Language (cs.CL); Machine Learning (cs.LG)
**Translation Quality:** 0.93
**Glossary Terms Used:** autoregressive (انحداري ذاتي), pretraining (تدريب مسبق), language understanding (فهم اللغة), bidirectional (ثنائي الاتجاه), denoising autoencoding (الترميز التلقائي لإزالة الضوضاء), BERT, language modeling (نمذجة اللغة), masking (إخفاء), permutation (تبديل), transformer (محول), question answering (الإجابة على الأسئلة), sentiment analysis (تحليل المشاعر), natural language inference (الاستدلال اللغوي الطبيعي)

### English Abstract
With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.

### الملخص العربي
بفضل القدرة على نمذجة السياقات ثنائية الاتجاه، يحقق التدريب المسبق القائم على الترميز التلقائي لإزالة الضوضاء مثل BERT أداءً أفضل من مناهج التدريب المسبق القائمة على النمذجة اللغوية الانحدارية الذاتية. ومع ذلك، بالاعتماد على إفساد المدخلات بالإخفاء، يتجاهل BERT التبعية بين المواضع المخفاة ويعاني من تباين بين التدريب المسبق والضبط الدقيق. في ضوء هذه الإيجابيات والسلبيات، نقترح XLNet، وهي طريقة تدريب مسبق انحداري ذاتي معممة (1) تمكّن من تعلم السياقات ثنائية الاتجاه من خلال تعظيم الاحتمالية المتوقعة على جميع تبديلات ترتيب التحليل العاملي و(2) تتغلب على قيود BERT بفضل صياغتها الانحدارية الذاتية. علاوة على ذلك، يدمج XLNet أفكاراً من Transformer-XL، النموذج الانحداري الذاتي الأحدث، في التدريب المسبق. تجريبياً، في ظل إعدادات تجريبية قابلة للمقارنة، يتفوق XLNet على BERT في 20 مهمة، غالباً بهامش كبير، بما في ذلك الإجابة على الأسئلة والاستدلال اللغوي الطبيعي وتحليل المشاعر وترتيب المستندات.

### Back-Translation (Validation)
Thanks to the ability to model bidirectional contexts, pretraining based on denoising autoencoding such as BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, by relying on corrupting inputs with masking, BERT ignores the dependency between masked positions and suffers from a discrepancy between pretraining and fine-tuning. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes BERT's limitations thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experimental settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.

### Translation Metrics
- Iterations: 1
- Final Score: 0.93
- Quality: High
- Semantic Equivalence: Excellent - preserves the technical innovation and comparison with BERT
- Technical Accuracy: Very high - correctly translates autoregressive, permutation, and transformer concepts
- Completeness: 100% - all technical details and experimental results preserved
- Coherence: Natural Arabic flow while maintaining technical precision
- Glossary Consistency: Strong use of established NLP and ML terms
