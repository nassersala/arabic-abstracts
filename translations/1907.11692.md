# RoBERTa: A Robustly Optimized BERT Pretraining Approach
## RoBERTa: نهج محسّن بقوة للتدريب المسبق لـ BERT

**arXiv ID:** 1907.11692
**Authors:** Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov
**Year:** 2019
**Categories:** Computation and Language (cs.CL)
**Translation Quality:** 0.94
**Glossary Terms Used:** language model (نموذج لغوي), pretraining (تدريب مسبق), BERT, hyperparameter (معامل فائق), training data (بيانات التدريب), performance (أداء), replication study (دراسة تكرار), benchmark (معيار), GLUE, RACE, SQuAD, optimization (تحسين)

### English Abstract
Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.

### الملخص العربي
أدى التدريب المسبق لنماذج اللغة إلى مكاسب كبيرة في الأداء، لكن المقارنة الدقيقة بين المناهج المختلفة تمثل تحدياً. التدريب مكلف حسابياً، وغالباً ما يتم على مجموعات بيانات خاصة بأحجام مختلفة، وكما سنُظهر، فإن اختيارات المعاملات الفائقة لها تأثير كبير على النتائج النهائية. نقدم دراسة تكرار للتدريب المسبق لـ BERT (Devlin وآخرون، 2019) تقيس بعناية تأثير العديد من المعاملات الفائقة الرئيسية وحجم بيانات التدريب. نجد أن BERT كان يعاني من نقص كبير في التدريب، ويمكنه مطابقة أو تجاوز أداء كل نموذج نُشر بعده. يحقق نموذجنا الأفضل نتائج متقدمة على GLUE وRACE وSQuAD. تبرز هذه النتائج أهمية خيارات التصميم المتجاهلة سابقاً، وتثير تساؤلات حول مصدر التحسينات المُبلغ عنها مؤخراً. نصدر نماذجنا والكود.

### Back-Translation (Validation)
Language model pretraining has led to significant performance gains, but careful comparison between different approaches is challenging. Training is computationally expensive, and is often done on private datasets of different sizes, and as we will show, hyperparameter choices have a significant impact on final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT suffered from significant undertraining, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE, and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.

### Translation Metrics
- Iterations: 1
- Final Score: 0.94
- Quality: High
- Semantic Equivalence: Excellent - preserves the key finding about BERT undertraining
- Technical Accuracy: Very high - correctly translates hyperparameters and benchmark names
- Completeness: 100% - all technical details and experimental results preserved
- Coherence: Natural Arabic flow with precise technical terminology
- Glossary Consistency: Strong use of established NLP and ML terms
