---
# Geometric Understanding of Deep Learning
## فهم هندسي للتعلم العميق

**arXiv ID:** 1805.10451
**Authors:** Na Lei, Zhongxuan Luo, Shing-Tung Yau, David Xianfeng Gu
**Year:** 2018
**Categories:** cs.LG, stat.ML
**Translation Quality:** 0.90
**Glossary Terms Used:** deep learning, machine learning, neural network, image classification

### English Abstract
Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning.

In this work, we give a geometric view to understand deep learning: we show that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it.

We further introduce the concepts of rectified linear complexity for deep neural network measuring its learning capability, rectified linear complexity of an embedding manifold describing the difficulty to be learned. Then we show for any deep neural network with fixed architecture, there exists a manifold that cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory to control the probability distribution in the latent space.

### الملخص العربي
التعلم العميق هو التقنية السائدة للعديد من مهام تعلم الآلة، بما في ذلك التعرف على الصور، والترجمة الآلية، والتعرف على الكلام، وما إلى ذلك. لقد تفوق على الأساليب التقليدية في مختلف المجالات وحقق نجاحات كبيرة. لسوء الحظ، لا يزال الفهم حول كيفية عمله غير واضح. من الأهمية المركزية وضع الأساس النظري للتعلم العميق. في هذا العمل، نقدم منظوراً هندسياً لفهم التعلم العميق: نظهر أن المبدأ الأساسي المسؤول عن النجاح هو بنية المتعدد في البيانات، أي أن البيانات الطبيعية عالية الأبعاد تتركز بالقرب من متعدد منخفض الأبعاد، التعلم العميق يتعلم المتعدد وتوزيع الاحتمالات عليه. نقدم أيضاً مفاهيم التعقيد الخطي المصحح للشبكة العصبية العميقة لقياس قدرتها على التعلم، والتعقيد الخطي المصحح لمتعدد التضمين الذي يصف صعوبة تعلمه. ثم نظهر أنه لأي شبكة عصبية عميقة ذات بنية ثابتة، يوجد متعدد لا يمكن للشبكة تعلمه. أخيراً، نقترح تطبيق نظرية النقل الأمثل للكتلة للتحكم في توزيع الاحتمالات في الفضاء الكامن.

### Back-Translation (Validation)
Deep learning is the dominant technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has surpassed traditional methods in various fields and achieved great successes. Unfortunately, the understanding of how it works remains unclear. It is of central importance to establish the theoretical foundation for deep learning. In this work, we provide a geometric perspective to understand deep learning: we show that the fundamental principle responsible for success is the manifold structure in data, that is, natural high-dimensional data concentrates near a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it. We also introduce the concepts of rectified linear complexity for deep neural network to measure its learning capability, and the rectified linear complexity of the embedding manifold that describes the difficulty of learning it. Then we show that for any deep neural network with fixed architecture, there exists a manifold that the network cannot learn. Finally, we propose to apply the optimal mass transportation theory to control the probability distribution in the latent space.

### Translation Metrics
- Iterations: 1
- Final Score: 0.90
- Quality: High
---
