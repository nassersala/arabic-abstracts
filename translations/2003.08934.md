# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
## NeRF: تمثيل المشاهد كحقول إشعاع عصبية لتركيب المناظر

**arXiv ID:** 2003.08934
**Authors:** Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng
**Year:** 2020
**Categories:** Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)
**Translation Quality:** 0.93
**Glossary Terms Used:** neural network (شبكة عصبية), view synthesis (تركيب المناظر), scene representation (تمثيل المشهد), rendering (تصيير), volume rendering (تصيير الحجم), radiance (إشعاع), optimization (تحسين), deep network (شبكة عميقة), differentiable (قابل للاشتقاق), camera pose (موضع الكاميرا), photorealistic (واقعي فوتوغرافياً), coordinate (إحداثي)

### English Abstract
We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.

### الملخص العربي
نقدم طريقة تحقق نتائج متقدمة في تركيب مناظر جديدة لمشاهد معقدة من خلال تحسين دالة مشهد حجمية مستمرة أساسية باستخدام مجموعة متفرقة من المناظر المدخلة. تمثل خوارزميتنا المشهد باستخدام شبكة عميقة متصلة بالكامل (غير تلافيفية)، يكون مدخلها إحداثي خماسي الأبعاد مستمر واحد (الموقع المكاني (x,y,z) واتجاه المشاهدة (θ, φ)) ويكون مخرجها كثافة الحجم والإشعاع المنبعث المعتمد على المنظر عند ذلك الموقع المكاني. نركب المناظر من خلال الاستعلام عن إحداثيات خماسية الأبعاد على طول أشعة الكاميرا ونستخدم تقنيات تصيير الحجم الكلاسيكية لإسقاط الألوان والكثافات الناتجة إلى صورة. نظراً لأن تصيير الحجم قابل للاشتقاق بطبيعته، فإن المدخل الوحيد المطلوب لتحسين تمثيلنا هو مجموعة من الصور ذات مواضع كاميرا معروفة. نصف كيفية تحسين حقول الإشعاع العصبية بشكل فعال لتصيير مناظر جديدة واقعية فوتوغرافياً لمشاهد ذات هندسة ومظهر معقدين، ونُظهر نتائج تتفوق على الأعمال السابقة في التصيير العصبي وتركيب المناظر. يُفضَّل مشاهدة نتائج تركيب المناظر كمقاطع فيديو، لذلك نحث القراء على مشاهدة الفيديو التكميلي للحصول على مقارنات مقنعة.

### Back-Translation (Validation)
We present a method that achieves advanced results in synthesizing new views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents the scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous five-dimensional coordinate (spatial location (x,y,z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying five-dimensional coordinates along camera rays and use classical volume rendering techniques to project the resulting colors and densities into an image. Since volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic new views of scenes with complex geometry and appearance, and show results that outperform previous work in neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to watch the supplementary video for convincing comparisons.

### Translation Metrics
- Iterations: 1
- Final Score: 0.93
- Quality: High
- Semantic Equivalence: Excellent - preserves all technical details about the method
- Technical Accuracy: Very high - correctly translates computer graphics and ML concepts
- Completeness: 100% - all information including the video recommendation preserved
- Coherence: Natural Arabic flow with precise technical terminology
- Glossary Consistency: Strong adherence to established graphics and neural network terms
