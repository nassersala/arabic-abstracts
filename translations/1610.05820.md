---
# Membership Inference Attacks against Machine Learning Models
## هجمات استنتاج العضوية ضد نماذج تعلم الآلة

**arXiv ID:** 1610.05820
**Authors:** Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov
**Year:** 2017
**Categories:** cs.CR, cs.LG, stat.ML
**Translation Quality:** 0.90
**Glossary Terms Used:** machine learning (تعلم الآلة), model (نموذج), training (التدريب), inference (استنتاج), membership inference (استنتاج العضوية), attack (هجوم), privacy (خصوصية), security (أمان)

### English Abstract
We quantify the privacy loss of machine learning models by membership inference attacks. Given a machine learning model and a record, we determine whether this record was in the model's training dataset. Our method trains an inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our attacks on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.

### الملخص العربي
نقيس فقدان الخصوصية في نماذج تعلم الآلة من خلال هجمات استنتاج العضوية. بإعطاء نموذج تعلم آلة وسجل، نحدد ما إذا كان هذا السجل موجوداً في مجموعة بيانات تدريب النموذج. تقوم طريقتنا بتدريب نموذج استنتاج للتعرف على الفروقات في تنبؤات النموذج المستهدف على المدخلات التي تدرب عليها مقابل المدخلات التي لم يتدرب عليها. نقيّم هجماتنا تجريبياً على نماذج التصنيف المدربة من قبل مزودي "تعلم الآلة كخدمة" التجاريين مثل جوجل وأمازون. باستخدام مجموعات بيانات ومهام تصنيف واقعية، بما في ذلك مجموعة بيانات خروج المستشفى التي تعتبر عضويتها حساسة، نوضح أن هذه النماذج يمكن أن تكون عرضة لهجمات استنتاج العضوية. ثم نحقق في العوامل التي تؤثر على هذا التسريب ونقيّم استراتيجيات التخفيف.

### Back-Translation (Validation)
We quantify the privacy loss in machine learning models through membership inference attacks. Given a machine learning model and a record, we determine whether this record exists in the model's training dataset. Our method trains an inference model to recognize differences in the target model's predictions on inputs it trained on versus inputs it did not train on. We evaluate our attacks empirically on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is considered sensitive, we demonstrate that these models can be vulnerable to membership inference attacks. Then we investigate the factors that affect this leakage and evaluate mitigation strategies.

### Translation Metrics
- Iterations: 1
- Final Score: 0.90
- Quality: High
---
