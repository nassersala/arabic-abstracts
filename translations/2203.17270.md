---
# BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers
## BEVFormer: تعلم تمثيل منظور عين الطائر من صور الكاميرات المتعددة عبر المحولات الزمكانية

**arXiv ID:** 2203.17270
**Authors:** Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, Jifeng Dai
**Year:** 2022
**Categories:** cs.CV (Computer Vision and Pattern Recognition)
**Translation Quality:** 0.94
**Glossary Terms Used:** القيادة الذاتية (autonomous driving), ثلاثي الأبعاد (3D), إطار عمل (framework), آلية الانتباه (attention mechanism), الانتباه المتقاطع (cross-attention), استعلام (query), الميزات (features), رؤى (views), الانتباه الذاتي (self-attention), تمثيل (representation), معيار (benchmark), أداء (performance), كشف الأجسام (object detection), تقدير (estimation)

### English Abstract
The paper addresses 3D perception for autonomous driving by introducing a unified framework that processes multi-camera imagery. The authors develop spatial cross-attention mechanisms enabling each query to extract features across camera views, and temporal self-attention for fusing historical representations. Their approach achieves 56.9% on the nuScenes benchmark—a 9-point improvement over prior methods—with performance comparable to LiDAR-based systems. The work demonstrates particular gains in velocity estimation accuracy and object detection under low-visibility scenarios.

### الملخص العربي
يتناول البحث الإدراك ثلاثي الأبعاد للقيادة الذاتية من خلال تقديم إطار عمل موحد يعالج صور الكاميرات المتعددة. يطور المؤلفون آليات انتباه متقاطع مكاني تمكن كل استعلام من استخراج الميزات عبر رؤى الكاميرات المختلفة، والانتباه الذاتي الزمني لدمج التمثيلات التاريخية. يحقق نهجهم 56.9٪ على معيار nuScenes - وهو تحسين بـ 9 نقاط على الطرق السابقة - مع أداء مماثل للأنظمة القائمة على الليدار. يُظهر العمل مكاسب خاصة في دقة تقدير السرعة وكشف الأجسام في سيناريوهات الرؤية المنخفضة.

### Back-Translation (Validation)
The research addresses three-dimensional perception for autonomous driving by presenting a unified framework that processes multi-camera images. The authors develop spatial cross-attention mechanisms that enable each query to extract features across different camera views, and temporal self-attention to fuse historical representations. Their approach achieves 56.9% on the nuScenes benchmark - a 9-point improvement over previous methods - with performance comparable to LiDAR-based systems. The work demonstrates particular gains in velocity estimation accuracy and object detection in low-visibility scenarios.

### Translation Metrics
- Iterations: 1
- Final Score: 0.94
- Quality: High
---
