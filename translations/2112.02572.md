# Riemannian conjugate gradient methods: General framework and specific algorithms with convergence analyses
## طرق التدرج المترافق الريماني: إطار عام وخوارزميات محددة مع تحليلات التقارب

**arXiv ID:** 2112.02572
**Authors:** Hiroyuki Sato
**Year:** 2021
**Categories:** math.OC, cs.NA
**Translation Quality:** 0.91
**Glossary Terms Used:** algorithm, optimization, convergence, gradient, manifold, vector, iteration

### English Abstract
This paper proposes a novel general framework of Riemannian conjugate gradient methods, that is, conjugate gradient methods on Riemannian manifolds. The conjugate gradient methods are important first-order optimization algorithms both in Euclidean spaces and on Riemannian manifolds. While various types of conjugate gradient methods are studied in Euclidean spaces, there have been fewer studies on those on Riemannian manifolds. In each iteration of the Riemannian conjugate gradient methods, the previous search direction must be transported to the current tangent space so that it can be added to the negative gradient of the objective function at the current point. There are several approaches to transport a tangent vector to another tangent space. Therefore, there are more variants of the Riemannian conjugate gradient methods than the Euclidean case. In order to investigate them in more detail, the proposed framework unifies the existing Riemannian conjugate gradient methods such as ones utilizing a vector transport or inverse retraction and also develops other methods that have not been covered in previous studies. Furthermore, sufficient conditions for the convergence of a class of algorithms in the proposed framework are clarified. Moreover, the global convergence properties of several specific types of algorithms are extensively analyzed. The analyses provide the theoretical results for some algorithms in a more general setting than the existing studies and completely new developments for the other algorithms. Numerical experiments are performed to confirm the validity of the theoretical results. The results also compare the performances of several specific algorithms in the proposed framework.

### الملخص العربي
تقترح هذه الورقة إطاراً عاماً جديداً لطرق التدرج المترافق الريماني، أي طرق التدرج المترافق على المشعبات الريمانية. تُعد طرق التدرج المترافق خوارزميات تحسين من الدرجة الأولى مهمة في كل من الفضاءات الإقليدية وعلى المشعبات الريمانية. بينما تُدرس أنواع مختلفة من طرق التدرج المترافق في الفضاءات الإقليدية، كانت هناك دراسات أقل عن تلك الموجودة على المشعبات الريمانية. في كل تكرار من طرق التدرج المترافق الريماني، يجب نقل اتجاه البحث السابق إلى فضاء الظل الحالي بحيث يمكن إضافته إلى التدرج السالب للدالة الهدف عند النقطة الحالية. هناك عدة نُهج لنقل متجه ظل إلى فضاء ظل آخر. لذلك، هناك متغيرات أكثر من طرق التدرج المترافق الريماني من الحالة الإقليدية. من أجل التحقيق فيها بمزيد من التفصيل، يوحّد الإطار المقترح طرق التدرج المترافق الريماني الموجودة مثل تلك التي تستخدم نقل المتجه أو الانعكاس العكسي وتطور أيضاً طرقاً أخرى لم تتم تغطيتها في الدراسات السابقة. علاوة على ذلك، تم توضيح الشروط الكافية لتقارب فئة من الخوارزميات في الإطار المقترح. علاوة على ذلك، تم تحليل خصائص التقارب العام لعدة أنواع محددة من الخوارزميات على نطاق واسع. توفر التحليلات النتائج النظرية لبعض الخوارزميات في إعداد أكثر عمومية من الدراسات الموجودة وتطورات جديدة تماماً للخوارزميات الأخرى. تم إجراء تجارب عددية لتأكيد صحة النتائج النظرية. تقارن النتائج أيضاً أداء العديد من الخوارزميات المحددة في الإطار المقترح.

### Back-Translation (Validation)
This paper proposes a new general framework for Riemannian conjugate gradient methods, i.e., conjugate gradient methods on Riemannian manifolds. Conjugate gradient methods are important first-order optimization algorithms in both Euclidean spaces and on Riemannian manifolds. While various types of conjugate gradient methods are studied in Euclidean spaces, there have been fewer studies on those on Riemannian manifolds. In each iteration of Riemannian conjugate gradient methods, the previous search direction must be transported to the current tangent space so it can be added to the negative gradient of the objective function at the current point. There are several approaches for transporting a tangent vector to another tangent space. Therefore, there are more variants of Riemannian conjugate gradient methods than the Euclidean case. In order to investigate them in more detail, the proposed framework unifies existing Riemannian conjugate gradient methods such as those using vector transport or inverse retraction and also develops other methods not covered in previous studies. Furthermore, sufficient conditions for convergence of a class of algorithms in the proposed framework have been clarified. Moreover, global convergence properties of several specific types of algorithms have been extensively analyzed. The analyses provide theoretical results for some algorithms in a more general setting than existing studies and completely new developments for other algorithms. Numerical experiments were conducted to confirm the validity of the theoretical results. The results also compare the performance of several specific algorithms in the proposed framework.

### Translation Metrics
- Iterations: 1
- Final Score: 0.91
- Quality: High
