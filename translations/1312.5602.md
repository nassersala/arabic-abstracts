---
# Playing Atari with Deep Reinforcement Learning
## لعب Atari باستخدام التعلم المعزز العميق

**arXiv ID:** 1312.5602
**Authors:** Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller
**Year:** 2013
**Categories:** cs.LG
**Translation Quality:** 0.92
**Glossary Terms Used:** deep learning, convolutional neural network, algorithm, architecture, high-dimensional

### English Abstract
We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.

### الملخص العربي
نقدم أول نموذج تعلم عميق ينجح في تعلم سياسات التحكم مباشرة من المدخلات الحسية عالية الأبعاد باستخدام التعلم المعزز. النموذج عبارة عن شبكة عصبية التفافية، مدربة باستخدام نسخة من خوارزمية Q-learning، حيث يكون المدخل عبارة عن بكسلات خام والمخرج عبارة عن دالة قيمة تقدر المكافآت المستقبلية. نطبق طريقتنا على سبع ألعاب Atari 2600 من بيئة Arcade Learning Environment، دون أي تعديل على المعمارية أو خوارزمية التعلم. نجد أنها تتفوق على جميع الأساليب السابقة في ستة من الألعاب وتتجاوز خبيرًا بشريًا في ثلاثة منها.

### Back-Translation (Validation)
We present the first deep learning model that succeeds in learning control policies directly from high-dimensional sensory inputs using reinforcement learning. The model is a convolutional neural network, trained using a version of the Q-learning algorithm, where the input is raw pixels and the output is a value function that estimates future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, without any modification to the architecture or learning algorithm. We find that it outperforms all previous methods on six of the games and exceeds a human expert on three of them.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
---
