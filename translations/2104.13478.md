# Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
## التعلم العميق الهندسي: الشبكات، والمجموعات، والرسوم البيانية، والجيوديسيات، والمقاييس

**arXiv ID:** 2104.13478
**Authors:** Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veličković
**Year:** 2021
**Categories:** Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
**Translation Quality:** 0.94
**Glossary Terms Used:** deep learning, machine learning, computer vision, protein, gradient descent, backpropagation, neural network, convolutional neural network, recurrent neural network, graph neural network, transformer, feature learning (new), representation learning (new), hierarchical, regularities (new), geometric (new), Erlangen Program (new), prior knowledge (new)

### English Abstract
The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.

### الملخص العربي
شهد العقد الماضي ثورة تجريبية في علم البيانات وتعلم الآلة، تجسدت في طرق التعلم العميق. في الواقع، العديد من مهام التعلم عالية الأبعاد التي كان يُعتقد سابقاً أنها بعيدة المنال - مثل رؤية الحاسوب، ولعب الجو، أو طي البروتينات - هي في الواقع قابلة للتحقيق مع المقياس الحسابي المناسب. والأمر اللافت للنظر أن جوهر التعلم العميق مبني من مبدأين خوارزميين بسيطين: أولاً، مفهوم التمثيل أو تعلم الميزات، حيث تلتقط الميزات المُكيّفة، والتي غالباً ما تكون هرمية، المفهوم المناسب للانتظام لكل مهمة، وثانياً، التعلم بواسطة طرق النزول بالتدرج الموضعي، والتي عادة ما يتم تنفيذها كانتشار خلفي. في حين أن تعلم الدوال العامة في الأبعاد العالية هو مسألة تقدير ملعونة، فإن معظم المهام ذات الاهتمام ليست عامة، وتأتي مع انتظامات أساسية محددة مسبقاً تنشأ من انخفاض الأبعاد والبنية الكامنة للعالم الفيزيائي. يهتم هذا النص بكشف هذه الانتظامات من خلال مبادئ هندسية موحدة يمكن تطبيقها عبر طيف واسع من التطبيقات. هذا المسعى للتوحيد الهندسي، بروح برنامج إرلانجن لفيليكس كلاين، يخدم غرضاً مزدوجاً: من ناحية، يوفر إطاراً رياضياً مشتركاً لدراسة البنى المعمارية الأكثر نجاحاً للشبكات العصبية، مثل الشبكات العصبية الالتفافية، والشبكات العصبية المتكررة، وشبكات الرسوم البيانية العصبية، والمحولات. من ناحية أخرى، يعطي إجراءً بنّاءً لدمج المعرفة الفيزيائية المسبقة في البنى العصبية ويوفر طريقة قائمة على مبادئ لبناء بنى مستقبلية لم تُخترع بعد.

### Back-Translation (Validation)
The last decade has witnessed an experimental revolution in data science and machine learning, embodied in deep learning methods. In fact, many high-dimensional learning tasks previously thought to be out of reach - such as computer vision, playing Go, or protein folding - are actually achievable with the appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the concept of representation or feature learning, where adapted, often hierarchical, features capture the appropriate concept of regularity for each task, and second, learning by local gradient descent methods, which are typically implemented as backpropagation. While learning general functions in high dimensions is a cursed estimation problem, most tasks of interest are not general, and come with essential pre-defined regularities arising from the low-dimensionality and underlying structure of the physical world. This text is concerned with revealing these regularities through unified geometric principles that can be applied across a wide spectrum of applications. This geometric unification endeavor, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, graph neural networks, and transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provides a principled way to build future architectures yet to be invented.

### Translation Metrics
- Iterations: 1
- Final Score: 0.94
- Quality: High
