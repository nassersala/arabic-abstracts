---
# Deep Learning and Geometric Deep Learning: an introduction for mathematicians and physicists
## التعلم العميق والتعلم العميق الهندسي: مقدمة لعلماء الرياضيات والفيزياء

**arXiv ID:** 2305.05601
**Authors:** R. Fioresi, F. Zanchetta
**Year:** 2023
**Categories:** cs.LG, math-ph, math.MP
**Translation Quality:** 0.87
**Glossary Terms Used:** deep learning, algorithm, neural network, training, theorem

### English Abstract
In this expository paper we want to give a brief introduction, with few key references for further reading, to the inner functioning of the new and successfull algorithms of Deep Learning and Geometric Deep Learning with a focus on Graph Neural Networks. We go over the key ingredients for these algorithms: the score and loss function and we explain the main steps for the training of a model. We do not aim to give a complete and exhaustive treatment, but we isolate few concepts to give a fast introduction to the subject. We provide some appendices to complement our treatment discussing Kullback-Leibler divergence, regression, Multi-layer Perceptrons and the Universal Approximation Theorem.

### الملخص العربي
في هذه الورقة التوضيحية نريد تقديم مقدمة موجزة، مع بعض المراجع الأساسية للقراءة الإضافية، حول الأداء الداخلي للخوارزميات الجديدة والناجحة للتعلم العميق والتعلم العميق الهندسي مع التركيز على الشبكات العصبية البيانية. نستعرض المكونات الأساسية لهذه الخوارزميات: دالة النقاط ودالة الخسارة ونشرح الخطوات الرئيسية لتدريب النموذج. لا نهدف إلى تقديم معالجة كاملة وشاملة، ولكننا نعزل بعض المفاهيم لتقديم مقدمة سريعة للموضوع. نقدم بعض الملاحق لاستكمال معالجتنا من خلال مناقشة تباعد كولباك-ليبلر، والانحدار، والشبكات الإدراكية متعددة الطبقات، ومبرهنة التقريب الشامل.

### Back-Translation (Validation)
In this explanatory paper we want to provide a brief introduction, with some key references for additional reading, on the internal performance of the new and successful algorithms of deep learning and geometric deep learning with focus on graph neural networks. We review the key components of these algorithms: the score function and loss function and explain the main steps for training the model. We do not aim to provide a complete and comprehensive treatment, but we isolate some concepts to provide a quick introduction to the subject. We provide some appendices to complete our treatment by discussing Kullback-Leibler divergence, regression, multi-layer perceptual networks, and the universal approximation theorem.

### Translation Metrics
- Iterations: 1
- Final Score: 0.87
- Quality: High
---
