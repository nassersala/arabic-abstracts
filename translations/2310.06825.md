---
# Mistral 7B
## ميسترال 7 مليار

**arXiv ID:** 2310.06825
**Authors:** Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed
**Year:** 2023
**Categories:** Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG)
**Translation Quality:** 0.982
**Glossary Terms Used:** language model, parameter, performance, efficiency, benchmark (2x), reasoning, code generation, inference (2x), fine-tuning

### English Abstract
We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.

### الملخص العربي
نقدم Mistral 7B v0.1، وهو نموذج لغة بسبعة مليارات معامل مُصمم لتحقيق أداء وكفاءة فائقين. يتفوق Mistral 7B على Llama 2 13B في جميع المعايير المُقيَّمة، وعلى Llama 1 34B في الاستدلال والرياضيات وتوليد الشفرة. يستفيد نموذجنا من انتباه الاستعلام المُجمّع (GQA) لتحقيق استنتاج أسرع، بالإضافة إلى انتباه النافذة المنزلقة (SWA) للتعامل الفعّال مع تسلسلات ذات أطوال تعسفية مع تكلفة استنتاج مُخفَّضة. نوفر أيضاً نموذجاً مُضبوطاً دقيقاً لاتباع التعليمات، وهو Mistral 7B -- Instruct، الذي يتفوق على نموذج Llama 2 13B -- Chat في المعايير البشرية والآلية على حد سواء. تم إطلاق نماذجنا بموجب ترخيص Apache 2.0.

### Back-Translation (Validation)
We introduce Mistral 7B v0.1, a language model with seven billion parameters designed to achieve superior performance and efficiency. Mistral 7B outperforms Llama 2 13B on all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) to achieve faster inference, in addition to sliding window attention (SWA) for effective handling of sequences with arbitrary lengths with reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, which outperforms the Llama 2 13B -- Chat model on human and automated benchmarks alike. Our models were released under the Apache 2.0 license.

### Translation Metrics
- Iterations: 2
- Final Score: 0.982
- Quality: High

### Quality Breakdown
- Semantic equivalence: 0.98 (Excellent preservation of original meaning)
- Technical accuracy: 0.98 (Precise use of technical terminology)
- Completeness: 1.0 (All information accurately conveyed)
- Coherence: 0.95 (Natural flow in Arabic)
- Consistency with glossary: 1.0 (Perfect adherence to established terms)

### New Terms Added to Glossary
1. **arbitrary length** → طول تعسفي (confidence: 0.75, usage: 1)
2. **grouped-query attention** → انتباه الاستعلام المُجمّع (confidence: 0.75, usage: 1)
3. **instruction-following** → اتباع التعليمات (confidence: 0.8, usage: 1)
4. **sequence** → تسلسل (confidence: 0.9, usage: 1)
5. **sliding window attention** → انتباه النافذة المنزلقة (confidence: 0.75, usage: 1)

---
