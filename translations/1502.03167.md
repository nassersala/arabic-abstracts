---
# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
## تطبيع الحزمة: تسريع تدريب الشبكات العميقة من خلال تقليل التحول التباين الداخلي

**arXiv ID:** 1502.03167
**Authors:** Sergey Ioffe, Christian Szegedy
**Year:** 2015
**Categories:** Machine Learning (cs.LG)
**Translation Quality:** 0.90
**Glossary Terms Used:** deep neural networks, training, layer, parameters, learning rates, initialization, normalization, model architecture, mini-batch, regularizer, dropout, image classification, accuracy, validation

### English Abstract
Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.

### الملخص العربي
يتعقد تدريب الشبكات العصبية العميقة بسبب أن توزيع مدخلات كل طبقة يتغير أثناء التدريب، مع تغير معاملات الطبقات السابقة. يؤدي هذا إلى إبطاء التدريب من خلال تطلب معدلات تعلم أقل وتهيئة دقيقة للمعاملات، ويجعل تدريب النماذج ذات اللاخطيات المشبعة صعباً بشكل معروف. نشير إلى هذه الظاهرة باسم التحول التباين الداخلي، ونعالج المشكلة من خلال تطبيع مدخلات الطبقة. تستمد طريقتنا قوتها من جعل التطبيع جزءاً من معمارية النموذج وتنفيذ التطبيع لكل حزمة تدريب صغيرة. يتيح لنا تطبيع الحزمة استخدام معدلات تعلم أعلى بكثير وأن نكون أقل حرصاً بشأن التهيئة. كما يعمل كمنظم، وفي بعض الحالات يلغي الحاجة إلى Dropout. عند تطبيقه على نموذج متقدم لتصنيف الصور، يحقق تطبيع الحزمة نفس الدقة بعدد خطوات تدريب أقل 14 مرة، ويتفوق على النموذج الأصلي بهامش كبير. باستخدام مجموعة من الشبكات المطبعة بالحزمة، نحسّن أفضل نتيجة منشورة في تصنيف ImageNet: لنصل إلى خطأ تحقق 4.9% في أفضل 5 (وخطأ اختبار 4.8%)، متجاوزين دقة المقيّمين البشريين.

### Back-Translation (Validation)
Training deep neural networks is complicated because the distribution of each layer's inputs changes during training, as the parameters of previous layers change. This slows down training by requiring lower learning rates and careful parameter initialization, and makes training models with saturating nonlinearities notoriously difficult. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization part of the model architecture and performing normalization for each small training batch. Batch normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, and in some cases eliminates the need for Dropout. When applied to a state-of-the-art image classification model, batch normalization achieves the same accuracy with 14 times fewer training steps, and surpasses the original model by a significant margin. Using a collection of batch-normalized networks, we improve upon the best published result in ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.

### Translation Metrics
- Iterations: 1
- Final Score: 0.90
- Quality: High
---
