---
# DeepSeek-V3 Technical Report
## تقرير تقني عن DeepSeek-V3

**arXiv ID:** 2412.19437
**Authors:** DeepSeek-AI (200+ contributors including Aixin Liu, Bei Feng, Bing Xue, and others)
**Year:** 2024
**Categories:** Computation and Language (cs.CL); Artificial Intelligence (cs.AI)
**Translation Quality:** 0.91
**Glossary Terms Used:** mixture-of-experts, architecture, parameter, token, multi-head attention, load balancing, fine-tuning, reinforcement learning, training, inference, GPU, open-source, closed-source, transformer

### English Abstract
DeepSeek-V3 is introduced as a large language model employing a Mixture-of-Experts (MoE) architecture with 671 billion total parameters, of which 37 billion activate per token. The model leverages Multi-head Latent Attention (MLA) and DeepSeekMoE architectures previously validated in DeepSeek-V2. Key innovations include an auxiliary-loss-free strategy for load balancing and a multi-token prediction training objective for enhanced performance. The model was pre-trained on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages. Despite its strong performance, DeepSeek-V3 required only 2.788 million H800 GPU hours for full training, with remarkably stable training showing no irrecoverable loss spikes or rollbacks. Comprehensive evaluations demonstrate that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.

### الملخص العربي
يُقدم DeepSeek-V3 كنموذج لغوي كبير يستخدم معمارية خليط الخبراء (MoE) بإجمالي 671 مليار معامل، منها 37 مليار تُنشَّط لكل رمز. يستفيد النموذج من معماريات الانتباه الكامن متعدد الرؤوس (MLA) وDeepSeekMoE التي تم التحقق منها مسبقاً في DeepSeek-V2. تشمل الابتكارات الرئيسية استراتيجية موازنة الحمل الخالية من الخسارة المساعدة وهدف تدريب بالتنبؤ متعدد الرموز لتحسين الأداء. تم التدريب المسبق للنموذج على 14.8 تريليون رمز متنوع وعالي الجودة، تلاه مراحل الضبط الدقيق الموجَّه والتعلم المعزز. على الرغم من أدائه القوي، تطلب DeepSeek-V3 فقط 2.788 مليون ساعة من وحدات معالجة الرسومات H800 للتدريب الكامل، مع استقرار تدريبي ملحوظ دون أي قفزات خسارة غير قابلة للاسترداد أو عمليات تراجع. تُظهر التقييمات الشاملة أن DeepSeek-V3 يتفوق على النماذج الأخرى مفتوحة المصدر ويحقق أداءً مماثلاً للنماذج الرائدة مغلقة المصدر.

### Back-Translation (Validation)
DeepSeek-V3 is presented as a large language model using a Mixture-of-Experts (MoE) architecture with a total of 671 billion parameters, of which 37 billion are activated for each token. The model leverages Multi-head Latent Attention (MLA) and DeepSeekMoE architectures previously validated in DeepSeek-V2. Key innovations include an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective for performance improvement. The model was pre-trained on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages. Despite its strong performance, DeepSeek-V3 required only 2.788 million H800 GPU hours for complete training, with remarkable training stability without any irrecoverable loss spikes or rollbacks. Comprehensive evaluations show that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.

### Translation Metrics
- Iterations: 1
- Final Score: 0.91
- Quality: High
---
