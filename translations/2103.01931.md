# Categorical Foundations of Gradient-Based Learning
## الأسس الفئوية للتعلم القائم على التدرجات

**arXiv ID:** 2103.01931
**Authors:** G.S.H. Cruttwell, Bruno Gavranović, Neil Ghani, Paul Wilson, Fabio Zanasi
**Year:** 2021
**Categories:** Machine Learning (cs.LG); Category Theory (math.CT)
**Translation Quality:** 0.93
**Glossary Terms Used:** category, gradient, lenses, parametric, derivative, machine learning, loss function, gradient descent (new), ADAM (new), AdaGrad (new), Nesterov momentum (new), MSE (new), Softmax (new), cross-entropy (new), boolean circuits (new), smooth maps (new)

### English Abstract
We propose a categorical semantics of gradient-based machine learning algorithms in terms of lenses, parametrised maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions such as as MSE and Softmax cross-entropy, shedding new light on their similarities and differences. Our approach to gradient-based learning has examples generalising beyond the familiar continuous domains (modelled in categories of smooth maps) and can be realized in the discrete setting of boolean circuits. Finally, we demonstrate the practical significance of our framework with an implementation in Python.

### الملخص العربي
نقترح دلاليات فئوية لخوارزميات تعلم الآلة القائمة على التدرجات من حيث العدسات، والخرائط البارامترية، وفئات المشتقات العكسية. يوفر هذا الأساس إطار عمل تفسيري وتوحيدي قوي: فهو يشمل مجموعة متنوعة من خوارزميات النزول بالتدرج مثل ADAM وAdaGrad وزخم نيستروف، بالإضافة إلى مجموعة متنوعة من دوال الخسارة مثل MSE والإنتروبيا التبادلية لـ Softmax، مما يلقي ضوءاً جديداً على أوجه التشابه والاختلاف بينها. نهجنا للتعلم القائم على التدرجات له أمثلة تعمم تتجاوز النطاقات المستمرة المألوفة (المنمذجة في فئات الخرائط السلسة) ويمكن تحقيقها في الإعداد المنفصل للدوائر المنطقية. أخيراً، نوضح الأهمية العملية لإطار عملنا من خلال تنفيذ في Python.

### Back-Translation (Validation)
We propose categorical semantics for machine learning algorithms based on gradients in terms of lenses, parametric maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, in addition to a variety of loss functions such as MSE and Softmax cross-entropy, shedding new light on the similarities and differences between them. Our approach to gradient-based learning has examples that generalize beyond the familiar continuous domains (modeled in categories of smooth maps) and can be realized in the discrete setting of boolean circuits. Finally, we demonstrate the practical significance of our framework through an implementation in Python.

### Translation Metrics
- Iterations: 1
- Final Score: 0.93
- Quality: High
