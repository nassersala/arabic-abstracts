---
# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
## التوليد المعزز بالاسترجاع لمهام معالجة اللغة الطبيعية كثيفة المعرفة

**arXiv ID:** 2005.11401
**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela
**Year:** 2020
**Categories:** cs.CL, cs.LG
**Translation Quality:** 0.88
**Glossary Terms Used:** language model, pre-trained, fine-tuning, natural language, architecture, vector, question answering, generation

### English Abstract
Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.

### الملخص العربي
أظهرت نماذج اللغة الكبيرة المدربة مسبقاً أنها تخزن المعرفة الواقعية في معاملاتها، وتحقق نتائج متقدمة عند الضبط الدقيق على مهام معالجة اللغة الطبيعية النهائية. ومع ذلك، لا تزال قدرتها على الوصول إلى المعرفة ومعالجتها بدقة محدودة، وبالتالي في المهام كثيفة المعرفة، يتأخر أداؤها عن المعماريات الخاصة بالمهام. بالإضافة إلى ذلك، يظل توفير المصدر لقراراتها وتحديث معرفتها بالعالم من المشاكل البحثية المفتوحة. يمكن للنماذج المدربة مسبقاً مع آلية وصول قابلة للاشتقاق إلى ذاكرة غير بارامترية صريحة التغلب على هذه المشكلة، لكن تم التحقيق فيها حتى الآن فقط للمهام النهائية الاستخراجية. نستكشف وصفة ضبط دقيق متعددة الأغراض للتوليد المعزز بالاسترجاع (RAG) - نماذج تجمع بين الذاكرة البارامترية وغير البارامترية المدربة مسبقاً لتوليد اللغة. نقدم نماذج RAG حيث الذاكرة البارامترية هي نموذج تسلسل إلى تسلسل مدرب مسبقاً والذاكرة غير البارامترية هي فهرس متجهات كثيف من ويكيبيديا. نقارن صيغتين لـ RAG، واحدة تعتمد على نفس المقاطع المستردة عبر التسلسل المولد بأكمله، والأخرى يمكنها استخدام مقاطع مختلفة لكل رمز. نقوم بالضبط الدقيق وتقييم نماذجنا على مجموعة واسعة من مهام معالجة اللغة الطبيعية كثيفة المعرفة ونحدد المستوى المتقدم في ثلاث مهام للإجابة على الأسئلة مفتوحة المجال، متفوقة على نماذج التسلسل إلى التسلسل البارامترية والمعماريات الخاصة بالمهام التي تسترجع وتستخرج. بالنسبة لمهام توليد اللغة، نجد أن نماذج RAG تولد لغة أكثر تحديداً وتنوعاً وواقعية من خط أساس تسلسل إلى تسلسل بارامتري فقط متقدم.

### Back-Translation (Validation)
Large pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream natural language processing tasks. However, their ability to access and precisely manipulate knowledge remains limited, and thus on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing the source for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this problem, but have so far been investigated only for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for Retrieval-Augmented Generation (RAG) - models that combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq-to-seq model and the non-parametric memory is a dense vector index of Wikipedia. We compare two RAG formulations, one that relies on the same retrieved passages across the entire generated sequence, and the other can use different passages for each token. We fine-tune and evaluate our models on a wide range of knowledge-intensive natural language processing tasks and set the state-of-the-art on three open-domain question answering tasks, outperforming parametric seq-to-seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq-to-seq baseline.

### Translation Metrics
- Iterations: 1
- Final Score: 0.88
- Quality: High
---
