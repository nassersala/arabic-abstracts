---
# Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study
## دراسة أداء نماذج اللغة في إكمال الشفرة بلغات البرمجة الوظيفية: دراسة حالة لـ Haskell

**arXiv ID:** 2403.15185
**Authors:** Tim van Dam, Frank van der Heijden, Philippe de Bekker, Berend Nieuwschepen, Marc Otten, Maliheh Izadi
**Year:** 2024
**Categories:** cs.CL
**Translation Quality:** 0.92
**Glossary Terms Used:** functional programming, programming language, code completion, training, benchmark, dataset, performance, function

### English Abstract
The research evaluates the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. Models were fine-tuned on Haskell functions from a HuggingFace dataset and assessed using a newly created HumanEval-Haskell benchmark. Key findings indicate that knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages, but that code completion on functional languages is feasible. The evaluation revealed that CodeGPT frequently generated empty predictions and extra comments, while UniXcoder produced more incomplete or incorrect outputs. The authors released HumanEval-Haskell, fine-tuned models, and reproducible code on GitHub.

### الملخص العربي
يقيّم البحث أداء نموذجين لغويين للشفرة البرمجية، وهما CodeGPT و UniXcoder، على لغة البرمجة الوظيفية Haskell. تم ضبط النماذج بدقة على دوال Haskell من مجموعة بيانات HuggingFace وتقييمها باستخدام معيار HumanEval-Haskell المُنشأ حديثاً. تشير النتائج الرئيسية إلى أن معرفة لغات البرمجة الأمرية في مرحلة التدريب المسبق لنماذج اللغة الكبيرة قد لا تنتقل بشكل جيد إلى اللغات الوظيفية، ولكن إكمال الشفرة على اللغات الوظيفية ممكن. كشف التقييم أن CodeGPT غالباً ما ينتج تنبؤات فارغة وتعليقات إضافية، بينما أنتج UniXcoder مخرجات أكثر نقصاً أو غير صحيحة. أصدر المؤلفون HumanEval-Haskell، والنماذج المضبوطة بدقة، والشفرة القابلة لإعادة الإنتاج على GitHub.

### Back-Translation (Validation)
The research evaluates the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. The models were fine-tuned on Haskell functions from a HuggingFace dataset and evaluated using the newly created HumanEval-Haskell benchmark. The key findings indicate that knowledge of imperative programming languages in the pre-training phase of large language models may not transfer well to functional languages, but code completion on functional languages is possible. The evaluation revealed that CodeGPT often produces empty predictions and additional comments, while UniXcoder produced more incomplete or incorrect outputs. The authors released HumanEval-Haskell, fine-tuned models, and reproducible code on GitHub.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
---
