---
# Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets
## نموذج انتشار الفيديو المستقر: توسيع نماذج انتشار الفيديو الكامنة إلى مجموعات بيانات كبيرة

**arXiv ID:** 2311.15127
**Authors:** Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, Robin Rombach
**Year:** 2023
**Categories:** cs.CV
**Translation Quality:** 0.91
**Glossary Terms Used:** diffusion models, latent space, training, fine-tuning, pretraining, 3D generation, performance

### English Abstract
We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://stability.ai/stable-video.

### الملخص العربي
يقدم هذا البحث نموذج انتشار فيديو كامن لتوليد فيديو عالي الجودة من النص ومن الصورة. يحدد المؤلفون ثلاث مراحل تدريب: التدريب المسبق من النص إلى الصورة، التدريب المسبق للفيديو، والضبط الدقيق للفيديو عالي الجودة. يؤكدون على ضرورة مجموعة بيانات تدريب مسبق منسقة جيداً ويفصلون عمليات التنسيق المنهجية. يُظهر النموذج أداءً تنافسياً مع الأنظمة المملوكة ويمتد إلى تطبيقات تشمل وحدات خاصة بالحركة وتوليد ثلاثي الأبعاد متعدد الرؤى. أصدر الفريق الكود والأوزان النموذجية بشكل عام.

### Back-Translation (Validation)
This research presents a latent video diffusion model for high-quality video generation from text and from image. The authors identify three training stages: text-to-image pretraining, video pretraining, and high-quality video fine-tuning. They emphasize the necessity of a well-curated pretraining dataset and detail systematic curation processes. The model demonstrates competitive performance with proprietary systems and extends to applications including motion-specific modules and multi-view 3D generation. The team released the code and model weights publicly.

### Translation Metrics
- Iterations: 1
- Final Score: 0.91
- Quality: High
---
