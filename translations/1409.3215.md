# Sequence to Sequence Learning with Neural Networks
## التعلم من تسلسل إلى تسلسل باستخدام الشبكات العصبية

**arXiv ID:** 1409.3215
**Authors:** Ilya Sutskever, Oriol Vinyals, Quoc V. Le
**Year:** 2014
**Categories:** Computation and Language (cs.CL); Machine Learning (cs.LG)
**Translation Quality:** 0.92
**Glossary Terms Used:** deep neural networks (الشبكات العصبية العميقة), long short-term memory (ذاكرة طويلة قصيرة المدى), sequence (تسلسل), vector (متجه), BLEU score (درجة BLEU), translation (ترجمة), optimization (تحسين), phrase-based (قائم على العبارات), representations (تمثيلات), word order (ترتيب الكلمات)

### English Abstract
Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.

### الملخص العربي
الشبكات العصبية العميقة (DNNs) هي نماذج قوية حققت أداءً ممتازاً في مهام التعلم الصعبة. على الرغم من أن الشبكات العصبية العميقة تعمل بشكل جيد عندما تتوفر مجموعات تدريب كبيرة مُعنونة، إلا أنها لا يمكن استخدامها لتخطيط التسلسلات إلى تسلسلات. في هذا البحث، نقدم نهجاً عاماً شاملاً من طرف إلى طرف لتعلم التسلسلات يضع افتراضات قليلة على بنية التسلسل. تستخدم طريقتنا ذاكرة طويلة قصيرة المدى (LSTM) متعددة الطبقات لتخطيط تسلسل الإدخال إلى متجه ذي بُعد ثابت، ثم تستخدم LSTM عميقة أخرى لفك تشفير التسلسل المستهدف من المتجه. نتيجتنا الرئيسية هي أنه في مهمة الترجمة من الإنجليزية إلى الفرنسية من مجموعة بيانات WMT'14، حققت الترجمات التي أنتجتها LSTM درجة BLEU قدرها 34.8 على مجموعة الاختبار بأكملها، حيث تم تخفيض درجة BLEU الخاصة بـ LSTM على الكلمات غير الموجودة في المفردات. بالإضافة إلى ذلك، لم تواجه LSTM صعوبة في الجمل الطويلة. للمقارنة، يحقق نظام الترجمة الآلية الإحصائية القائم على العبارات (SMT) درجة BLEU قدرها 33.3 على نفس مجموعة البيانات. عندما استخدمنا LSTM لإعادة ترتيب الفرضيات الـ 1000 التي أنتجها نظام SMT المذكور، ارتفعت درجة BLEU إلى 36.5، وهي قريبة من أفضل نتيجة سابقة في هذه المهمة. تعلمت LSTM أيضاً تمثيلات معقولة للعبارات والجمل حساسة لترتيب الكلمات ومستقرة نسبياً تجاه الصيغة المبنية للمعلوم والمبنية للمجهول. أخيراً، وجدنا أن عكس ترتيب الكلمات في جميع الجمل المصدر (وليس الجمل المستهدفة) حسّن أداء LSTM بشكل ملحوظ، لأن ذلك أدخل العديد من التبعيات قصيرة المدى بين الجملة المصدر والجملة المستهدفة مما جعل مسألة التحسين أسهل.

### Back-Translation (Validation)
Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although deep neural networks work well when large labeled training sets are available, they cannot be used to map sequences to sequences. In this research, we present a general comprehensive end-to-end approach for sequence learning that makes few assumptions about the sequence structure. Our method uses a multi-layered Long Short-Term Memory (LSTM) to map the input sequence to a vector of fixed dimensionality, then uses another deep LSTM to decode the target sequence from the vector. Our main result is that in the English to French translation task from the WMT'14 dataset, the translations produced by LSTM achieved a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was reduced for words not in the vocabulary. Additionally, LSTM did not face difficulty with long sentences. For comparison, a phrase-based Statistical Machine Translation (SMT) system achieves a BLEU score of 33.3 on the same dataset. When we used LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, the BLEU score increased to 36.5, which is close to the previous best result in this task. LSTM also learned reasonable representations for phrases and sentences that are sensitive to word order and relatively stable toward active and passive voice. Finally, we found that reversing the word order in all source sentences (not target sentences) improved LSTM's performance significantly, because that introduced many short-term dependencies between the source sentence and the target sentence which made the optimization problem easier.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
- Semantic Equivalence: Excellent - preserves all technical details and findings
- Technical Accuracy: High - correctly translates LSTM, BLEU, sequence-to-sequence concepts
- Completeness: 100% - all information preserved
- Coherence: Natural Arabic flow while maintaining technical precision
- Glossary Consistency: Strong use of established technical terms
