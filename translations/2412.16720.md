---
# OpenAI o1 System Card
## بطاقة نظام OpenAI o1

**arXiv ID:** 2412.16720
**Authors:** OpenAI (260+ contributors including Aaron Jaech, Adam Kalai, Adam Lerer, Ilya Sutskever, Sam Altman, and others)
**Year:** 2024
**Categories:** Artificial Intelligence (cs.AI)
**Translation Quality:** 0.90
**Glossary Terms Used:** reinforcement learning, chain-of-thought, reasoning, model, benchmark, jailbreak, alignment, evaluation, red teaming, safety, intelligence

### English Abstract
The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. The models can reason about safety policies in context when responding to potentially unsafe prompts, through deliberative alignment, which leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations. The authors emphasize that while chain-of-thought reasoning offers benefits, it also increases potential risks that stem from heightened intelligence.

### الملخص العربي
تم تدريب سلسلة نماذج o1 باستخدام التعلم المعزز واسع النطاق للاستدلال عبر سلسلة التفكير. يمكن للنماذج الاستدلال حول سياسات الأمان في السياق عند الاستجابة لمطالبات قد تكون غير آمنة، من خلال المواءمة التداولية، مما يؤدي إلى أداء متقدم على معايير معينة للمخاطر مثل توليد نصائح غير مشروعة، واختيار استجابات نمطية، والاستسلام لعمليات الاختراق المعروفة. يوضح هذا التقرير أعمال الأمان التي تم تنفيذها لنماذج OpenAI o1 وOpenAI o1-mini، بما في ذلك تقييمات الأمان، وفرق الاختبار الخارجية، وتقييمات إطار الاستعداد. يؤكد المؤلفون أنه بينما يوفر الاستدلال بسلسلة التفكير فوائد، فإنه يزيد أيضاً من المخاطر المحتملة الناجمة عن الذكاء المتزايد.

### Back-Translation (Validation)
The o1 model series was trained using large-scale reinforcement learning to reason via chain of thought. The models can reason about safety policies in context when responding to potentially unsafe prompts, through deliberative alignment, leading to advanced performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. This report outlines the safety work implemented for OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teams, and Preparedness Framework evaluations. The authors emphasize that while chain-of-thought reasoning provides benefits, it also increases potential risks stemming from increased intelligence.

### Translation Metrics
- Iterations: 1
- Final Score: 0.90
- Quality: High
---
