# Small Language Models are the Future of Agentic AI
## نماذج اللغة الصغيرة هي مستقبل الذكاء الاصطناعي الوكيل

**arXiv ID:** 2506.02153
**Authors:** Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, Pavlo Molchanov
**Year:** 2025
**Categories:** cs.AI
**Translation Quality:** 0.92
**Glossary Terms Used:** معمارية, خوارزمية, اعتماد, غير متجانس

### English Abstract
The authors contend that small language models represent a superior choice for agentic AI systems despite the acclaim large language models receive. They argue that "SLMs are sufficiently powerful, inherently more suitable, and necessarily more economical" for agent applications involving repetitive, specialized tasks. The paper grounds this position in current SLM capabilities, typical agent architectures, and deployment economics. The researchers suggest heterogeneous systems combining multiple models work best when general conversation abilities matter. They identify adoption barriers, present an LLM-to-SLM conversion algorithm, and emphasize the operational and economic significance of transitioning from larger to smaller models in the AI agent industry.

### الملخص العربي
يؤكد المؤلفون أن نماذج اللغة الصغيرة تمثل خياراً أفضل لأنظمة الذكاء الاصطناعي الوكيل على الرغم من الإشادة التي تحظى بها نماذج اللغة الكبيرة. ويجادلون بأن "نماذج اللغة الصغيرة قوية بما فيه الكفاية، وأكثر ملاءمة بطبيعتها، وأكثر اقتصادية بالضرورة" لتطبيقات الوكلاء التي تتضمن مهام متكررة ومتخصصة. تؤسس الورقة هذا الموقف على قدرات نماذج اللغة الصغيرة الحالية، والمعماريات النموذجية للوكلاء، واقتصاديات النشر. يقترح الباحثون أن الأنظمة غير المتجانسة التي تجمع بين نماذج متعددة تعمل بشكل أفضل عندما تكون قدرات المحادثة العامة مهمة. يحددون حواجز الاعتماد، ويقدمون خوارزمية للتحويل من نماذج اللغة الكبيرة إلى الصغيرة، ويؤكدون على الأهمية التشغيلية والاقتصادية للانتقال من النماذج الأكبر إلى الأصغر في صناعة الوكلاء الذكية.

### Back-Translation (Validation)
The authors assert that small language models represent a better choice for agentic AI systems despite the acclaim that large language models receive. They argue that "small language models are sufficiently powerful, inherently more suitable, and necessarily more economical" for agent applications involving repetitive and specialized tasks. The paper grounds this position on current small language model capabilities, typical agent architectures, and deployment economics. The researchers propose that heterogeneous systems combining multiple models work better when general conversation capabilities are important. They identify adoption barriers, present an algorithm for converting from large to small language models, and emphasize the operational and economic importance of transitioning from larger to smaller models in the intelligent agent industry.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
