---
# ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction
## ChemBERTa: التدريب المسبق ذاتي الإشراف واسع النطاق للتنبؤ بخصائص الجزيئات

**arXiv ID:** 2010.09885
**Authors:** Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar
**Year:** 2020
**Categories:** Machine Learning (cs.LG); Computation and Language (cs.CL); Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)
**Translation Quality:** 0.92
**Glossary Terms Used:** graph neural network, molecular properties, transformer, natural language, attention mechanism, dataset, representation learning

### English Abstract
GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.

### الملخص العربي
تُعد الشبكات العصبية البيانية والبصمات الكيميائية النهج السائد لتمثيل الجزيئات للتنبؤ بالخصائص. ومع ذلك، في معالجة اللغة الطبيعية، أصبحت المحولات المعيار الفعلي لتعلم التمثيلات بفضل قدرتها القوية على نقل المهام النهائية. بالتوازي، يتطور النظام البيئي البرمجي حول المحولات بسرعة، حيث تمكّن مكتبات مثل HuggingFace و BertViz التدريب والفحص المبسط. في هذا العمل، نقوم بإحدى المحاولات الأولى لتقييم المحولات بشكل منهجي على مهام التنبؤ بخصائص الجزيئات عبر نموذجنا ChemBERTa. يتوسع ChemBERTa بشكل جيد مع حجم مجموعة بيانات التدريب المسبق، مما يوفر أداءً تنافسياً على المهام النهائية في MoleculeNet ونماذج تصور مفيدة قائمة على الانتباه. تشير نتائجنا إلى أن المحولات توفر مساراً واعداً للعمل المستقبلي لتعلم التمثيلات الجزيئية والتنبؤ بالخصائص. لتسهيل هذه الجهود، نطلق مجموعة بيانات منسقة من 77 مليون SMILES من PubChem مناسبة للتدريب المسبق ذاتي الإشراف واسع النطاق.

### Back-Translation (Validation)
Graph neural networks and chemical fingerprints are the predominant approaches for representing molecules for property prediction. However, in natural language processing, transformers have become the de facto standard for representation learning thanks to their strong ability to transfer downstream tasks. In parallel, the software ecosystem around transformers is rapidly evolving, with libraries like HuggingFace and BertViz enabling streamlined training and inspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, providing competitive performance on downstream tasks in MoleculeNet and useful attention-based visualization models. Our results suggest that transformers offer a promising avenue for future work on molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77 million SMILES from PubChem suitable for large-scale self-supervised pretraining.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
---
