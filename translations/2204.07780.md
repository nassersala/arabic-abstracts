# Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks
## نحو محول خفيف الوزن عبر التحويل الجماعي لمهام الرؤية واللغة

**arXiv ID:** 2204.07780
**Authors:** Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, Rongrong Ji
**Year:** 2022
**Categories:** cs.CV
**Translation Quality:** 0.93
**Glossary Terms Used:** transformer, attention mechanism, neural network (implied)

### English Abstract
This study introduces group-wise techniques to reduce Transformer parameters and computations while maintaining efficiency in Multi-Head Attention and Feed-Forward Networks. The resulting LW-Transformer is evaluated across vision-and-language benchmarks and image classification. Results demonstrate competitive performance despite substantial parameter reduction, with effectiveness confirmed across multiple architectural variants.

### الملخص العربي
تقدم هذه الدراسة تقنيات جماعية لتقليل معاملات المحول والعمليات الحسابية مع الحفاظ على الكفاءة في آلية الانتباه متعددة الرؤوس والشبكات التغذية الأمامية. يتم تقييم المحول الخفيف الوزن (LW-Transformer) الناتج عبر معايير الرؤية واللغة وتصنيف الصور. تُظهر النتائج أداءً تنافسيًا على الرغم من التخفيض الكبير في المعاملات، مع تأكيد الفعالية عبر متغيرات معمارية متعددة.

### Back-Translation (Validation)
This study introduces group techniques to reduce transformer parameters and computational operations while maintaining efficiency in Multi-Head Attention mechanisms and Feed-Forward Networks. The resulting Lightweight Transformer (LW-Transformer) is evaluated across vision-and-language benchmarks and image classification. Results demonstrate competitive performance despite significant parameter reduction, with effectiveness confirmed across multiple architectural variants.

### Translation Metrics
- Iterations: 1
- Final Score: 0.93
- Quality: High
