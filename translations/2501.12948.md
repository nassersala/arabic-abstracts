---
# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
## DeepSeek-R1: تحفيز قدرة الاستدلال في نماذج اللغة الكبيرة عبر التعلم المعزز

**arXiv ID:** 2501.12948
**Authors:** DeepSeek-AI (200 contributors including Daya Guo, Dejian Yang, Haowei Zhang, and others)
**Year:** 2025
**Categories:** Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
**Translation Quality:** 0.92
**Glossary Terms Used:** reinforcement learning, reasoning, large language model, fine-tuning, training, open-source, architecture, distillation

### English Abstract
The paper introduces DeepSeek-R1-Zero and DeepSeek-R1, reasoning models developed through large-scale reinforcement learning. DeepSeek-R1-Zero was trained via RL without supervised fine-tuning, naturally developing sophisticated reasoning behaviors. However, this version had limitations including poor readability and language mixing. The enhanced DeepSeek-R1 incorporates multi-stage training and demonstrates performance comparable to OpenAI-o1-1217 on reasoning tasks. The authors have open-sourced both models along with six distilled dense variants based on existing architectures.

### الملخص العربي
يقدم البحث DeepSeek-R1-Zero وDeepSeek-R1، نماذج استدلال مطورة من خلال التعلم المعزز واسع النطاق. تم تدريب DeepSeek-R1-Zero عبر التعلم المعزز دون الضبط الدقيق الموجَّه، مما أدى إلى تطوير سلوكيات استدلال متطورة بشكل طبيعي. ومع ذلك، كان لهذا الإصدار قيود تشمل ضعف القابلية للقراءة وخلط اللغات. يدمج DeepSeek-R1 المحسَّن التدريب متعدد المراحل ويُظهر أداءً مماثلاً لـ OpenAI-o1-1217 في مهام الاستدلال. أصدر المؤلفون النموذجين كمصدر مفتوح إلى جانب ستة متغيرات كثيفة مقطرة تعتمد على معماريات موجودة.

### Back-Translation (Validation)
The research introduces DeepSeek-R1-Zero and DeepSeek-R1, reasoning models developed through large-scale reinforcement learning. DeepSeek-R1-Zero was trained via reinforcement learning without supervised fine-tuning, leading to the natural development of sophisticated reasoning behaviors. However, this version had limitations including poor readability and language mixing. The enhanced DeepSeek-R1 integrates multi-stage training and demonstrates performance comparable to OpenAI-o1-1217 in reasoning tasks. The authors released both models as open source along with six dense distilled variants based on existing architectures.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
---
