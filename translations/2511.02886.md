---
# Test-time Adaptation of Tiny Recursive Models
## التكيف أثناء الاختبار للنماذج العودية الصغيرة

**arXiv ID:** 2511.02886
**Authors:** Ronan Killian McGovern
**Year:** 2025
**Categories:** Machine Learning (cs.LG), Artificial Intelligence (cs.AI)
**Translation Quality:** 0.92
**Glossary Terms Used:** fine-tuning, recursive, neural network, parameters, model, training, GPU, benchmark, accuracy, performance, gradient, adaptation

### English Abstract
The paper describes a method for efficiently fine-tuning a compact recursive neural network on ARC competition tasks within computational constraints. A 7-million parameter model was initially trained on 1,280 public tasks over 48 hours using 4 H100 GPUs, achieving approximately 10% accuracy on public benchmarks. Through targeted post-training using only 12,500 gradient steps during competition, the model reached 6.67% performance on semi-private evaluation sets. The approach emphasizes "full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone," demonstrating practical test-time adaptation within resource limitations.

### الملخص العربي
تصف الورقة طريقة للضبط الدقيق الفعال لشبكة عصبية عودية صغيرة على مهام مسابقة ARC ضمن قيود حسابية. تم تدريب نموذج بـ7 ملايين معامل في البداية على 1,280 مهمة عامة على مدار 48 ساعة باستخدام 4 وحدات معالجة رسومات H100، محققاً دقة تقارب 10٪ على المعايير العامة. من خلال التدريب اللاحق الموجه باستخدام 12,500 خطوة تدرج فقط أثناء المسابقة، حقق النموذج أداءً بنسبة 6.67٪ على مجموعات التقييم شبه الخاصة. يؤكد النهج على "الضبط الدقيق الكامل للنموذج الصغير، وليس الضبط الدقيق بـ LoRA أو الضبط الدقيق للتضمينات المهمة وحدها"، مما يُظهر التكيف العملي أثناء الاختبار ضمن قيود الموارد.

### Back-Translation (Validation)
The paper describes a method for efficient fine-tuning of a small recursive neural network on ARC competition tasks within computational constraints. A model with 7 million parameters was initially trained on 1,280 public tasks over 48 hours using 4 H100 GPUs, achieving approximately 10% accuracy on public benchmarks. Through targeted post-training using only 12,500 gradient steps during the competition, the model achieved 6.67% performance on semi-private evaluation sets. The approach emphasizes "full fine-tuning of the small model, not fine-tuning with LoRA or fine-tuning of task embeddings alone," demonstrating practical test-time adaptation within resource limitations.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
---
