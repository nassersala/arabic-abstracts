---
# LLaMA: Open and Efficient Foundation Language Models
## LLaMA: نماذج اللغة الأساسية المفتوحة والفعّالة

**arXiv ID:** 2302.13971
**Authors:** Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
**Year:** 2023
**Categories:** Computer Science - Computation and Language (cs.CL)
**Translation Quality:** 0.96
**Glossary Terms Used:** foundation, language model, parameter, training, token, dataset, state-of-the-art, benchmark, outperform, competitive, release, research community

### English Abstract
We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.

### الملخص العربي
نقدم LLaMA، وهي مجموعة من نماذج اللغة الأساسية التي تتراوح من 7 مليار إلى 65 مليار معامل. ندرب نماذجنا على تريليونات من الرموز، ونُظهر أنه من الممكن تدريب نماذج متقدمة باستخدام مجموعات بيانات متاحة للعموم حصرياً، دون اللجوء إلى مجموعات بيانات خاصة وغير قابلة للوصول. على وجه الخصوص، يتفوق LLaMA-13B على GPT-3 (175B) على معظم المعايير، بينما يُنافس LLaMA-65B أفضل النماذج مثل Chinchilla-70B وPaLM-540B. نُطلق جميع نماذجنا لمجتمع الأبحاث.

### Back-Translation (Validation)
We introduce LLaMA, which is a collection of foundation language models that range from 7 billion to 65 billion parameters. We train our models on trillions of tokens, and we show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, while LLaMA-65B competes with the best models such as Chinchilla-70B and PaLM-540B. We release all our models to the research community.

### Translation Metrics
- Iterations: 2
- Final Score: 0.96
- Quality: High
---
