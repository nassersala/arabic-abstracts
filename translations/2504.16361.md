# Comparing Different Transformer Model Structures for Stock Prediction
## مقارنة بنى نماذج المحول المختلفة للتنبؤ بالأسهم

**arXiv ID:** 2504.16361
**Authors:** Qizhao Chen
**Year:** 2025
**Categories:** cs.CE
**Translation Quality:** 0.92
**Glossary Terms Used:** transformer, attention mechanism

### English Abstract
This investigation compares five Transformer variants for stock forecasting: encoder-only, decoder-only, vanilla with/without embedding, and vanilla with ProbSparse attention. Findings indicate decoder-only structures outperform alternatives consistently, while ProbSparse variants underperformed. Results suggest Transformer-based approaches generally exceed traditional methods for financial prediction tasks.

### الملخص العربي
يقارن هذا البحث خمسة متغيرات من المحول للتنبؤ بالأسهم: المشفر فقط، وفك التشفير فقط، والمحول الأساسي مع/بدون التضمين، والمحول الأساسي مع آلية الانتباه ProbSparse. تشير النتائج إلى أن بنى فك التشفير فقط تتفوق على البدائل بشكل ثابت، بينما أظهرت متغيرات ProbSparse أداءً أقل. تشير النتائج إلى أن الأساليب القائمة على المحول تتجاوز عمومًا الطرق التقليدية لمهام التنبؤ المالي.

### Back-Translation (Validation)
This research compares five Transformer variants for stock prediction: encoder-only, decoder-only, vanilla Transformer with/without embedding, and vanilla Transformer with ProbSparse attention mechanism. Results indicate that decoder-only structures consistently outperform alternatives, while ProbSparse variants showed lower performance. Results suggest that Transformer-based approaches generally exceed traditional methods for financial prediction tasks.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
