# Improving neural networks by preventing co-adaptation of feature detectors
## تحسين الشبكات العصبية من خلال منع التكيف المشترك لكاشفات الميزات

**arXiv ID:** 1207.0580
**Authors:** Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov
**Year:** 2012
**Categories:** Neural and Evolutionary Computing (cs.NE); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)
**Translation Quality:** 0.91
**Glossary Terms Used:** neural network, feed-forward network, training, speech recognition, benchmark, overfitting, dropout, feature detector, co-adaptation, robust

### English Abstract
When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.

### الملخص العربي
تتناول هذه الورقة مشكلة الإفراط في التدريب في الشبكات العصبية الأمامية الكبيرة المدربة على مجموعات بيانات صغيرة. يقدم المؤلفون تقنية "تحذف بشكل عشوائي نصف كاشفات الميزات في كل حالة تدريب" لمنع التكيف المشترك الإشكالي للميزات. يمكّن هذا النهج الخلايا العصبية الفردية من تعلم ميزات قوية فعالة عبر سياقات شبكية داخلية متنوعة. حققت هذه الطريقة، المسماة "dropout"، نتائج متقدمة على مهام معيارية متعددة في التعرف على الكلام والكائنات.

### Back-Translation (Validation)
This paper addresses the problem of overfitting in large feedforward neural networks trained on small datasets. The authors introduce a technique that "randomly omits half of the feature detectors in each training case" to prevent problematic co-adaptation of features. This approach enables individual neurons to learn robust features effective across diverse internal network contexts. This method, called "dropout," achieved state-of-the-art results on multiple benchmark tasks in speech and object recognition.

### Translation Metrics
- Iterations: 1
- Final Score: 0.91
- Quality: High
