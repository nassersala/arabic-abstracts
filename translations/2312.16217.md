# ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation
## ManipLLM: نموذج لغة كبير متعدد الوسائط مُجسَّد للتلاعب الروبوتي المتمحور حول الكائنات

**arXiv ID:** 2312.16217
**Authors:** Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong
**Year:** 2023
**Categories:** cs.CV, cs.RO
**Translation Quality:** 0.92
**Glossary Terms Used:** robot manipulation, end-effector, multimodal, large language model, generalization, fine-tuning, reasoning, affordance, chain-of-thought, closed-loop, adaptation, simulator

### English Abstract
Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM.

### الملخص العربي
يعتمد التلاعب الروبوتي على التنبؤ الدقيق بنقاط الاتصال واتجاهات المؤثر النهائي لضمان نجاح العملية. ومع ذلك، فإن التلاعب الروبوتي القائم على التعلم، والمُدرَّب على فئة محدودة داخل محاكي، غالباً ما يواجه صعوبة في تحقيق القابلية للتعميم، خاصة عند مواجهة فئات واسعة. لذلك، نقدم نهجاً مبتكراً للتلاعب الروبوتي يستفيد من قدرات الاستدلال القوية لنماذج اللغة الكبيرة متعددة الوسائط (MLLMs) لتعزيز استقرار وتعميم التلاعب. من خلال الضبط الدقيق للمحولات المُدخلة، نحافظ على الحس المشترك المتأصل وقدرة الاستدلال لنماذج اللغة الكبيرة متعددة الوسائط مع تزويدها بالقدرة على التلاعب. تكمن الرؤية الأساسية في نموذج الضبط الدقيق المقدم، الذي يشمل فهم فئة الكائن، والاستدلال المسبق للإمكانات، والتنبؤ بالوضعية المتمحورة حول الكائن لتحفيز قدرة الاستدلال لنماذج اللغة الكبيرة متعددة الوسائط في التلاعب. أثناء الاستنتاج، يستخدم نهجنا صورة RGB وموجهاً نصياً للتنبؤ بوضعية المؤثر النهائي في سلسلة من الأفكار. بعد إنشاء الاتصال الأولي، يتم تقديم سياسة تكيف ممانعة نشطة للتخطيط لنقاط المسار القادمة بطريقة الحلقة المغلقة. علاوة على ذلك، في العالم الحقيقي، نصمم استراتيجية التكيف في وقت الاختبار (TTA) للتلاعب لتمكين النموذج من التكيف بشكل أفضل مع تكوين المشهد الحقيقي الحالي. تظهر التجارب في المحاكي والعالم الحقيقي الأداء الواعد لـ ManipLLM.

### Back-Translation (Validation)
Robot manipulation depends on accurate prediction of contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often faces difficulty in achieving generalizability, especially when confronting extensive categories. Therefore, we present an innovative approach for robot manipulation that leverages the strong reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. Through fine-tuning of the injected adapters, we preserve the inherent common sense and reasoning ability of the multimodal large language models while equipping them with manipulation capability. The fundamental insight lies in the presented fine-tuning paradigm, which encompasses object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of multimodal large language models in manipulation. During inference, our approach uses an RGB image and text prompt to predict the end-effector's pose in a chain of thoughts. After establishing initial contact, an active impedance adaptation policy is introduced to plan upcoming waypoints in a closed-loop manner. Moreover, in the real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model to better adapt to the current real-world scene configuration. Experiments in the simulator and real world show the promising performance of ManipLLM.

### Translation Metrics
- Iterations: 1
- Final Score: 0.92
- Quality: High
- Semantic Equivalence: Excellent preservation of technical meaning
- Technical Accuracy: All technical terms correctly translated using glossary
- Completeness: Full translation with no omissions
- Coherence: Natural Arabic flow while maintaining technical precision
- Glossary Consistency: Consistent use of established technical terms
